[{"id":"af845c075b322bafbf0dd249cd968eb7","title":"如何更好地使用Kafka","content":"引言         要确保Kafka在使用过程中的稳定性，需要从kafka在业务中的使用周期进行依次保障。主要可以分为：事先预防（通过规范的使用、开发，预防问题产生）、运行时监控（保障集群稳定，出问题能及时发现）、故障时解决（有完整的应急预案）这三阶段。\n事先预防事先预防即通过规范的使用、开发，预防问题产生。主要包含集群&#x2F;生产端&#x2F;消费端的一些最佳实践、上线前测试以及一些针对紧急情况（如消息积压等）的临时开关功能。\nKafka调优原则：\n1.确定优化目标，并且定量给出目标（Kafka 常见的优化目标是吞吐量、延时、持久性和可用性）。\n2.确定了目标之后，需要明确优化的维度。\n通用性优化：操作系统、JVM 等。\n针对性优化：优化 Kafka 的 TPS、处理速度、延时等。\n\n生产端最佳实践参数调优\n使用 Java 版的 Client；\n\n使用 kafka-producer-perf-test.sh 测试你的环境；\n\n设置内存、CPU、batch 压缩；\n\nbatch.size：该值设置越大，吞吐越大，但延迟也会越大；\n\nlinger.ms：表示 batch 的超时时间，该值越大，吞吐越大、但延迟也会越大；\n\nmax.in.flight.requests.per.connection：默认为5，表示 client 在 blocking 之前向单个连接（broker）发送的未确认请求的最大数，超过1时，将会影响数据的顺序性；\n\ncompression.type：压缩设置，会提高吞吐量；\n\nacks：数据 durability 的设置；\n\n避免大消息（占用过多内存、降低broker处理速度）；\n\nbroker调整：增加 num.replica.fetchers，提升 Follower 同步 TPS，避免 Broker Full GC 等；\n\n当吞吐量小于网络带宽时：增加线程、提高 batch.size、增加更多 producer 实例、增加 partition 数；\n\n设置 acks&#x3D;-1 时，如果延迟增大：可以增大 num.replica.fetchers（follower 同步数据的线程数）来调解；\n\n跨数据中心的传输：增加 socket 缓冲区设置以及 OS tcp 缓冲区设置。\n\n\n开发实践a.做好Topic隔离\n根据具体场景（是否允许一定延迟、实时消息、定时周期任务等）区分kafka topic，避免挤占或阻塞实时业务消息的处理。\nb.做好消息流控\n如果下游消息消费存在瓶颈或者集群负载过高等，需要在生产端（或消息网关）实施流量生产速率的控制或者延时&#x2F;暂定消息发送等策略，避免短时间内发送大量消息。\nc.做好消息补推\n手动去查询丢失的那部分数据，然后将消息重新发送到mq里面，把丢失的数据重新补回来。\nd.做好消息顺序性保障\n如果需要在保证Kafka在分区内严格有序的话（即需要保证两个消息是有严格的先后顺序），需要设置key，让某类消息根据指定规则路由到同一个topic的同一个分区中（能解决大部分消费顺序的问题）。但是，需要避免分区内消息倾斜的问题（例如，按照店铺Id进行路由，容易导致消息不均衡的问题）。\n1.生产端：消息发送指定key，确保相同key的消息发送到同一个partition。\n2.消费端：单线程消费或者写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue。\ne.适当提高消息发送效率\n批量发送：kafka先将消息缓存在内存中的双端队列（buffer）中，当消息量达到batch size指定大小时进行批量发送，减少了网络传输频次，提高了传输效率；\n端到端压缩消息：将一批消息打包后进行压缩，发送给 Broker 服务器后，但频繁的压缩和解压也会降低性能，最终还是以压缩的方式传递到消费者的手上，在 Consumer 端进行解压；\n异步发送：将生产者改造为异步的方式，可以提升发送效率，但是如果消息异步产生过快，会导致挂起线程过多，内存不足，最终导致消息丢失；\n索引分区并行消费：当一个时间相对长的任务在执行时，它会占用该消息所在索引分区被锁定，后面的任务不能及时派发给空闲的客户端处理，若服务端如果启用索引分区并行消费的特性，就可以及时的把后面的任务派发给其他的客户端去执行，同时也不需要调整索引的分区数（但此类消息仅适用于无需保证消息顺序关系的消息）。\nf.保证消息发送可靠性\nProducer：如果对数据可靠性要求很高的话，在发送消息的时候，需要选择带有 callBack 的api进行发送，并设置 acks、retries、factor等等些参数来保证Producer发送的消息不丢失。\nBroker：kafka为了得到更高的性能和吞吐量，将数据异步批量的存储在磁盘中，并采用了批量刷盘的做法，如果对数据可靠性要求很高的话，可以修改为同步刷盘的方式提高消息的可靠性。\n消费端最佳实践参数调优\n吞吐量：调整partition 数、OS page cache（分配足够的内存来缓存数据）；\n\noffset topic（__consumer_offsets）：offsets.topic.replication.factor（默认为3）、offsets.retention.minutes（默认为1440，即 1day）；\n\noffset commit较慢：异步 commit 或 手动 commit；\n\nfetch.min.bytes 、fetch.max.wait.ms；\n\nmax.poll.interval.ms：调用 poll() 之后延迟的最大时间，超过这个时间没有调用 poll() 的话，就会认为这个 consumer 挂掉了，将会进行 rebalance；\n\nmax.poll.records：当调用 poll() 之后返回最大的 record 数，默认为500；\n\nsession.timeout.ms；\n\nConsumer Rebalance：check timeouts、check processing times&#x2F;logic、GC Issues；\n\n网络配置。\n\n\n开发实践a.做好消息消费幂等\n消息消费的幂等主要根据业务逻辑做调整。\n以处理订单消息为例：\n1.由订单编号+订单状态唯一的幂等key，并存入redis；\n2.在处理之前，首先会去查Redis是否存在该Key，如果存在，则说明已经处理过了，直接丢掉；\n3.如果Redis没处理过，则将处理过的数据插入到业务DB上，再到最后把幂等Key插入到Redis上；\n简而言之，即通过Redis做前置处理 + DB唯一索引做最终保证来实现幂等性。\nb.做好Consumer隔离\n在消息量非常大的情况下，实时和离线消费者同时消费一个集群，离线数据繁重的磁盘 IO 操作会直接影响实时业务的实时性和集群的稳定性。\n根据消费的实时性可以将消息消费者行为划分两类：实时消费者和离线消费者。\n实时消费者：对数据实时性要求较高；在实时消费的场景下，Kafka 会利用系统的 page cache 缓存，直接从内存转发给实时消费者（热读），磁盘压力为零，适合广告、推荐等业务场景。\n离线消费者（定时周期性消费者）：通常是消费数分钟前或是数小时前的消息，这类消息通常存储在磁盘中，消费时会触发磁盘的 IO 操作（冷读），适合报表计算、批量计算等周期性执行的业务场景。\nc.避免消息消费堆积\n\n延迟处理、控制速度，时间范围内分摊消息（针对实时性不高的消息）；\n\n生产速度大于消费速度，这样可以适当增加分区，增加consumer数量，提升消费TPS；\n\n避免很重的消费逻辑，优化consumer TPS：\n\n\n是否有大量DB操作；\n下游&#x2F;外部服务接口调用超时；\n是否有lock操作（导致线程阻塞）；\n需要特别关注kafka异步链路中的涉及消息放大的逻辑。\n\n如果有较重的消费逻辑，需要调整xx参数，避免消息没消费完时，消费组退出，造成reblance等问题；\n\n确保consumer端没有因为异常而导致消费hang住;\n\n如果使用的是消费者组，确保没有频繁地发生rebalance；\n\n多线程消费，批量拉取处理。\n\n\n注：批量拉取处理时，需注意下kafka版本，spring-kafka 2.2.11.RELEASE版本以下，如果配置kafka.batchListener&#x3D;true，但是将消息接收的元素设置为单个元素（非批量List），可能会导致kafka在拉取一批消息后，仅仅消费了头部的第一个消息。\nd.避免Rebalance问题\n\n触发条件：\n\n1.消费者数量变化：新消费者加入、消费者下线（未能及时发送心跳，被“踢出”Group）、消费者主动退出消费组（Consumer 消费时间过长导致）；\n2.消费组内订阅的主题或者主题的分区数量发生变化；\n3.消费组对应的 GroupCoorinator 节点发生变化。\n\n如何避免非必要rebalance（消费者下线、消费者主动退出消费组导致的reblance）：\n\n1.需要仔细地设置session.timeout.ms（决定了 Consumer 存活性的时间间隔）和heartbeat.interval.ms（控制发送心跳请求频率的参数） 的值。\n2.max.poll.interval.ms参数配置：控制 Consumer 实际消费能力对 Rebalance 的影响，限定了 Consumer 端应用程序两次调用 poll 方法的最大时间间隔。默认值是 5 分钟，表示 Consumer 程序如果在 5 分钟之内无法消费完 poll 方法返回的消息，那么 Consumer 会主动发起“离开组”的请求，Coordinator 也会开启新一轮 Rebalance。具体可以统计下历史的时间花费，把最长的时间为参考进行设置。\ne.保证消息消费可靠性\n一般情况下，还是client 消费 broker 丢消息的场景比较多，想client端消费数据不能丢，肯定是不能使用autoCommit的，所以必须是手动提交的。\nConsumer自动提交的机制是根据一定的时间间隔，将收到的消息进行commit。commit过程和消费消息的过程是异步的。也就是说，可能存在消费过程未成功（比如抛出异常），commit消息已经提交了，则此时消息就丢失了。\n\nf.保证消息消费顺序性\n1.不同topic（乱序消息）：如果支付与订单生成对应不同的topic，只能在consumer层面去处理了。\n2.同一个topic（乱序消息）：一个topic可以对应多个分区，分别对应了多个consumer，与“不同topic”没什么本质上的差别。（可以理解为我们的服务有多个pod，生产者顺序发送消息，但被路由到不同分区，就可能变得乱序了，服务消费的就是无序的消息）。\n3.同一个topic，同一个分区（顺序消息）：Kafka的消息在分区内是严格有序的，例如把同一笔订单的所有消息，按照生成的顺序一个个发送到同一个topic的同一个分区。\n针对乱序消息：\n例如：订单和支付分别封装了各自的消息，但是消费端的业务场景需要按订单消息-&gt;支付消息的顺序依次消费消息。\n宽表（业务主题相关的指标、维度、属性关联在一起的一张数据库表）：消费消息时，只更新对应的字段就好，消息只会存在短暂的状态不一致问题，但是状态最终是一致的。例如订单，支付有自己的状态字段，订单有自己的状态字段，售后有自己的状态字段，就不需要保证支付、订单、售后消息的有序，即使消息无序，也只会更新自己的状态字段，不会影响到其他状态；\n消息补偿机制：将消息与DB进行对比，如果发现数据不一致，再重新发送消息至主进程处理，保证最终一致性；\nMQ队列：一个中间方（比如redis的队列）来维护MQ的顺序；\n业务保证：通过业务逻辑保障消费顺序；\n针对顺序消息：\n两者都是通过将消息绑定到定向的分区或者队列来保证顺序性，通过增加分区或者线程来提升消费能力。\n1.Consumer单线程顺序消费\n生产者在发送消息时，已保证消息在分区内有序，一个分区对应了一个消费者，保证了消息消费的顺序性。\n\n2.Consumer多线程顺序消费（具体策略在后面章节）\n单线程顺序消费的扩展能力很差。为了提升消费者的处理速度，除了横向扩展分区数，增加消费者外，还可以使用多线程顺序消费。\n将接收到的kafka数据进行hash取模（注意：如果kafka分区接受消息已经是取模的了，这里一定要对id做一次hash再取模）发送到不同的队列，然后开启多个线程去消费对应队列里面的数据。\n此外，这里通过配置中心进行开关、动态扩容&#x2F;缩容线程池。\ng.处理Consumer的事务\n通过事务消息，可以很好的保证一些业务场景的事务逻辑，不会因为网络不可用等原因出现系统之间状态不一致。\n当更新任何一个服务出现故障时就抛出异常，事务消息不会被提交或回滚，消息服务器会回调发送端的事务查询接口，确定事务状态，发送端程序可以根据消息的内容对未做完的任务重新执行，然后告诉消息服务器该事务的状态。\n集群配置最佳实践集群配置Broker 评估：每个 Broker 的 Partition 数不应该超过2k、控制 partition 大小（不要超过25GB）。\n集群评估（Broker 的数量根据以下条件配置）：数据保留时间、集群的流量大小。\n集群扩容：磁盘使用率应该在 60% 以下、网络使用率应该在 75% 以下。\n集群监控：保持负载均衡、确保 topic 的 partition 均匀分布在所有 Broker 上、确保集群的阶段没有耗尽磁盘或带宽。\nTopic 评估1.Partition 数：\nPartition 数应该至少与最大 consumer group 中 consumer 线程数一致；\n对于使用频繁的 topic，应该设置更多的 partition；\n控制 partition 的大小（25GB 左右）；\n考虑应用未来的增长（可以使用一种机制进行自动扩容）；\n2.使用带 key 的 topic；\n3.partition 扩容：当 partition 的数据量超过一个阈值时应该自动扩容（实际上还应该考虑网络流量）。\n分区配置设置多个分区在一定程度上是可以提高消费者消费的并发度，但是分区数量过多时可能会带来：句柄开销过大、生产端占用内存过大、可能增加端到端的延迟、影响系统可用性、故障恢复时间较长等问题。\n根据吞吐量的要求设置 partition 数：\n1.假设 Producer 单 partition 的吞吐量为 P\n2.consumer 消费一个 partition 的吞吐量为 C\n3.而要求的吞吐量为 T\n4.那么 partition 数至少应该大于 T&#x2F;P、T&#x2F;c 的最大值\n性能调优调优目标：高吞吐量、低延时。\n分层调优自上而下分为应用程序层、框架层、JVM层和操作系统层，层级越靠上，调优的效果越明显。\n|调优类型\n | \n建议\n || — | — || \n操作系统\n | \n挂载文件系统时禁掉atime更新；选择ext4或XFS文件系统；swap空间的设置；页缓存大小\n || \nJVM（堆设置和GC收集器）\n | \n将JVM 堆大小设置成 6～8GB；建议使用 G1 收集器，方便省事，比 CMS 收集器的优化难度小\n || \nBroker端\n | \n保持服务器端和客户端版本一致\n || \n应用层\n | \n要频繁地创建Producer和Consumer对象实例；用完及时关闭；合理利用多线程来改善性能\n |\n吞吐量(TPS)调优\n\n\n\n参数列表\n\n\n\n\n\n\n\nBroker端\n\n\n\n | \n适当增加num.replica.fetchers参数值，但不超过CPU核数\n ||  | \n调优GC参数以避免经常性的Full GC\n || \nProducer端\n | \n适当增加batch.size参数值，比如从默认的16KB增加到512KB或1MB\n ||  | \n适当增加linger.ms参数值，比如10～100\n ||  | \n设置compression.type&#x3D;lz4或zstd\n ||  | \n设置acks&#x3D;0或1\n ||  | \n设置retries&#x3D;0\n ||  | \n如果多线程共享同一个Producer实例，则增加buffer.memory参数值\n || \nConsumer端\n | \n采用多Consumer进程或线程同时消费数据\n ||  | \n增加fetch.min.bytes参数值，比如设置成1KB或更大\n |\n延时调优\n\n\n\n参数列表\n\n\n\n\n\n\n\nBroker端\n\n\n\n | \n适当设置num.replica.fetchers值\n || \nProducer端\n | \n设置linger.ms&#x3D;0\n ||  | \n不启用压缩，即设置compression.type&#x3D;none\n ||  | \n设置ackes&#x3D;1\n || \nConsumer端\n | \n设置fetch.min.bytes&#x3D;1\n |\n稳定性测试kafka的稳定性测试主要在业务上线前针对Kafka实例&#x2F;集群健康性、高可用性的测试。\n健康性检查1.检查实例：查看Kafka 实例对象中拿到所有的信息（例如 IP、端口等）；\n2.测试可用性：访问生产者和消费者，测试连接。\n高可用测试单节点异常测试：重启Leader副本或Follower副本所在Pod\n步骤：\n1.查看topic的副本信息\n2.删除相应pod\n3.脚本检测Kafka的可用性\n预期：对生产者和消费者的可用性均无影响。\n集群异常测试：重启所有pod\n步骤：\n1.删除所有pod\n2.脚本检测Kafka的可用性\n预期：所有broker ready后服务正常。\n运行时监控运行时监控主要包含集群稳定性配置与Kafka监控的最佳实践，旨在及时发现Kafka在运行时产生的相关问题与异常。\n\n集群稳定性监控腾讯云CKafka集群配置合理进行kafka实例配，主要关注这几个数据：\n\n磁盘容量和峰值带宽\n\n消息保留时长；\n\n动态保留策略；\n\n\na.磁盘容量和峰值带宽\n可根据实际业务的消息内容大小、发送消息qps等进行预估，可以尽量设置大点；具体数值可根据实例监控查看，如果短时间内磁盘使用百分比就达到较高值，则需扩容。\n峰值带宽&#x3D;最大生产流量*副本数\nb.消息保留时长\n消息即使被消费，也会持久化到磁盘存储保留时长的时间。该设置会占用磁盘空间，如果每天消息量很大的话，可适当缩短保留时间。\nc.动态保留策略\n推荐开启动态保留设置。当磁盘容量达到阈值，则删除最早的消息，最多删除到保底时长范围外的消息（淘汰策略），可以很大程度避免磁盘被打满的情况。\n但有调整时不会主动通知，但我们可以通过配置告警感知磁盘容量的变化。\n自建Kafka集群配置1.设置日志配置参数以使日志易于管理；\n2.了解 kafka 的(低)硬件需求；\n3.充分利用 Apache ZooKeeper；\n4.以正确的方式设置复制和冗余；\n5.注意主题配置；\n6.使用并行处理；\n7.带着安全性思维配置和隔离 Kafka；\n8.通过提高限制避免停机；\n9.保持低网络延迟；\n10.利用有效的监控和警报。\n资源隔离a.Broker级别物理隔离\n如果不同业务线的 topic 会共享一块磁盘，若某个consumer 出现问题而导致消费产生 lag，进而导致频繁读盘，会影响在同一块磁盘的其他业务线 TP 的写入。\n解决：Broker级别物理隔离：创建Topic、迁移Topic、宕机恢复流程\nb.RPC队列隔离\nKafka RPC 队列缺少隔离，一旦某个 topic 处理慢，会导致所有请求 hang 住。\n解决：需要按照控制流、数据流分离，且数据流要能够按照 topic 做隔离。\n1.将 call 队列按照拆解成多个，并且为每个 call 队列都分配一个线程池。\n2.一个队列单独处理 controller 请求的队列（隔离控制流），其余多个队列按照 topic 做 hash 的分散开（数据流之间隔离）。\n如果一个 topic 出现问题，则只会阻塞其中的一个 RPC 处理线程池，以及 call 队列，可以保障其他的处理链路是畅通的。\n智能限速整个限速逻辑实现在 RPC 工作线程处理的末端，一旦 RPC 处理完毕，则通过限速控制模块进行限速检测。\n1.配置等待时间，之后放入到 delayed queue 中，否则放到 response queue 中。\n2.放入到 delayed queue 中的请求，等待时间达到后，会被 delayed 线程放入到 response queue 中。\n3.最终在 response queue 中的请求被返回给 consumer。\nKafka监控白盒监控：服务或系统自身指标，如CPU 负载、堆栈信息、连接数等；\n黑盒监控：一般是通过模拟外部用户对其可见的系统功能进行监控的一种监控方式，相关指标如消息的延迟、错误率和重复率等性能和可用性指标。\n|监控\n | \n功能&#x2F;指标\n | \n详情\n || — | — | — || \n黑盒监控\n | \n操作\n | \n主题操作：创建、预览、查看、更新、删除\n ||  | \n服务\n | \n数据写入、是否消费成功\n ||  | \n系统\n | \nCPU 负载、堆栈信息、连接数等\n || \n白盒监控\n | \n容量\n | \n总存储空间、已用存储空间、最大分区使用、集群资源、分区数量、主题数量；\n ||  | \n流量\n | \n消息写入、消费速率、集群网络进出；\n ||  | \n延迟\n | \n消息写入、消费耗时(平均值、99分位、最大耗时)、主题消费延迟量(offset lag)\n ||  | \n错误\n | \n集群异常节点数量、消息写入拒绝量、消息消费失败量、依赖zookeeper的相关错误\n |\n腾讯云CKafka告警针对CKafka，需要配置告警（此类告警一般为消息积压、可用性、集群&#x2F;机器健康性等检查）。\na.指标\n如：实例健康状态、节点数量、健康节点数量、问题分区数、生产消息数、消费请求数、jvm内存利用率、平均生产响应时间、分区消费偏移量等。\n具体指标可以参考：https://cloud.tencent.com/document/product/597/54514\nb.配置\n配置文档：https://cloud.tencent.com/document/product/597/57244\n选择监控实例，配置告警内容和阈值。\n一般会对当前服务自身的kafka集群做告警配置，但是如果是依赖自身消息的下游服务出现消费问题，我们是感知不到了；而且针对消费端服务不共用同一个集群的情况，出现消息重复发送的问题，服务自身是很难发现的。\nc.预案\n在业务上线前，最好梳理下自身服务所涉及的topic消息（上游生产端和下游消费端），并细化告警配置，如果出现上游kafka异常或者下游kafka消息堆积可以及时感知。特别需要把可能有瞬时大量消息的场景（如批量数据导入、定时全量数据同步等）做一定的告警或者预案，避免服务不可用或者影响正常业务消息。\n自建告警平台通过自建告警平台配置对服务自身的异常告警，其中包括对框架在使用kafka组件时抛出与kafka消费逻辑过程中抛出的业务异常。\n其中，可能需要异常升级的情况（由于）单独做下处理（针对spring kafka）：\n1.自定义kafka异常处理器：实现KafkaListenerErrorHandler接口的方法，注册自定义异常监听器，区分业务异常并抛出；\n2.消费Kafka消息时，将@KafkaListener的errorHandler参数设置为定义的Kafka异常处理器；\n3.此后，指定的业务异常会被抛出，而不会被封装成Spring kafka的框架异常，导致不能清晰地了解具体异常信息。\nKafka监控组件目前业界并没有公认的解决方案，各家都有各自的监控之道。\nKafka Manager：应该算是最有名的专属 Kafka 监控框架了，是独立的监控系统。\nKafka Monitor：LinkedIn 开源的免费框架，支持对集群进行系统测试，并实时监控测试结果。\nCruiseControl：也是 LinkedIn 公司开源的监控框架，用于实时监测资源使用率，以及提供常用运维操作等。无 UI 界面，只提供 REST API。\nJMX 监控：由于 Kafka 提供的监控指标都是基于 JMX 的，因此，市面上任何能够集成 JMX 的框架都可以使用，比如 Zabbix 和 Prometheus。已有大数据平台自己的监控体系：像 Cloudera 提供的 CDH 这类大数据平台，天然就提供 Kafka 监控方案。\nJMXTool：社区提供的命令行工具，能够实时监控 JMX 指标。答上这一条，属于绝对的加分项，因为知道的人很少，而且会给人一种你对 Kafka 工具非常熟悉的感觉。如果你暂时不了解它的用法，可以在命令行以无参数方式执行一下kafka-run-class.sh kafka.tools.JmxTool，学习下它的用法。\nKafka Monitor其中，Kafka Monitor通过模拟客户端行为，生产和消费数据并采集消息的延迟、错误率和重复率等性能和可用性指标，可以很好地发现下游的消息消费情况进而可以动态地调整消息的发送。（使用过程中需注意对样本覆盖率、功能覆盖率、流量、数据隔离、时延的控制）\nKakfa Monitor 优势：1.通过为每个 Partition 启动单独的生产任务，确保监控覆盖所有 Partition。\n2.在生产的消息中包含了时间戳、序列号，Kafka Monitor 可以依据这些数据对消息的延迟、丢失率和重复率进行统计。\n3.通过设定消息生成的频率，来达到控制流量的目的。\n4.生产的消息在序列化时指定为一个可配置的大小（验证对不同大小数据的处理能力、相同消息大小的性能比较）。\n5.通过设定单独的 Topic 和 Producer ID 来操作 Kafka 集群，可避免污染线上数据，做到一定程度上的数据隔离。\n基于Kafka Monitor的设计思想，可以针对业务特点引入对消息的延迟、错误率和重复率等性能的监控告警指标。\n故障时解决防微杜渐，遇到问题&#x2F;故障时有完整的应急预案，以快速定位并解决问题。\n\nKafka消息堆积紧急预案问题描述：消费端产生消息积压，导致依赖该消息的服务不能及时感知业务变化，导致一些业务逻辑、数据处理出现延迟，容易产生业务阻塞和数据一致性问题。\n方案：问题排查、扩容升配策略、消息Topic转换策略、可配置多线程的消费策略。\n问题排查遇到消息积压时，具体可以从以下几个角度去定位问题原因：\n1.消息生产端数据量是否存在陡升的情况。\n2.消息消费端消费能力是否有下降。\n3.消息积压是发生在所有的partition还是所有的partition都有积压情况。\n对于第1、2点导致的消息积压：为暂时性的消息积压，通过扩分区、扩容升配、多线程消费、批量消费等方式提高消费速度能在一定程度上解决这类问题。\n对于第3点导致的消息积压：可以采用消息Topic中转策略。\n扩容升配策略1.检查生产端消费发送情况（主要检查是否继续有消息产生、是否存在逻辑缺陷、是否有重复消息发送）；\n2.观察消费端的消费情况（预估下堆积消息的处理清理以及是否有降低趋势）；\n3.若为生产端问题，则评估是否可以通过增加分区数、调整偏移量、删除topic（需要评估影响面）等解决；\n4.消费端新增机器及依赖资源，提高消费能力；\n5.如果涉及数据一致性问题，需要通过数据比对、对账等功能进行校验。\n配置多线程的消费策略简而言之，即线程池消费+动态线程池配置策略：将接收到的kafka数据进行hash取模（如果kafka分区接受消息已经是取模的了，这里一定要对id做一次hash再取模）发送到不同的队列，然后开启多个线程去消费对应队列里面的数据。\n设计思路：\n1.在应用启动时初始化对应业务的顺序消费线程池（demo中为订单消费线程池）；\n2.订单监听类拉取消息提交任务至线程池中对应的队列；\n3.线程池的线程处理绑定队列中的任务数据；\n4.每个线程处理完任务后增加待提交的offsets标识数；\n5.监听类中校验待提交的offsets数与拉取到的记录数是否相等，如果相等则；\n6.手动提交offset（关闭kafka的自动提交，待本次拉取到的任务处理完成之后再提交位移）\n\n另外，可以根据业务流量调整的线程配置与pod的配置，如高峰期设置一个相对较高的并发级别数用来快速处理消息，平峰期设置一个较小的并发级别数来让出系统资源。这里，可以参考美团提供的一种配置中心修改配置动态设置线程池参数的思路，实现动态的扩容或者缩容。\n实现了动态扩容与缩容：1.通过配置中心刷新OrderKafkaListener监听类中的配置concurrent的值。\n2.通过set方法修改concurrent的值时，先修改stopped的值去停止当前正在执行的线程池。\n3.执行完毕后通过新的并发级别数新建一个新的线程池，实现了动态扩容与缩容。\n\n此外，还可以新增开关，它设置为true是可以中断启动中的线程池，故障时进行功能开关。\n注意：如果涉及数据一致性问题，需要通过数据比对、对账等功能进行校验。\nTopic中转策略当消息积压是发生在所有的partition还是所有的partition都有积压情况时，只能操作临时扩容，以更快的速度去消费数据了。\n设计思路：\n1.临时建立好原先10倍或者20倍的queue数量(新建一个topic，partition是原来的10倍)；\n2.然后写一个临时分发消息的consumer程序，这个程序部署上去消费积压的消息，消费之后不做耗时处理，直接均匀轮询写入临时建好分10数量的queue里面；\n3.紧接着征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的消息；\n4.这种做法相当于临时将queue资源和consumer资源扩大10倍，以正常速度的10倍来消费消息。\n5.等快速消费完了之后，恢复原来的部署架构，重新用原来的consumer机器来消费消息。\n\n改进：\n1.consumer程序可以写在服务里面；\n2.指定一个“预案topic”，在服务中预先写好对“预案topic”；\n3.采用策略模式进行”业务topic“-&gt;“预案topic”的转换。\n注意：\n1.如果涉及数据一致性问题，需要通过数据比对、对账等功能进行校验；\n2.需要有个单独的topic转换服务，或修改服务代码，或在事前将多线程逻辑写好。\nKafka消费异常导致消费阻塞问题描述：某个消息消费异常或者某个操作较为耗时，导致单个pod的消费能力下降，甚至产生阻塞。\n方案：设置偏移量；开关多线程的消费策略。\n设置偏移量1.调整偏移量：联系运维，将offset后移一位；\n2.消息补推：针对跳过的消息或某个时间段内的数据进行消息补推；3.如果涉及数据一致性问题，需要通过数据比对、对账等功能进行校验。\n开关多线程的消费策略参考上面的“可配置多线程的消费策略”，在发生阻塞时开启多线程消费开关。\n注：需要修改代码或者在事前将多线程逻辑写好\nKafka消息丢失预案问题描述：服务没有按照预期消费到kafka消息，导致业务产生问题。\n方案：根因分析；消息补推。\n根因分析1.生产端是否成功发送消费（源头丢失）\nBroker丢失消息：Kafka为了得到更高的性能和吞吐量，将数据异步批量的存储在磁盘中，异步刷盘有肯能造成源头数据丢失；\nProducer丢失消息：发送逻辑存在Bug，导致消息为发送成功。\n解决：需要检查生产端与集群健康性；消息补发。\n2.是否被成功消费\nConsumer自动提交的机制是根据一定的时间间隔，将收到的消息进行commit。commit过程和消费消息的过程是异步的。也就是说，可能存在消费过程未成功（比如抛出异常），commit消息已经提交了。\n此外，如果消费逻辑有bug，也导致消息丢失的假象。\n解决：修复问题，视情况修改消费确认机制。\n3.是否有其他服务共用了同一个消费组\n多服务误用同一个消费组会导致消息一定比率或规律性丢失。\n例如，创建用户的kafka消息，可能价格中心和促销服务误用了一个消费组，导致每个服务都是消费了部分消息，导致一些问题出现偶现的情况。\n解决：修改配置，重启服务，各种建立的消费组；事前需要有检查是否有多个服务共用一个消费的情况（检测+比对）。\n消息补推1.通过业务影响查询影响的数据信息；\n2.构建kafka消息，进行消息补偿；\n3.如果涉及数据一致性问题，需要通过数据比对、对账等功能进行校验。\n针对每个对外发送的服务，生产端一般都需要有较为完善的消息补推接口，并且消费端也需要保障消息消费的幂等。\n其他\nKafka成本控制机器、存储和网络\n机器需要重新评估你的实例类型决策：你的集群是否饱和？在什么情况下饱和？是否存在其他实例类型，可能比你第一次创建集群时选择的类型更合适？EBS 优化实例与 GP2&#x2F;3 或 IO2 驱动器的混合是否真的比 i3 或 i3en 机器（及其带来的优势）有更好的性价比？\n存储与网络压缩在 Kafka 中并不新鲜，大多数用户已经知道了自己可以在 GZIP、Snappy 和 LZ4 之间做出选择。但自从KIP-110被合并进 Kafka，并添加了用于 Zstandard 压缩的压缩器后，它已实现了显著的性能改进，并且是降低网络成本的完美方式。\n以生产者端略高的 CPU 使用率为代价，你将获得更高的压缩率并在线上“挤进”更多信息。\nAmplitude在他们的帖子中介绍，在切换到 Zstandard 后，他们的带宽使用量减少了三分之二，仅在处理管道上就可以节省每月数万美元的数据传输成本。\n集群不平衡的集群可能会损害集群性能，导致某些 borker 比其他 broker 的负载更大，让响应延迟更高，并且在某些情况下会导致这些 broker 的资源饱和，从而导致不必要的扩容，进而会影响集群成本。\n此外，不平衡集群还面临一个风险：在一个 broker 出故障后出现更高的 MTTR（例如当该 broker 不必要地持有更多分区时），以及更高的数据丢失风险（想象一个复制因子为 2 的主题，其中一个节点由于启动时要加载的 segment 过多，于是难以启动）。\n消息消费的幂等定义：\n所谓幂等性，数学概念就是: f(f(x)) &#x3D; f(x) 。f函数表示对消息的处理。通俗点来讲就是，在消费者收到重复消息进行重复处理时，也要保证最终结果的一致性。\n比如，银行转账、下单等，不管重试多少次，都要保证最终结果一定是一致的。\n利用数据库的唯一约束将数据库中的多个字段联合，创建一个唯一约束，即使多次操作也能保证表里至多存在一条记录（如创建订单、创建账单、创建流水等）。\n此外，只要是支持类似“INSERT IF NOT EXIST”语义的存储类系统（如Redis的SETNX）都可以用于实现幂等消费。\n设置前置条件1.给数据变更设置一个前置条件（版本号version、updateTime）；\n2.如果满足条件就更新数据，否则拒绝更新数据；\n3.在更新数据的时候，同时变更前置条件中的数据（版本号+1、更新updateTime）。\n记录并检查操作1.给每条消息都记录一个全局唯一 ID；\n2.消费时，先根据这个全局唯一 ID 检查这条消息是否有被消费过；\n3.如果没有消费过，则更新数据，并将消费状态置为“已消费”状态。\n其中，在“检查消费状态，然后更新数据并且设置消费状态”中，三个操作必须作为一组操作保证原子性。\n\nArticle link： https://tqgoblin.site/post/csdn/如何更好地使用Kafka/  Author： Stephen  \n","slug":"csdn/如何更好地使用Kafka","date":"2022-12-01T09:31:05.000Z","categories_index":"mq","tags_index":"kafka java 大数据","author_index":"Stephen"},{"id":"23a83ea7b13859971f6150dc36d3dd3b","title":"RocketMQ介绍","content":"RocketMQ 整体架构设计        整体的架构设计主要分为四大部分，分别是：Producer、Consumer、Broker、NameServer。\n\n为了更贴合实际，我画的都是集群部署，像 Broker 我还画了主从。\n        Producer：就是消息生产者，可以集群部署。它会先和 NameServer 集群中的随机一台建立长连接，得知当前要发送的 Topic 存在哪台 Broker Master上，然后再与其建立长连接，支持多种负载平衡模式发送消息。Consumer：消息消费者，也可以集群部署。它也会先和 NameServer 集群中的随机一台建立长连接，得知当前要消息的 Topic 存在哪台 Broker Master、Slave上，然后它们建立长连接，支持集群消费和广播消费消息。Broker：主要负责消息的存储、查询消费，支持主从部署，一个 Master 可以对应多个 Slave，Master 支持读写，Slave 只支持读。Broker 会向集群中的每一台 NameServer 注册自己的路由信息。NameServer：是一个很简单的 Topic 路由注册中心，支持 Broker 的动态注册和发现，保存 Topic 和 Borker 之间的关系。通常也是集群部署，但是各 NameServer 之间不会互相通信， 各 NameServer 都有完整的路由信息，即无状态。我再用一段话来概括它们之间的交互：\n        先启动 NameServer 集群，各 NameServer 之间无任何数据交互，Broker 启动之后会向所有 NameServer 定期（每 30s）发送心跳包，包括：IP、Port、TopicInfo，NameServer 会定期扫描 Broker 存活列表，如果超过 120s 没有心跳则移除此 Broker 相关信息，代表下线。\n        这样每个 NameServer 就知道集群所有 Broker 的相关信息，此时 Producer 上线从 NameServer 就可以得知它要发送的某 Topic 消息在哪个 Broker 上，和对应的 Broker （Master 角色的）建立长连接，发送消息。\n        Consumer 上线也可以从 NameServer 得知它所要接收的 Topic 是哪个 Broker ，和对应的 Master、Slave 建立连接，接收消息。\n        简单的工作流程如上所述，相信大家对整体数据流转已经有点印象了，我们再来看看每个部分的详细情况。\nNameServer        它的特点就是轻量级，无状态。角色类似于 Zookeeper 的情况，从上面描述知道其主要的两个功能就是：Broker 管理、路由信息管理。\n总体而言比较简单，我再贴一些字段，让大家有更直观的印象知道它存储了些什么。\n\nProducer        Producer 无非就是消息生产者，那首先它得知道消息要发往哪个 Broker ，于是每 30s 会从某台 NameServer 获取 Topic 和 Broker 的映射关系存在本地内存中，如果发现新的 Broker 就会和其建立长连接，每 30s 会发送心跳至 Broker 维护连接。并且会轮询当前可以发送的 Broker 来发送消息，达到负载均衡的目的，在同步发送情况下如果发送失败会默认重投两次（retryTimesWhenSendFailed &#x3D; 2），并且不会选择上次失败的 broker，会向其他 broker 投递。在异步发送失败的情况下也会重试，默认也是两次 （retryTimesWhenSendAsyncFailed &#x3D; 2），但是仅在同一个 Broker 上重试。\nProducer 启动流程然后我们再来看看 Producer 的启动流程看看都干了些啥。\n\n        大致启动流程图中已经表明的很清晰的，但是有些细节可能还不清楚，比如重平衡啊，TBW102 啥玩意啊，有哪些定时任务啊，别急都会提到的。\n有人可能会问这生产者为什么要启拉取服务、重平衡？\n        因为 Producer 和 Consumer 都需要用 MQClientInstance，而同一个 clientId 是共用一个 MQClientInstance 的， clientId 是通过本机 IP 和 instanceName（默认值 default）拼起来的，所以多个 Producer 、Consumer 实际用的是一个MQClientInstance。\n至于有哪些定时任务，请看下图：\n\nProducer 发消息流程        我们再来看看发消息的流程，大致也不是很复杂，无非就是找到要发送消息的 Topic 在哪个 Broker 上，然后发送消息。\n\n        现在就知道 TBW102 是啥用的，就是接受自动创建主题的 Broker 启动会把这个默认主题登记到 NameServer，这样当 Producer 发送新 Topic 的消息时候就得知哪个 Broker 可以自动创建主题，然后发往那个 Broker。\n        而 Broker 接受到这个消息的时候发现没找到对应的主题，但是它接受创建新主题，这样就会创建对应的 Topic 路由信息。\n自动创建主题的弊端        自动创建主题那么有可能该主题的消息都只会发往一台 Broker，起不到负载均衡的作用。\n        因为创建新 Topic 的请求到达 Broker 之后，Broker 创建对应的路由信息，但是心跳是每 30s 发送一次，所以说 NameServer 最长需要 30s 才能得知这个新 Topic 的路由信息。\n        假设此时发送方还在连续快速的发送消息，那 NameServer 上其实还没有关于这个 Topic 的路由信息，所以有机会让别的允许自动创建的 Broker 也创建对应的 Topic 路由信息，这样集群里的 Broker 就能接受这个 Topic 的信息，达到负载均衡的目的，但也有个别 Broker 可能，没收到。\n        如果发送方这一次发了之后 30s 内一个都不发，之前的那个 Broker 随着心跳把这个路由信息更新到 NameServer 了，那么之后发送该 Topic 消息的 Producer 从 NameServer 只能得知该 Topic 消息只能发往之前的那台 Broker ，这就不均衡了，如果这个新主题消息很多，那台 Broker 负载就很高了。\n所以不建议线上开启允许自动创建主题，即 autoCreateTopicEnable 参数。\n发送消息故障延迟机制        有一个参数是 sendLatencyFaultEnable，默认不开启。这个参数的作用是对于之前发送超时的 Broker 进行一段时间的退避。\n        发送消息会记录此时发送消息的时间，如果超过一定时间，那么此 Broker 就在一段时间内不允许发送。\n\n比如发送时间超过 15000ms 则在 600000 ms 内无法向该 Broker 发送消息。\n这个机制其实很关键，发送超时大概率表明此 Broker 负载高，所以先避让一会儿，让它缓一缓，这也是实现消息发送高可用的关键。\n小结一下\n\nProducer 每 30s 会向 NameSrv 拉取路由信息更新本地路由表，有新的 Broker 就和其建立长连接，每隔 30s 发送心跳给 Broker 。\n不要在生产环境开启 autoCreateTopicEnable。\nProducer 会通过重试和延迟机制提升消息发送的高可用。\n\nBroker        Broker 就比较复杂一些了，但是非常重要。大致分为以下五大模块，我们来看一下官网的图。\n\nRemoting 远程模块，处理客户请求。Client Manager 管理客户端，维护订阅的主题。Store Service 提供消息存储查询服务。HA Serivce，主从同步高可用。Index Serivce，通过指定key 建立索引，便于查询。有几个模块没啥可说的就不分析了，先看看存储的。\nBroker 的存储        RocketMQ 存储用的是本地文件存储系统，效率高也可靠。\n主要涉及到三种类型的文件，分别是 CommitLog、ConsumeQueue、IndexFile。\nCommitLog        RocketMQ 的所有主题的消息都存在 CommitLog 中，单个 CommitLog 默认 1G，并且文件名以起始偏移量命名，固定 20 位，不足则前面补 0，比如 00000000000000000000 代表了第一个文件，第二个文件名就是 00000000001073741824，表明起始偏移量为 1073741824，以这样的方式命名用偏移量就能找到对应的文件。\n        所有消息都是顺序写入的，超过文件大小则开启下一个文件。\nConsumeQueue        ConsumeQueue 消息消费队列，可以认为是 CommitLog 中消息的索引，因为 CommitLog 是糅合了所有主题的消息，所以通过索引才能更加高效的查找消息。\n        ConsumeQueue 存储的条目是固定大小，只会存储 8 字节的 commitlog 物理偏移量，4 字节的消息长度和 8 字节 Tag 的哈希值，固定 20 字节。\n        在实际存储中，ConsumeQueue 对应的是一个Topic 下的某个 Queue，每个文件约 5.72M，由 30w 条数据组成。\n        消费者是先从 ConsumeQueue 来得到消息真实的物理地址，然后再去 CommitLog 获取消息。\nIndexFile        IndexFile 就是索引文件，是额外提供查找消息的手段，不影响主流程。\n        通过 Key 或者时间区间来查询对应的消息，文件名以创建时间戳命名，固定的单个 IndexFile 文件大小约为400M，一个 IndexFile 存储 2000W个索引。\n我们再来看看以上三种文件的内容是如何生成的：\n\n        消息到了先存储到 Commitlog，然后会有一个 ReputMessageService 线程接近实时地将消息转发给消息消费队列文件与索引文件，也就是说是异步生成的。\n消息刷盘机制        RocketMQ 提供消息同步刷盘和异步刷盘两个选择，关于刷盘我们都知道效率比较低，单纯存入内存中的话效率是最高的，但是可靠性不高，影响消息可靠性的情况大致有以下几种：\n        Broker 被暴力关闭，比如 kill -9Broker 挂了操作系统挂了机器断电机器坏了，开不了机磁盘坏了如果都是 1-4 的情况，同步刷盘肯定没问题，异步的话就有可能丢失部分消息，5 和 6就得依靠副本机制了，如果同步双写肯定是稳的，但是性能太差，如果异步则有可能丢失部分消息。\n所以需要看场景来使用同步、异步刷盘和副本双写机制。\n页缓存与内存映射        Commitlog 是混合存储的，所以所有消息的写入就是顺序写入，对文件的顺序写入和内存的写入速度基本上没什么差别。并且 RocketMQ 的文件都利用了内存映射即 Mmap，将程序虚拟页面直接映射到页缓存上，无需有内核态再往用户态的拷贝，来看一下我之前文章画的图。\n\n        页缓存其实就是操作系统对文件的缓存，用来加速文件的读写，也就是说对文件的写入先写到页缓存中，操作系统会不定期刷盘（时间不可控），对文件的读会先加载到页缓存中，并且根据局部性原理还会预读临近块的内容。其实也是因为使用内存映射机制，所以 RocketMQ 的文件存储都使用定长结构来存储，方便一次将整个文件映射至内存中。\n文件预分配和文件预热        而内存映射也只是做了映射，只有当真正读取页面的时候产生缺页中断，才会将数据真正加载到内存中，所以 RocketMQ 做了一些优化，防止运行时的性能抖动。\n文件预分配        CommitLog 的大小默认是1G，当超过大小限制的时候需要准备新的文件，而 RocketMQ 就起了一个后台线程 AllocateMappedFileService，不断的处理 AllocateRequest，AllocateRequest 其实就是预分配的请求，会提前准备好下一个文件的分配，防止在消息写入的过程中分配文件，产生抖动。\n文件预热        有一个 warmMappedFile 方法，它会把当前映射的文件，每一页遍历多去，写入一个0字节，然后再调用mlock 和 madvise(MADV_WILLNEED)。\n\nmlock：可以将进程使用的部分或者全部的地址空间锁定在物理内存中，防止其被交换到 swap 空间。\nmadvise：给操作系统建议，说这文件在不久的将来要访问的，因此，提前读几页可能是个好主意。\n\n小结一下\n\nCommitLog 采用混合型存储，也就是所有 Topic 都存在一起，顺序追加写入，文件名用起始偏移量命名。\n消息先写入 CommitLog 再通过后台线程分发到 ConsumerQueue 和 IndexFile 中。\n消费者先读取 ConsumerQueue 得到真正消息的物理地址，然后访问 CommitLog 得到真正的消息。\n利用了 mmap 机制减少一次拷贝，利用文件预分配和文件预热提高性能。\n提供同步和异步刷盘，根据场景选择合适的机制。\n\nBroker 的 HA        从 Broker 会和主 Broker 建立长连接，然后获取主 Broker commitlog 最大偏移量，开始向主 Broker 拉取消息，主 Broker 会返回一定数量的消息，循环进行，达到主从数据同步。\n        消费者消费消息会先请求主 Broker ，如果主 Broker 觉得现在压力有点大，则会返回从 Broker 拉取消息的建议，然后消费者就去从服务器拉取消息。\nConsumer消费有两种模式，分别是广播模式和集群模式。\n\n广播模式：一个分组下的每个消费者都会消费完整的Topic 消息。\n集群模式：一个分组下的消费者瓜分消费Topic 消息。\n\n一般我们用的都是集群模式。\n        而消费者消费消息又分为推和拉模式，详细看我这篇文章消息队列推拉模式，分别从源码级别分析了 RokcetMQ 和 Kafka 的消息推拉，以及推拉模式的优缺点。\nConsumer 端的负载均衡机制        Consumer 会定期的获取 Topic 下的队列数，然后再去查找订阅了该 Topic 的同一消费组的所有消费者信息，默认的分配策略是类似分页排序分配。\n        将队列排好序，然后消费者排好序，比如队列有 9 个，消费者有 3 个，那消费者-1 消费队列 0、1、2 的消息，消费者-2 消费队列 3、4、5，以此类推。\n        所以如果负载太大，那么就加队列，加消费者，通过负载均衡机制就可以感知到重平衡，均匀负载。\nConsumer 消息消费的重试        难免会遇到消息消费失败的情况，所以需要提供消费失败的重试，而一般的消费失败要么就是消息结构有误，要么就是一些暂时无法处理的状态，所以立即重试不太合适。\n        RocketMQ 会给每个消费组都设置一个重试队列，Topic 是 %RETRY%+consumerGroup，并且设定了很多重试级别来延迟重试的时间。\n        为了利用 RocketMQ 的延时队列功能，重试的消息会先保存在 Topic 名称为“SCHEDULE_TOPIC_XXXX”的延迟队列，在消息的扩展字段里面会存储原来所属的 Topic 信息。\n        delay 一段时间后再恢复到重试队列中，然后 Consumer 就会消费这个重试队列主题，得到之前的消息。\n        如果超过一定的重试次数都消费失败，则会移入到死信队列，即 Topic %DLQ%“ + ConsumerGroup 中，存储死信队列即认为消费成功，因为实在没辙了，暂时放过。\n然后我们可以通过人工来处理死信队列的这些消息。\n消息的全局顺序和局部顺序        全局顺序就是消除一切并发，一个 Topic 一个队列，Producer 和 Consuemr 的并发都为一。局部顺序其实就是指某个队列顺序，多队列之间还是能并行的。\n        可以通过 MessageQueueSelector 指定 Producer 某个业务只发这一个队列，然后 Comsuer 通过MessageListenerOrderly 接受消息，其实就是加锁消费。\n        在 Broker 会有一个 mqLockTable ，顺序消息在创建拉取消息任务的时候需要在 Broker 锁定该消息队列，之后加锁成功的才能消费。\n        而严格的顺序消息其实很难，假设现在都好好的，如果有个 Broker 宕机了，然后发生了重平衡，队列对应的消费者实例就变了，就会有可能会出现乱序的情况，如果要保持严格顺序，那此时就只能让整个集群不可用了。\n一些注意点1、订阅消息是以 ConsumerGroup 为单位存储的，所以ConsumerGroup 中的每个 Consumer 需要有相同的订阅。\n        因为订阅消息是随着心跳上传的，如果一个 ConsumerGroup 中 Consumer 订阅信息不一样，那么就会出现互相覆盖的情况。\n        比如消费者 A 订阅 Topic a，消费者 B 订阅 Topic b，此时消费者 A 去 Broker 拿消息，然后 B 的心跳包发出了，Broker 更新了，然后接到 A 的请求，一脸懵逼，没这订阅关系啊。\n2、RocketMQ 主从读写分离\n        从只能读，不能写，并且只有当前客户端读的 offset 和 当前 Broker 已接受的最大 offset 超过限制的物理内存大小时候才会去从读，所以正常情况下从分担不了流量。\n3、单单加机器提升不了消费速度，队列的数量也需要跟上。\n4、之前提到的，不要允许自动创建主题\nRocketMQ 的最佳实践这些最佳实践部分参考自官网。\nTags的使用\n        建议一个应用一个 Topic，利用 tages 来标记不同业务，因为 tages 设置比较灵活，且一个应用一个 Topic 很清晰，能直观的辨别。\nKeys的使用\n        如果有消息业务上的唯一标识，请填写到 keys 字段中，方便日后的定位查找。\n提高 Consumer 的消费能力1、提高消费并行度：增加队列数和消费者数量，提高单个消费者的并行消费线程，参数 consumeThreadMax。\n2、批处理消费，设置 consumeMessageBatchMaxSize 参数，这样一次能拿到多条消息，然后比如一个 update语句之前要执行十次，现在一次就执行完。\n3、跳过非核心的消息，当负载很重的时候，为了保住那些核心的消息，设置那些非核心的消息，例如此时消息堆积 1W 条了之后，就直接返回消费成功，跳过非核心消息。\nNameServer 的寻址请使用 HTTP 静态服务器寻址（默认），这样 NameServer 就能动态发现。\nJVM选项以下抄自官网：\n        如果不关心 RocketMQ Broker的启动时间，通过“预触摸” Java 堆以确保在 JVM 初始化期间每个页面都将被分配。\n        那些不关心启动时间的人可以启用它：-XX:+AlwaysPreTouch禁用偏置锁定可能会减少JVM暂停， -XX:-UseBiasedLocking至于垃圾回收，建议使用带JDK 1.8的G1收集器。\n        -XX:+UseG1GC -XX:G1HeapRegionSize&#x3D;16m-XX:G1ReservePercent&#x3D;25-XX:InitiatingHeapOccupancyPercent&#x3D;30\n        另外不要把-XX:MaxGCPauseMillis的值设置太小，否则JVM将使用一个小的年轻代来实现这个目标，这将导致非常频繁的minor GC，所以建议使用rolling GC日志文件:\n        -XX:+UseGCLogFileRotation-XX:NumberOfGCLogFiles&#x3D;5-XX:GCLogFileSize&#x3D;30m\nLinux内核参数以下抄自官网：\n\nvm.extra_free_kbytes，告诉VM在后台回收（kswapd）启动的阈值与直接回收（通过分配进程）的阈值之间保留额外的可用内存。RocketMQ使用此参数来避免内存分配中的长延迟。（与具体内核版本相关）\nvm.min_free_kbytes，如果将其设置为低于1024KB，将会巧妙的将系统破坏，并且系统在高负载下容易出现死锁。\nvm.max_map_count，限制一个进程可能具有的最大内存映射区域数。RocketMQ将使用mmap加载CommitLog和ConsumeQueue，因此建议将为此参数设置较大的值。（agressiveness –&gt; aggressiveness）\nvm.swappiness，定义内核交换内存页面的积极程度。较高的值会增加攻击性，较低的值会减少交换量。建议将值设置为10来避免交换延迟。\nFile descriptor limits，RocketMQ需要为文件（CommitLog和ConsumeQueue）和网络连接打开文件描述符。我们建议设置文件描述符的值为655350。\nDisk scheduler，RocketMQ建议使用I&#x2F;O截止时间调度器，它试图为请求提供有保证的延迟。\n\nArticle link： https://tqgoblin.site/post/csdn/RocketMQ介绍/  Author： Stephen  \n","slug":"csdn/RocketMQ介绍","date":"2022-11-10T09:36:17.000Z","categories_index":"mq","tags_index":"面试 面试 java-rocketmq","author_index":"Stephen"},{"id":"a66c39428303def8c1769f2fae79ffad","title":"RocketMQ与kafka的区别","content":"一、前言        淘宝内部的交易系统使用了淘宝自主研发的Notify消息中间件，使用MySQL作为消息存储媒介，支持水平扩容。为了进一步降低成本，阿里中间件团队认为Notify可进一步优化。\n        2011年初，Linkedin开源了kafka, 阿里中间件团队在对kafka做了充分的review之后，被kafka的无限消息堆积能力、高效的持久化速度深深吸引，但同时发现kafka主要定位于日志传输，对于使用在淘宝交易、订单、充值等场景下，还有若干特性不满足。因此，阿里中间件团队基于Java重新编写了RocketMQ，定位于不仅限于日志场景的可靠消息传输。\n        目前，RocketMQ在阿里集团被广泛应用于订单、充值、交易、流计算、消息推送、日志流式处理、binlog分发等场景。\n二、RocketMQ与kafka的不同1、数据可靠性RocketMQ：支持异步实时刷盘、同步刷盘、同步复制、异步复制。kafka：使用异步刷盘方式，异步复制&#x2F;同步复制。\n总结：1、RocketMQ支持kafka所不具备的“同步刷盘”功能，在单机可靠性上比kafka更高，不会因为操作系统Crash而导致数据丢失。2、kafka的同步replication理论上性能低于RocketMQ的replication，这是因为kafka的数据以partition为单位，这样一个kafka实例上可能多上百个partition。而一个RocketMQ实例上只有一个partition，RocketMQ可以充分利用IO组的commit机制，批量传输数据。同步replication与异步replication相比，同步replication性能上损耗约20%-30%。\n一句话概括：RocketMQ新增了同步刷盘机制，保证了可靠性；一个RocketMQ实例只有一个partition, 在replication时性能更好。\n2、性能对比1、kafka单机写入TPS月在百万条&#x2F;秒，消息大小为10个字节。2、RocketMQ单机写入TPS单实例约7万条&#x2F;秒，若单机部署3个broker，可以跑到最高12万条&#x2F;秒，消息大小为10个字节。\n总结：kafka的单机TPS能跑到每秒上百万，是因为Producer端将多个小消息合并，批量发向broker。\n那么RocketMQ为什么没有这样做呢？\n发送消息的Producer通常是用Java语言，缓存过多消息，GC是个很严重的问题。（问题：难道kafka用scala不需要GC？）Producer发送消息到broker, 若消息发送出去后，未达到broker，就通知业务消息发送成功，若此时Broker宕机，则会导致消息丢失，从而导致业务出错。Producer通常为分布式系统，且每台机器都是多线程发送，通常来说线上单Producer产生的消息数量不会过万。消息合并功能完全可由上层业务来做。一句话概括：RocketMQ写入性能上不如kafka, 主要因为kafka主要应用于日志场景，而RocketMQ应用于业务场景，为了保证消息必达牺牲了性能，且基于线上真实场景没有在RocketMQ层做消息合并，推荐在业务层自己做。\n3、单机支持的队列数1、kafka单机若超过了64个partition&#x2F;队列，CPU load会发生明显飙高，partition越多，CPU load越高，发消息的响应时间变长。2、RocketMQ单机支持最高5万个队列，CPU load不会发生明显变化。\n队列多有什么好处呢？1、单机可以创建更多个topic, 因为每个topic都是有一组队列组成。2、消费者的集群规模和队列数成正比，队列越多，消费类集群可以越大。\n一句话概括：RocketMQ支持的队列数远高于kafka支持的partition数，这样RocketMQ可以支持更多的consumer集群。\n4、消息投递的实时性1、kafka采用短轮询的方式，实时性取决于轮询时间间隔，0.8以后版本支持长轮询。2、RocketMQ使用长轮询，同Push实时性一致，消息投递的延迟通常在几毫秒内，\n一句话：kafka与RocketMQ都支持长轮询，消息投递的延迟在几毫秒内。\n5、消费失败重试1、kafka不支持消费失败重试。2、RocketMQ消费失败支持定时重试，每次重试间隔时间顺延。\n总结：以充值类应用为例，若当前时刻调用运营商网管失败，可能运营商网关此时压力过大，稍后再调用就会成功。这里的重试指可靠的重试，即失败重试的消息不是因为consumer宕机而导致的消息丢失。\n一句话概括：RocketMQ支持消费失败重试功能，主要用于第一次调用不成功，后面可调用成功的场景。而kafka不支持消费失败重试。\n6、严格保证消息有序1、kafka可保证同一个partition上的消息有序，但一旦broker宕机，就会产生消息乱序。2、Rocket支持严格的消息顺序，一台broker宕机，发送消息会失败，但不会乱序。举例：MySQL的二进制日志分发需要保证严格的顺序。\n一句话概括：kafka不保证消息有序，RocketMQ可保证严格的消息顺序，即使单台Broker宕机，仅会造成消息发送失败，但不会消息乱序。\n7、定时消息1、kafka不支持定时消息2、开源版本的RocketMQ仅支持定时级别，定时级别用户可定制\n8、分布式事务消息1、kafka不支持分布式事务消息2、RocketMQ支持分布式事务消息。\n9、消息查询1、kafka不支持消息查询2、RocketMQ支持根据消息标识（发送消息时指定一个消息key, 任意字符串，如指定为订单编号）查询消息，也支持根据消息内容查询消息。\n总结：消息查询功能对于定位消息丢失问题非常有用，例如某个订单处理失败，可用此功能查询是消息没收到，还是收到了但处理出错了。\n一句话概括：RocketMQ支持按消息标识或消息内容查询消息，用于排查消息丢失问题；kafka不支持消息查询。\n10、消息回溯1、kafka可按照消息的offset来回溯消息2、RocketMQ支持按照时间来回溯消息，精度到毫秒，例如从一天的几点几分几秒几毫秒来重新消费消息。\n总结：RocketMQ按时间做回溯消息的典型应用场景为，consumer做订单分析，但是由于程序逻辑或依赖的系统发生故障等原因，导致今天处理的消息全部无效，需要从昨天的零点重新处理。\n11、消息并行度1、kafka的消息并行度，依赖于topic里配置的partition数，如果partition数为10，那么最多10台机器来消费，每台机器只能开启一个线程；或者一台机器消费，最多开启10个线程。消费的并行度与partition个数一致。2、RocketMQ并行消费分两种情况：1）顺序消费方式的并行度与kafka一致。2）乱序消费方式的并行度取决于consumer的线程数，如topic配置10个队列，10台机器消费，每台机器100个线程，那么并行度为1000。\n一句话概括：kafka的消费并行度等于partition数；RocketMQ的消费并行度等于消费的线程数，不受队列数限制。\n12、开发语言1、kafka采用scala开发2、RocketMQ采用Java开发\n13、消息堆积能力kafka比RocketMQ的消息堆积能力更强，不过RocketMQ单机也可支持亿级的消息积压能力，这个堆积能力也能够完全满足业务需求。\n14、开源社区活跃度1、kafka社区更新较慢2、RocketMQ的Github社区有250人，公司用户登记了联系方式，QQ群超过1000人，3、kafka原开发团队成立了新公司，暂时未看到相关产品。4、RocketMQ已在阿里云商业化，目前以云服务形式供外部商用，并向用户承诺99.99%的可靠性，同时彻底解决了用户自己搭建MQ产品的运维复杂性问题。\n15、应用领域成熟度1、kafka在日志领域比较成熟2、RocketMQ在阿里集团内部有大量的应用在使用，并顺利支持了多次天猫双十一的考验。\n三、总结kafka和RocketMQ的总体区别是，kafka设计初衷是用于日志传输，而RocketMQ的设计用于解决各类应用可靠的消息传输，阿里云官网承诺RocketMQ数据可靠性为10个9，服务可靠性为99.95%。\nkafka相比RocketMQ的优势1、单机吞吐量TPS可上百万，远高于RocketMQ的TPS7万每秒，适用于日志类消息。2、kafka支持多语言的客户端\nRocketMQ相比kafka的优势**1、保证消息不丢（ 数据可靠性达10个9）2、可严格保证消息有序3、支持分布式事务消息4、支持按时间做消息回溯（可精确到毫秒级）5、支持按标识和内容查询消息，用于排查丢消息6、支持消费失败重试7、可支持更多的partition, 即更多的消费线程数————————————————版权声明：本文为CSDN博主「Shi Peng」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/shijinghan1126/article/details/104724407\nArticle link： https://tqgoblin.site/post/csdn/RocketMQ与kafka的区别/  Author： Stephen  \n","slug":"csdn/RocketMQ与kafka的区别","date":"2022-11-10T09:13:06.000Z","categories_index":"mq","tags_index":"面试 kafka 面试","author_index":"Stephen"},{"id":"4a7935236206ce0c80a1379bd98b2262","title":"kafka的rebalance机制","content":"一、Reblance是什么        Reblance就像他的名称一样，意思是再平衡，平衡什么？平衡消费者和分区之间的对应关系。本质上来讲，Reblance是一种协议，规定了一个Consumer Group下所有Consumer如何达成一致，来分配订阅Topic的每个分区，尽量让每个消费者分配到相对均匀的分区，使Consumer的资源都能得到充分利用，防止有些Consumer比较忙，有的Consumer比较闲。\n二、Reblance触发的时机        当kafka感知到存在让分区和消费者分配不均匀的事情发生时，就会触发Reblance，来保证分区和消费者再次平衡。那么那些事情会触发Reblance呢？或者说那些事情会导致分区和消费者分配不均匀呢？主要有三种：\n\n消费者组消费的分区个数发生变化。\n消费者组消费的主题个数发生变化。\n消费者组内的消费者个数发生变化。\n\n        其实第2种情况，本质上是第1种情况的一个特例，消费组消费的主题个数发生变化，体现到消费组中的消费者身上，就是费配到消费者上的分区个数发生了变化。\n三、Reblance的执行流程理解Reblance的流程，需要先了解一下消费组协调者。\n        协调者(Coordinator)是broker进程中的一个组件，每一个broker都会有一个Coordinator，协调者的职责在于：服务ConsumerGroup，完成整个Reblance的过程，提供Consumer Group位移的管理，以及组内成员的管理。\n这里有几个需要注意的问题：1.Consumer Group如何知道哪个协调者是为自己服务的？        Consumer Group的位移保存在__consumer_offset主题的某个partition中，这个partition是 hash(group_id)%50(__consumer_offset主题有50个分区)，然后这个partition的leader副本所在的broker上的coordintor就是该Consumer Group的协调者。\n2.协调者如何管理组成员？        当Consumer启动的时候，会向协调者所在broker发送多种请求，其中包含一个Join Group的请求，收到请求后，协调者执行消费者组的注册，消费者元数据信息保存。同时组中的各个消费者，都会定时的向协调者发送心跳请求，让协调者了解每个消费者的状态信息。\n        当协调者发现某个消费者长时间没有发送心跳，那么协调者就会认为这个消费者挂了，把这个消费者从消费者组中剔除出去，然后在其他Consumer的心跳请求中回复 reblance_need,让每个消费者停止消息消费，并开启Reblance。\n        当各个消费者收到reblance_need响应后，都会停止消息消费，并向协调者发送 SyncGroup请求，来询问分配给自己的分区信息。\n四、分区分配的策略有哪些        分配策略是指：当Reblance触发时，重新将分区分配给消费者的方式。常用的分区方式有三种：Range，RoundRobin 和 StickyAssignor。\n        为了方便描述分区分配的流程，我们假设以下场景：一个消费组中有3个消费者，分别为Consumer_1,Consumer_2和Consumer_3。消费7个分区，分别为partition_0-7。\n\nRange分配策略是指按照分区号的范围进行分区分配。\nRoundRobin分配策略是指轮训每个分区，将分区逐个分配给消费者\nStickyAssignor分区策略，又称为粘性分配，所谓的有粘性，是指每次 Rebalance 时，该策略会尽可能地保留之前的分配方案，尽量实现分区分配的最小变动。\n\n        例如实例Consumer1之前负责消费分区 0、1、2，那么 Rebalance 之后，如果可能的话，最好还是让实例 Consumer1 继续消费分区0、1、2，而不是被重新分配其他的分区。这样的话，实例 Consumer1 连接这些分区所在 Broker 的 TCP 连接就可以继续用，不用重新创建连接其他 Broker 的 Socket 资源。\n不过这个分配策略要到 kafka 0.11版本才可以使用。\n五、Reblance产生的影响Reblance产生的影响主要有两个：\n1.影响消费效率。        因为发生Reblance时，所有消费者都会停止消费，比较影响消息消费效率，当一个消费组中消费者比较多的时候，Reblance的过程会比较耗时。\n2.可能会产生消息重复消费        因为Consumer消费分区消息的offset提交过程，不是实时的(以offset自动提交为例)，由参数auto.commit.interval.ms控制提交的最小频率，默认是5000，也就是最少每5s提交一次。我们试想以下场景：提交位移之后的 3 秒发生了 Rebalance ，在 Rebalance 之后，所有 Consumer 从上一次提交的位移处继续消费，但该位移已经是 3 秒前的位移数据了，故在 Rebalance 发生前 3 秒消费的所有数据都要重新再消费一次。虽然可以通过减少 auto.commit.interval.ms 的值来提高提交频率，但这么做只能缩小重复消费的时间窗口，不可能完全消除它。\n很遗憾的是，目前kafka社区对于Reblance带来的影响，也没有彻底的解决办法。只能通过避免不必要的Reblance，来降低Reblance产生的影响。\n六、减少Reblance的产生        有些时候，Reblance是不可避免的，比如在运维过程中，为了增加客户端的处理能力，需要增加partition个数或者consumer个数，那么不可避免的需要触发Reblance。\n        但是有些时候可能有些参数配置的问题，会导致一些不必要的Reblance的发生，这些Reblance的发生主要就是协调者错误的认为消费者实例挂了，然后触发Reblance。\n有哪些参数配置异常，会导致协调者认为消费者实例挂了呢？\n1.session.timeout.ms\n        这个参数是协调者最长等待消费者没有发送心跳的时间间隔，如果协调者在该参数指定的时间内没有收到某个消费者的心跳请求，那么就认为该消费者挂了，就会将这个消费者从组里面剔除，然后触发Reblance。\n        同时参数heartbeat.interval.ms,表示消费者向协调者发送心跳请求的时间间隔，这个参数设置的过大，会导致消费者长时间不会向协调者发送心跳，同时协调者向消费者发送的 Reblance的消息也会不及时(协调者不是主从向消费者发送Reblance消息的，而是将Reblance消息封装到消费者心跳请求的响应消息中)。设置的小一些，消息会及时一些，但是，可能会消耗过多的带宽。\n通常在生产环境中保证session.timeout.ms &gt;&#x3D; 3 * heartbeat.interval.ms。\n2.max.poll.interval.ms\n        消费者调用poll方法的时间间隔，如果时间间隔大于该参数的设置，会认为这个Consumer存在问题(消息处理效率低，消费者可能不健康)，那么Consumer会向协调者发送 leaveGroup请求，自动退出消费者组，此时消费者数量发生变化，触发Reblance。\n        在业务中，该参数可以设置的长一些，但是也不要违背了该参数的初衷(kafka对消费者的一种优胜劣汰的优化机制，poll的慢导致消息堆积)。因为业务上的确可能存在一些耗时的操作，或者poll拉去的消息过多，导致消息处理的慢，进而导致poll方法消息拉去的时间间隔过长，对于这种情况可以调整每次拉去消息个条数，或者优化消息处理逻辑，加快消息处理效率。\nArticle link： https://tqgoblin.site/post/csdn/kafka的rebalance机制/  Author： Stephen  \n","slug":"csdn/kafka的rebalance机制","date":"2022-11-10T08:01:25.000Z","categories_index":"面试","tags_index":"kafka kafka 面试","author_index":"Stephen"},{"id":"5c0f27a0ed8d73f08892501931e4fc3a","title":"HashMap底层原理","content":"\n\n一、HashMap特点：数组＋链表存储数据 线理不安全\n默认数组大小16，2倍扩容，扩容因子0.75\n\n        HashMap 基于键 HashCode 值唯一标识一条数据，根据hashCode定位到数组的具体下标，然后对存储的链表进行遍历查找到需要的数据，复杂度O(n).其中链表中每个元素都是一个Entry实例，包换4个属性，key,value,hash,next指向。\n        为了减小链表遍历的开销，java8改成数组+链表+红黑树的方式，当链表元素超过8后会将链表结构转化成红黑树，提高查询效率。时间复杂度为O(logN),长度小于8的话会重新转化成链表。\n        HashMap 非线程安全的， 即在同一时刻有多个线程同时写 HashMap 时将可能导致数据的不一致，1.7版本也有可能导致链表成环问题，查询map导致死循环cpu飙升。1.8解决了这个问题。如果需要满足线程安全的条件， 则可以用 Collections synchronizedMap方法使 HashMap 具有线程安全的能力，或者使用 ConcurrentHashMap\n二、hashMap链表成环现象（死循环）1.7hashMap链表成环的时机\n\nHashMap 扩容时。\n多线程环境下。\n\n\n\n插入的时候和平时我们追加到尾部的思路是不一致的，是链表的头结点开始循环插入，导致插入的顺序和原来链表的顺序相反的。\ntable 是共享的，table 里面的元素也是共享的，扩容的transfer方法中while 循环都直接修改 table 里面的元素的 next 指向，导致指向混乱。\n\n1.8版本解决方案\n        JDK 8 中扩容时，已经没有 JDK7 中的 transfer 方法了，而是自己重新写了扩容方法，叫做 resize，链表从老数组拷贝到新数组时的代码如下：\n\n解决办法其实代码中的注释已经说的很清楚了，我们总结一下：\n        JDK8 是等链表整个 while 循环结束后，才给数组赋值，此时使用局部变量 loHead 和 hiHead来保存链表的值，因为是局部变量，所以多线程的情况下，肯定是没有问题的。\n        为什么有 loHead 和 hiHead两个新老值来保存链表呢，主要是因为扩容后，链表中的元素的索引位置是可能发生变化的\n        jdk8采用的插入方式是尾插法，jdk7是头插法。\n三、ConcurrentHashMap        众所周知，在 Java 中，HashMap 是非线程安全的，如果想在多线程下安全的操作 map，主要有以下解决方法：\n\n第一种方法，使用Hashtable线程安全类；\n第二种方法，使用Collections.synchronizedMap方法，对方法进行加同步锁；\n第三种方法，使用并发包中的ConcurrentHashMap类；\n\n        第一种方法Hashtable 是一个线程安全的类，Hashtable 几乎所有的添加、删除、查询方法都加了synchronized同步锁！\n        第二种方法，使用Collections.synchronizedMap方法本质也是对 HashMap进行全表锁，多线程环境下性能依然也非常差\n        第三种方法，ConcurrentHashMap 类所采用的正是分段锁的思想，将 HashMap 进行切割，把 HashMap 中的哈希数组切分成小数组，每个小数组有 n 个 HashEntry 组成，其中小数组继承自ReentrantLock（可重入锁），这个小数组名叫Segment\n\n        JDK1.7 和 JDK1.8 对 ConcurrentHashMap 的实现有很大的不同\n        JDK1.8 对 HashMap 做了改造，当冲突链表长度大于 8 时，会将链表转变成红黑树结构\n\n        JDK1.8 中 ConcurrentHashMap 类取消了 Segment 分段锁，采用 CAS + synchronized 来保证并发安全，数据结构跟 jdk1.8 中 HashMap 结构类似，都是数组 + 链表（当链表长度大于 8 时，链表结构转为红黑二叉树）结构。\n        ConcurrentHashMap 中 synchronized 只锁定当前链表或红黑二叉树的首节点，只要节点 hash 不冲突，就不会产生并发，相比 JDK1.7 的 ConcurrentHashMap 效率又提升了 N 倍！\n        HashEntry和HashMap中的Entry非常类似，唯一的区别就是其中的核心数据如value 以及next都使用了volatile关键字修饰，保证了多线程环境下数据获取时的可见性！\nArticle link： https://tqgoblin.site/post/csdn/HashMap底层原理/  Author： Stephen  \n","slug":"csdn/HashMap底层原理","date":"2022-10-18T08:56:52.000Z","categories_index":"Java","tags_index":"面试 java","author_index":"Stephen"},{"id":"92e817b0edbefb52af49230332579af2","title":"Spring加载流程","content":"一、spring加载流程\n        tomcat在启动ServletContext容器的时候会发布ServletContextEvent事件，Spring就通过实现ServletContextListener接口，监听该事件来监听ServletContext的生命周期。web.xml中配置了ContextLoaderListener实现了ServletContextListener接口。对ServletContext启动监听，在initWebApplicationContext方法中会通过ServletContext获取web.xml中配置的contextConfigLocation参数值，也就是spring的xml配置\n然后会调用SpringIOC容器refresh()方法初始化容器：\n        refresh 是 AbstractApplicationContext 中的一个方法，负责初始化 ApplicationContext 容器，容器必须调用 refresh 才能正常工作。它的内部主要会调用 12 个方法，我们把它们称为 refresh 的 12 个步骤：\n\nprepareRefresh：\n一开始设置启动时间及开启启动标识\n\nEnvironment 对象的作用之一是为后续 @Value，值注入时提供键值\n\n保存 java 环境键值、保存系统环境键值、保存自定义键值\n\n\n\nobtainFreshBeanFactory\n这一步获取(或创建）BeanFactory，BeanFactory 的作用是负责 bean 的创建、依赖注入和初始化，bean 的各项特征由 BeanDefinition 定义\n\nBeanDefinition 作为 bean 的设计蓝图，规定了 bean 的特征，如单例多例、依赖关系、初始销毁方法等\n\n所有的 BeanDefinition 会存入 BeanFactory 中的 beanDefinitionMap 集合\n\nBeanDefinition 的来源有多种多样，可以是通过 xml 获得、配置类获得、组件扫描获得，也可以是编程添加\n\n\n\nprepareBeanFactory\n这一步会进一步完善 BeanFactory，为它的各项成员变量赋值，并应用 ApplicationContext 提供的 Environment 完成 ${ } 解析\n\n\npostProcessBeanFactory\n这一步是空实现，留给子类扩展。\n\n一般 Web 环境的 ApplicationContext 都要利用它注册新的 Scope，完善 Web 下的 BeanFactory\n\n这里体现的是模板方法设计模式\n\n\n\ninvokeBeanFactoryPostProcessors\n这一步会调用 beanFactory 后处理器\n\nbeanFactory 后处理器，充当 beanFactory 的扩展点，可以用来补充或修改 BeanDefinition\n\n常见的 beanFactory 后处理器有：ConfigurationClassPostProcessor – 解析 @Configuration、@Bean、@Import、@PropertySource 等；PropertySourcesPlaceHolderConfigurer – 替换 BeanDefinition 中的 ${ }；MapperScannerConfigurer – 补充 Mapper 接口对应的 BeanDefinition。\n\n\n\nregisterBeanPostProcessors\n这一步是继续从 beanFactory 中找出 bean 后处理器，添加至 beanPostProcessors 集合中\n\nbean 后处理器，充当 bean 的扩展点，可以工作在 bean 的实例化、依赖注入、初始化阶段，常见的有：AutowiredAnnotationBeanPostProcessor 功能有：解析 @Autowired，@Value 注解；CommonAnnotationBeanPostProcessor 功能有：解析 @Resource，@PostConstruct，@PreDestroy；AnnotationAwareAspectJAutoProxyCreator 功能有：为符合切点的目标 bean 自动创建代理。\n\n\n\ninitMessageSource\n这一步是为 ApplicationContext 添加 messageSource 成员，实现国际化功能\n\n去 beanFactory 内找名为 messageSource 的 bean，如果没有，则提供空的 MessageSource 实现\n\n\n\ninitApplicationEventMulticaster\n这一步为 ApplicationContext 添加事件广播器成员，即 applicationContextEventMulticaster，它的作用是发布事件给监听器\n\n去 beanFactory 找名为 applicationEventMulticaster 的 bean 作为事件广播器，若没有，会创建默认的事件广播器\n\n\n\nonRefresh\n这一步是空实现，留给子类扩展\n\nSpringBoot 中的子类在这里准备了 WebServer，即内嵌 web 容器\n\n体现的是模板方法设计模式\n\n\n\nregisterListeners1.  这一步会从多种途径找到事件监听器，并添加至 applicationEventMulticaster用来接收事件广播器发布的事件\n\ntxt2.  有如下来源：事先编程添加的；来自容器中的 bean；来自于 \\@EventListener 的解析；要实现事件监听器，只需要实现 ApplicationListener 接口，重写其中 onApplicationEvent\\(E e\\) 方法即可；\nfinishBeanFactoryInitialization1.  这一步会将 beanFactory 的成员补充完毕，并初始化所有非延迟单例 bean\n\ntxt2.  singletonObjects 即单例池，缓存所有单例对象\n\n3.  对象的创建都分三个阶段，每一阶段都有不同的 bean 后处理器参与进来，扩展功能\nfinishRefresh1.  这一步会为 ApplicationContext 添加 lifecycleProcessor 成员，用来控制容器内需要生命周期管理的 bean\n\ntxt2.  如果容器中有名称为 lifecycleProcessor 的 bean 就用它，否则创建默认的生命周期管理器\n\n3.  准备好生命周期管理器，就可以实现\n\n4.  调用 context 的 start，即可触发所有实现 LifeCycle 接口 bean 的 start。调用 context 的 stop，即可触发所有实现 LifeCycle 接口 bean 的 stop。\n\n5.  发布 ContextRefreshed 事件，整个 refresh 执行完成其中：\n1 为准备环境\n2 3 4 5 6 为准备 BeanFactory\n7 8 9 10 12 为准备 ApplicationContext\n11 为初始化 BeanFactory 中非延迟单例 bean\nException（异常情况）\n        如果过程中出现异常走到这里销毁所有创建的单例bean,AtomicBoolean active标识false.最后清除所有缓存及类加载资源 \n\nArticle link： https://tqgoblin.site/post/csdn/Spring加载流程/  Author： Stephen  \n","slug":"csdn/Spring加载流程","date":"2022-10-18T08:43:29.000Z","categories_index":"面试","tags_index":"spring spring java 面试","author_index":"Stephen"},{"id":"5702e63d59ce606e9e7b7128f2ea5bdd","title":"SpringMVC介绍","content":"一、springmvc初始化        我们使用springMVC的时候，它的主要入口时dispatcherServlet类，它最终实现了Servlet接口。Servlet初始化执行init()方法，其中执行到springMVC子容器初始化的会调用DispatcherServlet的onRefresh()方法，而在onRefresh()方法中只做了一件事，就是调用initStrategies()方法来初始化springMVC的九大组件。\n        initStrategies直译过来就是初始化策略，而springMVC把九大组件设计成就打策略，其实就是为了去明确各个组件的职责，达到解耦的目的。按照springMVC九大组件的初始化顺序，他们分别是MultipartResolve多文件上传组件、LocaleResolver多语言支持组件、ThemeResolver主题模板处理组件、HandlerMappings URL映射组件、HandlerAdapters业务逻辑适配组件、HandlerExceptionResolvers异常处理组件、RequestToViewNameTranslator视图名称提取组件、ViewResolvers视图渲染组件、FlashMapManager内存管理组件。如图：\n\n下面详细分析一下每个组件的功能和职责。\n1.MultipartResolver        它主要是支持多文件上传，如代码所示。它主要的逻辑就是将enctype为”multipart&#x2F;form-data”的表单请求request封装成MultipartHttpServletRequest的对象，那程序员在开发的时候就只需要调用MultipartHttpServletRequest的getFile方法就可以获取客户端上传的文件列表了。\n2.LocaleResolver        它主要是用于支持国际化，多语言切换的组件，而LocaleResolver的主要作用就是从request中解析出local参数的值，如源码所示。resolveLocale方法它是从request中去解析出local而setLocal()方法是将指定的local值设置到request中，local对象大多数情况下都是用来做国际化处理的，一般会配合多语言字典properties来使用。例如中国的Local值为zh_CN。\n3.ThemeResolver        他主要是支持web页面的多主体风格切换，可以通过ThemeResolver来读取和解析页面的主题样式配置，它的实现原理和LocaleResolver类似，也是配置一套properties文件，然后根据不同的参数来切换读取。当然，使用ThemeResolver也可以实现国际化。如源码所示。它的主要方法和LocalResolver也类似。一个是从request中去提取主题名称的方法，一个是设置主题名称的方法。\n4.HandlerMappings        它主要是用来保存url和业务逻辑的对应关系，它本质上是一个Map，它的key是url，值就是对应Controller中配置了@RequestMapping注解的方法，但是这种关系呢在spring源码中被封装成一个叫做HandlerMapping的对象，然后每个HandlerMapping对象都被缓存到一个List中，如源码所示。\n5.HandlerAdapters        它主要的功能是动态解析参数以及动态适配业务逻辑对应的Handler，如源码所示。在HandlerAdapter中它提供了一个叫做handle()的方法，它的第三个参数Object handler就是指业务逻辑处理器，也就是HandlerMapping。\n        handle()方法在DispatcherServlet中的doDispath()方法中被调用，而handler对象就是根据用户请求Url从handlerMapping的缓存List中去获取到的HandlerMapping对象，在HandlerAdapter的handle()方法中首先回去动态解析用户传过来的参数，并且会完成数据类型的转换，然后会反射调用HandlerMapping封装的Controller中的方法，最后会将调用方法的返回值封装为一个叫做ModelAndView的对象。\n6.HandlerExceptionResolvers        它主要是用于拦截对不同异常的个性化处理，spring可以给不同的异常配置不同的ModelAndView,而HandlerExceptionResolver就是根据异常类型将处理封装为一个ModelAndView，从而将异常转化为更加友好的web页面展示。如源码所示。在HandlerExceptionResolver组件中只有一个方法就是将异常转化为ModelAndView，当HandlerExceptionResolver自己发生异常的时候或者在异常页面渲染过程中发生异常的时候HandlerExceptionResolver他不会进行处理，但是spring提供了一个全局的配置可以去设置500页面或者是404页面来处理这个问题。\n7.RequestToViewNameTranslator        这个组件的主要功能是从request中去提取viewName,而这个viewName它可以设置在url参数上也可以设置在request的header上。如源码所示。这个其实挺有意思，就是将request的请求转化为视图名称。它只有一个getViewName()方法。\n8.ViewResolvers        就是根据视图名称找到视图对应的模板文件，然后进行解析。如源码所示。ViewResolver组件只有一个resolveViewName()方法，我们可以看到resolveViewName()方法中有两个参数，第一个参数是viewName,他是string类型它其实就是视图名称对应的模板文件的名称。第二个参数是local，前面我们讲过它代表的是本地语言环境，它可以用来做国际化。而resolveViewName()方法的返回值是一个View对象，View对象就是用来渲染的，也就是将程序返回的结果填入到具体的模板里面生产具体的视图文件，比如说jsp,ftl,html等待。\n9.FlashMapManager        它相当于一个参数缓存器可以用来去保证请求跳转过程中参数不会丢失，和struts2中的ValueStack值栈非常类似，主要是redirect重定向的时候参数传递会丢失，这时候FlashMapManager就能够大显身手，它可以实现Redirect重定向也能和Forward转发同样的效果。如源码所示。FlashMapManager主要有两个方法，一个是retrieveAndUpdate()方法它是用来恢复参数的，而且会将恢复的和超时的参数删除掉。第二个是saveOutputFlashMap()方法，它是用来保存参数的，FlashMapManager会默认将参数保存到session中。在日常开发中如果不想将参数暴露在url路径中，如果不想将参数暴露在url路径中那就一个在请求转发时在参数上添加@RedirectAttributes注解将参数缓存起来然后在下一个处理器中就可以获取到。\n        注意：ModelAndView和View并不属于MVC的九大组件之中，ModelAndView只是对ViewName和Model的封装然后作为返回值把信息反馈给用户，并且它并没有包含任何的执行逻辑。而view呢而是对模板的封装他是用作参数来传递。\n二、springMVC请求执行流程        我们使用springMVC的时候，它的主要入口时dispatcherServlet类它最终实现了Servlet接口,那它就是个Servlet类。请求该方法会执行doService方法。请求来了，那接下来springMvc执行流程用图表示\n\n\n用户向服务端发送一次请求，这个请求会先到前端控制器DispatcherServlet(也叫中央控制器)。\nDispatcherServlet接收到请求后会调用HandlerMapping处理器映射器。由此得知，该请求该由哪个Controller来处理（并未调用Controller，只是得知）\nDispatcherServlet调用HandlerAdapter处理器适配器，告诉处理器适配器应该要去执行哪个Controller\nHandlerAdapter处理器适配器去执行Controller并得到ModelAndView(数据和视图)，并层层返回给DispatcherServlet\nDispatcherServlet将ModelAndView交给ViewReslover视图解析器解析，然后返回真正的视图。\nDispatcherServlet将模型数据填充到视图中\nDispatcherServlet将结果响应给用户\n\nArticle link： https://tqgoblin.site/post/csdn/springMVC介绍/  Author： Stephen  \n","slug":"csdn/springMVC介绍","date":"2022-10-18T06:59:27.000Z","categories_index":"Java","tags_index":"面试 java spring","author_index":"Stephen"},{"id":"1c003f281cf294408d7d24cc6188aa8e","title":"java中的锁","content":"1. 乐观锁 VS 悲观锁乐观锁与悲观锁是一种广义上的概念，体现了看待线程同步的不同角度。在Java和数据库中都有此概念对应的实际应用。\n先说概念。对于同一个数据的并发操作，悲观锁认为自己在使用数据的时候一定有别的线程来修改数据，因此在获取数据的时候会先加锁，确保数据不会被别的线程修改。Java中，synchronized关键字和Lock的实现类都是悲观锁。\n而乐观锁认为自己在使用数据时不会有别的线程修改数据，所以不会添加锁，只是在更新数据的时候去判断之前有没有别的线程更新了这个数据。如果这个数据没有被更新，当前线程将自己修改的数据成功写入。如果数据已经被其他线程更新，则根据不同的实现方式执行不同的操作（例如报错或者自动重试）。\n乐观锁在Java中是通过使用无锁编程来实现，最常采用的是CAS算法，Java原子类中的递增操作就通过CAS自旋实现的。\n\n根据从上面的概念描述我们可以发现：\n\n悲观锁适合写操作多的场景，先加锁可以保证写操作时数据正确。\n\n乐观锁适合读操作多的场景，不加锁的特点能够使其读操作的性能大幅提升。\n\n\n光说概念有些抽象，我们来看下乐观锁和悲观锁的调用方式示例：\n\n通过调用方式示例，我们可以发现悲观锁基本都是在显式的锁定之后再操作同步资源，而乐观锁则直接去操作同步资源。那么，为何乐观锁能够做到不锁定同步资源也可以正确的实现线程同步呢？我们通过介绍乐观锁的主要实现方式 “CAS” 的技术原理来为大家解惑。\nCAS全称 Compare And Swap（比较与交换），是一种无锁算法。在不使用锁（没有线程被阻塞）的情况下实现多线程之间的变量同步。java.util.concurrent包中的原子类就是通过CAS来实现了乐观锁。\nCAS算法涉及到三个操作数：\n\n需要读写的内存值 V。\n\n进行比较的值 A。\n\n要写入的新值 B。\n\n\n当且仅当 V 的值等于 A 时，CAS通过原子方式用新值B来更新V的值（“比较+更新”整体是一个原子操作），否则不会执行任何操作。一般情况下，“更新”是一个不断重试的操作。\n之前提到java.util.concurrent包中的原子类，就是通过CAS来实现了乐观锁，那么我们进入原子类AtomicInteger的源码，看一下AtomicInteger的定义：\n\n根据定义我们可以看出各属性的作用：\n\nunsafe： 获取并操作内存的数据。\n\nvalueOffset： 存储value在AtomicInteger中的偏移量。\n\nvalue： 存储AtomicInteger的int值，该属性需要借助volatile关键字保证其在线程间是可见的。\n\n\n接下来，我们查看AtomicInteger的自增函数incrementAndGet()的源码时，发现自增函数底层调用的是unsafe.getAndAddInt()。但是由于JDK本身只有Unsafe.class，只通过class文件中的参数名，并不能很好的了解方法的作用，所以我们通过OpenJDK 8 来查看Unsafe的源码：\n\n根据OpenJDK 8的源码我们可以看出，getAndAddInt()循环获取给定对象o中的偏移量处的值v，然后判断内存值是否等于v。如果相等则将内存值设置为 v + delta，否则返回false，继续循环进行重试，直到设置成功才能退出循环，并且将旧值返回。整个“比较+更新”操作封装在compareAndSwapInt()中，在JNI里是借助于一个CPU指令完成的，属于原子操作，可以保证多个线程都能够看到同一个变量的修改值。\n后续JDK通过CPU的cmpxchg指令，去比较寄存器中的 A 和 内存中的值 V。如果相等，就把要写入的新值 B 存入内存中。如果不相等，就将内存值 V 赋值给寄存器中的值 A。然后通过Java代码中的while循环再次调用cmpxchg指令进行重试，直到设置成功为止。\nCAS虽然很高效，但是它也存在三大问题，这里也简单说一下：\n1. ABA问题。CAS需要在操作值的时候检查内存值是否发生变化，没有发生变化才会更新内存值。但是如果内存值原来是A，后来变成了B，然后又变成了A，那么CAS进行检查时会发现值没有发生变化，但是实际上是有变化的。ABA问题的解决思路就是在变量前面添加版本号，每次变量更新的时候都把版本号加一，这样变化过程就从“A－B－A”变成了“1A－2B－3A”。\nJDK从1.5开始提供了AtomicStampedReference类来解决ABA问题，具体操作封装在compareAndSet()中。compareAndSet()首先检查当前引用和当前标志与预期引用和预期标志是否相等，如果都相等，则以原子方式将引用值和标志的值设置为给定的更新值。\n2. 循环时间长开销大。CAS操作如果长时间不成功，会导致其一直自旋，给CPU带来非常大的开销。\n3. 只能保证一个共享变量的原子操作。对一个共享变量执行操作时，CAS能够保证原子操作，但是对多个共享变量操作时，CAS是无法保证操作的原子性的。\nJava从1.5开始JDK提供了AtomicReference类来保证引用对象之间的原子性，可以把多个变量放在一个对象里来进行CAS操作。\n2. 自旋锁 VS 适应性自旋锁在介绍自旋锁前，我们需要介绍一些前提知识来帮助大家明白自旋锁的概念。\n阻塞或唤醒一个Java线程需要操作系统切换CPU状态来完成，这种状态转换需要耗费处理器时间。如果同步代码块中的内容过于简单，状态转换消耗的时间有可能比用户代码执行的时间还要长。\n在许多场景中，同步资源的锁定时间很短，为了这一小段时间去切换线程，线程挂起和恢复现场的花费可能会让系统得不偿失。如果物理机器有多个处理器，能够让两个或以上的线程同时并行执行，我们就可以让后面那个请求锁的线程不放弃CPU的执行时间，看看持有锁的线程是否很快就会释放锁。\n而为了让当前线程“稍等一下”，我们需让当前线程进行自旋，如果在自旋完成后前面锁定同步资源的线程已经释放了锁，那么当前线程就可以不必阻塞而是直接获取同步资源，从而避免切换线程的开销。这就是自旋锁。\n\n自旋锁本身是有缺点的，它不能代替阻塞。自旋等待虽然避免了线程切换的开销，但它要占用处理器时间。如果锁被占用的时间很短，自旋等待的效果就会非常好。反之，如果锁被占用的时间很长，那么自旋的线程只会白浪费处理器资源。所以，自旋等待的时间必须要有一定的限度，如果自旋超过了限定次数（默认是10次，可以使用-XX:PreBlockSpin来更改）没有成功获得锁，就应当挂起线程。\n自旋锁的实现原理同样也是CAS，AtomicInteger中调用unsafe进行自增操作的源码中的do-while循环就是一个自旋操作，如果修改数值失败则通过循环来执行自旋，直至修改成功。\n\n自旋锁在JDK1.4.2中引入，使用-XX:+UseSpinning来开启。JDK 6中变为默认开启，并且引入了自适应的自旋锁（适应性自旋锁）。\n自适应意味着自旋的时间（次数）不再固定，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过，那在以后尝试获取这个锁时将可能省略掉自旋过程，直接阻塞线程，避免浪费处理器资源。\n在自旋锁中 另有三种常见的锁形式:TicketLock、CLHlock和MCSlock，本文中仅做名词介绍，不做深入讲解，感兴趣的同学可以自行查阅相关资料。\n3. 无锁 VS 偏向锁 VS 轻量级锁 VS 重量级锁这四种锁是指锁的状态，专门针对synchronized的。在介绍这四种锁状态之前还需要介绍一些额外的知识。\n首先为什么Synchronized能实现线程同步？\n在回答这个问题之前我们需要了解两个重要的概念：“Java对象头”、“Monitor”。\nJava对象头\nsynchronized是悲观锁，在操作同步资源之前需要给同步资源先加锁，这把锁就是存在Java对象头里的，而Java对象头又是什么呢？\n我们以Hotspot虚拟机为例，Hotspot的对象头主要包括两部分数据：Mark Word（标记字段）、Klass Pointer（类型指针）。\nMark Word：默认存储对象的HashCode，分代年龄和锁标志位信息。这些信息都是与对象自身定义无关的数据，所以Mark Word被设计成一个非固定的数据结构以便在极小的空间内存存储尽量多的数据。它会根据对象的状态复用自己的存储空间，也就是说在运行期间Mark Word里存储的数据会随着锁标志位的变化而变化。\nKlass Point：对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。\nMonitor\nMonitor可以理解为一个同步工具或一种同步机制，通常被描述为一个对象。每一个Java对象就有一把看不见的锁，称为内部锁或者Monitor锁。\nMonitor是线程私有的数据结构，每一个线程都有一个可用monitor record列表，同时还有一个全局的可用列表。每一个被锁住的对象都会和一个monitor关联，同时monitor中有一个Owner字段存放拥有该锁的线程的唯一标识，表示该锁被这个线程占用。\n现在话题回到synchronized，synchronized通过Monitor来实现线程同步，Monitor是依赖于底层的操作系统的Mutex Lock（互斥锁）来实现的线程同步。\n如同我们在自旋锁中提到的“阻塞或唤醒一个Java线程需要操作系统切换CPU状态来完成，这种状态转换需要耗费处理器时间。如果同步代码块中的内容过于简单，状态转换消耗的时间有可能比用户代码执行的时间还要长”。这种方式就是synchronized最初实现同步的方式，这就是JDK 6之前synchronized效率低的原因。这种依赖于操作系统Mutex Lock所实现的锁我们称之为“重量级锁”，JDK 6中为了减少获得锁和释放锁带来的性能消耗，引入了“偏向锁”和“轻量级锁”。\n所以目前锁一共有4种状态，级别从低到高依次是：无锁、偏向锁、轻量级锁和重量级锁。锁状态只能升级不能降级。\n通过上面的介绍，我们对synchronized的加锁机制以及相关知识有了一个了解，那么下面我们给出四种锁状态对应的的Mark Word内容，然后再分别讲解四种锁状态的思路以及特点：\n\n无锁\n无锁没有对资源进行锁定，所有的线程都能访问并修改同一个资源，但同时只有一个线程能修改成功。\n无锁的特点就是修改操作在循环内进行，线程会不断的尝试修改共享资源。如果没有冲突就修改成功并退出，否则就会继续循环尝试。如果有多个线程修改同一个值，必定会有一个线程能修改成功，而其他修改失败的线程会不断重试直到修改成功。上面我们介绍的CAS原理及应用即是无锁的实现。无锁无法全面代替有锁，但无锁在某些场合下的性能是非常高的。\n偏向锁\n偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁，降低获取锁的代价。\n在大多数情况下，锁总是由同一线程多次获得，不存在多线程竞争，所以出现了偏向锁。其目标就是在只有一个线程执行同步代码块时能够提高性能。\n当一个线程访问同步代码块并获取锁时，会在Mark Word里存储锁偏向的线程ID。在线程进入和退出同步块时不再通过CAS操作来加锁和解锁，而是检测Mark Word里是否存储着指向当前线程的偏向锁。引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令即可。\n偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动释放偏向锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态。撤销偏向锁后恢复到无锁（标志位为“01”）或轻量级锁（标志位为“00”）的状态。\n偏向锁在JDK 6及以后的JVM里是默认启用的。可以通过JVM参数关闭偏向锁：-XX:-UseBiasedLocking&#x3D;false，关闭之后程序默认会进入轻量级锁状态。\n轻量级锁\n是指当锁是偏向锁的时候，被另外的线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，从而提高性能。\n在代码进入同步块的时候，如果同步对象锁状态为无锁状态（锁标志位为“01”状态，是否为偏向锁为“0”），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝，然后拷贝对象头中的Mark Word复制到锁记录中。\n拷贝成功后，虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，并将Lock Record里的owner指针指向对象的Mark Word。\n如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为“00”，表示此对象处于轻量级锁定状态。\n如果轻量级锁的更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是就说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行，否则说明多个线程竞争锁。\n若当前只有一个等待线程，则该线程通过自旋进行等待。但是当自旋超过一定的次数，或者一个线程在持有锁，一个在自旋，又有第三个来访时，轻量级锁升级为重量级锁。\n重量级锁\n升级为重量级锁时，锁标志的状态值变为“10”，此时Mark Word中存储的是指向重量级锁的指针，此时等待锁的线程都会进入阻塞状态。\n整体的锁状态升级流程如下：\n\n综上，偏向锁通过对比Mark Word解决加锁问题，避免执行CAS操作。而轻量级锁是通过用CAS操作和自旋来解决加锁问题，避免线程阻塞和唤醒而影响性能。重量级锁是将除了拥有锁的线程以外的线程都阻塞。\n4. 公平锁 VS 非公平锁公平锁是指多个线程按照申请锁的顺序来获取锁，线程直接进入队列中排队，队列中的第一个线程才能获得锁。公平锁的优点是等待锁的线程不会饿死。缺点是整体吞吐效率相对非公平锁要低，等待队列中除第一个线程以外的所有线程都会阻塞，CPU唤醒阻塞线程的开销比非公平锁大。\n非公平锁是多个线程加锁时直接尝试获取锁，获取不到才会到等待队列的队尾等待。但如果此时锁刚好可用，那么这个线程可以无需阻塞直接获取到锁，所以非公平锁有可能出现后申请锁的线程先获取锁的场景。非公平锁的优点是可以减少唤起线程的开销，整体的吞吐效率高，因为线程有几率不阻塞直接获得锁，CPU不必唤醒所有线程。缺点是处于等待队列中的线程可能会饿死，或者等很久才会获得锁。\n直接用语言描述可能有点抽象，这里作者用从别处看到的一个例子来讲述一下公平锁和非公平锁。\n\n如上图所示，假设有一口水井，有管理员看守，管理员有一把锁，只有拿到锁的人才能够打水，打完水要把锁还给管理员。每个过来打水的人都要管理员的允许并拿到锁之后才能去打水，如果前面有人正在打水，那么这个想要打水的人就必须排队。管理员会查看下一个要去打水的人是不是队伍里排最前面的人，如果是的话，才会给你锁让你去打水；如果你不是排第一的人，就必须去队尾排队，这就是公平锁。\n但是对于非公平锁，管理员对打水的人没有要求。即使等待队伍里有排队等待的人，但如果在上一个人刚打完水把锁还给管理员而且管理员还没有允许等待队伍里下一个人去打水时，刚好来了一个插队的人，这个插队的人是可以直接从管理员那里拿到锁去打水，不需要排队，原本排队等待的人只能继续等待。如下图所示：\n\n接下来我们通过ReentrantLock的源码来讲解公平锁和非公平锁。\n\n根据代码可知，ReentrantLock里面有一个内部类Sync，Sync继承AQS（AbstractQueuedSynchronizer），添加锁和释放锁的大部分操作实际上都是在Sync中实现的。它有公平锁FairSync和非公平锁NonfairSync两个子类。ReentrantLock默认使用非公平锁，也可以通过构造器来显示的指定使用公平锁。\n下面我们来看一下公平锁与非公平锁的加锁方法的源码:\n\n通过上图中的源代码对比，我们可以明显的看出公平锁与非公平锁的lock()方法唯一的区别就在于公平锁在获取同步状态时多了一个限制条件：hasQueuedPredecessors()。\n\n再进入hasQueuedPredecessors()，可以看到该方法主要做一件事情：主要是判断当前线程是否位于同步队列中的第一个。如果是则返回true，否则返回false。\n综上，公平锁就是通过同步队列来实现多个线程按照申请锁的顺序来获取锁，从而实现公平的特性。非公平锁加锁时不考虑排队等待问题，直接尝试获取锁，所以存在后申请却先获得锁的情况。\n5. 可重入锁 VS 非可重入锁可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，再进入该线程的内层方法会自动获取锁（前提锁对象得是同一个对象或者class），不会因为之前已经获取过还没释放而阻塞。Java中ReentrantLock和synchronized都是可重入锁，可重入锁的一个优点是可一定程度避免死锁。下面用示例代码来进行分析：\n\n在上面的代码中，类中的两个方法都是被内置锁synchronized修饰的，doSomething()方法中调用doOthers()方法。因为内置锁是可重入的，所以同一个线程在调用doOthers()时可以直接获得当前对象的锁，进入doOthers()进行操作。\n如果是一个不可重入锁，那么当前线程在调用doOthers()之前需要将执行doSomething()时获取当前对象的锁释放掉，实际上该对象锁已被当前线程所持有，且无法释放。所以此时会出现死锁。\n而为什么可重入锁就可以在嵌套调用时可以自动获得锁呢？我们通过图示和源码来分别解析一下。\n还是打水的例子，有多个人在排队打水，此时管理员允许锁和同一个人的多个水桶绑定。这个人用多个水桶打水时，第一个水桶和锁绑定并打完水之后，第二个水桶也可以直接和锁绑定并开始打水，所有的水桶都打完水之后打水人才会将锁还给管理员。这个人的所有打水流程都能够成功执行，后续等待的人也能够打到水。这就是可重入锁。\n\n但如果是非可重入锁的话，此时管理员只允许锁和同一个人的一个水桶绑定。第一个水桶和锁绑定打完水之后并不会释放锁，导致第二个水桶不能和锁绑定也无法打水。当前线程出现死锁，整个等待队列中的所有线程都无法被唤醒。\n\n之前我们说过ReentrantLock和synchronized都是重入锁，那么我们通过重入锁ReentrantLock以及非可重入锁NonReentrantLock的源码来对比分析一下为什么非可重入锁在重复调用同步资源时会出现死锁。\n首先ReentrantLock和NonReentrantLock都继承父类AQS，其父类AQS中维护了一个同步状态status来计数重入次数，status初始值为0。\n当线程尝试获取锁时，可重入锁先尝试获取并更新status值，如果status &#x3D;&#x3D; 0表示没有其他线程在执行同步代码，则把status置为1，当前线程开始执行。如果status !&#x3D; 0，则判断当前线程是否是获取到这个锁的线程，如果是的话执行status+1，且当前线程可以再次获取锁。而非可重入锁是直接去获取并尝试更新当前status的值，如果status !&#x3D; 0的话会导致其获取锁失败，当前线程阻塞。\n释放锁时，可重入锁同样先获取当前status的值，在当前线程是持有锁的线程的前提下。如果status-1 &#x3D;&#x3D; 0，则表示当前线程所有重复获取锁的操作都已经执行完毕，然后该线程才会真正释放锁。而非可重入锁则是在确定当前线程是持有锁的线程之后，直接将status置为0，将锁释放。\n\n6. 独享锁 VS 共享锁独享锁和共享锁同样是一种概念。我们先介绍一下具体的概念，然后通过ReentrantLock和ReentrantReadWriteLock的源码来介绍独享锁和共享锁。\n独享锁也叫排他锁，是指该锁一次只能被一个线程所持有。如果线程T对数据A加上排它锁后，则其他线程不能再对A加任何类型的锁。获得排它锁的线程即能读数据又能修改数据。JDK中的synchronized和JUC中Lock的实现类就是互斥锁。\n共享锁是指该锁可被多个线程所持有。如果线程T对数据A加上共享锁后，则其他线程只能对A再加共享锁，不能加排它锁。获得共享锁的线程只能读数据，不能修改数据。\n独享锁与共享锁也是通过AQS来实现的，通过实现不同的方法，来实现独享或者共享。\n下图为ReentrantReadWriteLock的部分源码：\n\n我们看到ReentrantReadWriteLock有两把锁：ReadLock和WriteLock，由词知意，一个读锁一个写锁，合称“读写锁”。再进一步观察可以发现ReadLock和WriteLock是靠内部类Sync实现的锁。Sync是AQS的一个子类，这种结构在CountDownLatch、ReentrantLock、Semaphore里面也都存在。\n在ReentrantReadWriteLock里面，读锁和写锁的锁主体都是Sync，但读锁和写锁的加锁方式不一样。读锁是共享锁，写锁是独享锁。读锁的共享锁可保证并发读非常高效，而读写、写读、写写的过程互斥，因为读锁和写锁是分离的。所以ReentrantReadWriteLock的并发性相比一般的互斥锁有了很大提升。\n那读锁和写锁的具体加锁方式有什么区别呢？在了解源码之前我们需要回顾一下其他知识。\n在最开始提及AQS的时候我们也提到了state字段（int类型，32位），该字段用来描述有多少线程获持有锁。\n在独享锁中这个值通常是0或者1（如果是重入锁的话state值就是重入的次数），在共享锁中state就是持有锁的数量。但是在ReentrantReadWriteLock中有读、写两把锁，所以需要在一个整型变量state上分别描述读锁和写锁的数量（或者也可以叫状态）。于是将state变量“按位切割”切分成了两个部分，高16位表示读锁状态（读锁个数），低16位表示写锁状态（写锁个数）。如下图所示：\n\n了解了概念之后我们再来看代码，先看写锁的加锁源码：\n\n\n这段代码首先取到当前锁的个数c，然后再通过c来获取写锁的个数w。因为写锁是低16位，所以取低16位的最大值与当前的c做与运算（ int w &#x3D; exclusiveCount(c); ），高16位和0与运算后是0，剩下的就是低位运算的值，同时也是持有写锁的线程数目。\n\n在取到写锁线程的数目后，首先判断是否已经有线程持有了锁。如果已经有线程持有了锁（c!&#x3D;0），则查看当前写锁线程的数目，如果写线程数为0（即此时存在读锁）或者持有锁的线程不是当前线程就返回失败（涉及到公平锁和非公平锁的实现）。\n\n如果写入锁的数量大于最大数（65535，2的16次方-1）就抛出一个Error。\n\n如果当且写线程数为0（那么读线程也应该为0，因为上面已经处理c!&#x3D;0的情况），并且当前线程需要阻塞那么就返回失败；如果通过CAS增加写线程数失败也返回失败。\n\n如果c&#x3D;0，w&#x3D;0或者c&gt;0，w&gt;0（重入），则设置当前线程或锁的拥有者，返回成功！\n\n\ntryAcquire()除了重入条件（当前线程为获取了写锁的线程）之外，增加了一个读锁是否存在的判断。如果存在读锁，则写锁不能被获取，原因在于：必须确保写锁的操作对读锁可见，如果允许读锁在已被获取的情况下对写锁的获取，那么正在运行的其他读线程就无法感知到当前写线程的操作。\n因此，只有等待其他读线程都释放了读锁，写锁才能被当前线程获取，而写锁一旦被获取，则其他读写线程的后续访问均被阻塞。写锁的释放与ReentrantLock的释放过程基本类似，每次释放均减少写状态，当写状态为0时表示写锁已被释放，然后等待的读写线程才能够继续访问读写锁，同时前次写线程的修改对后续的读写线程可见。\n接着是读锁的代码：\n\n可以看到在tryAcquireShared(int unused)方法中，如果其他线程已经获取了写锁，则当前线程获取读锁失败，进入等待状态。如果当前线程获取了写锁或者写锁未被获取，则当前线程（线程安全，依靠CAS保证）增加读状态，成功获取读锁。读锁的每次释放（线程安全的，可能有多个读线程同时释放读锁）均减少读状态，减少的值是“1&lt;&lt;16”。所以读写锁才能实现读读的过程共享，而读写、写读、写写的过程互斥。\n此时，我们再回头看一下互斥锁ReentrantLock中公平锁和非公平锁的加锁源码：\n\n我们发现在ReentrantLock虽然有公平锁和非公平锁两种，但是它们添加的都是独享锁。根据源码所示，当某一个线程调用lock方法获取锁时，如果同步资源没有被其他线程锁住，那么当前线程在使用CAS更新state成功后就会成功抢占该资源。而如果公共资源被占用且不是被当前线程占用，那么就会加锁失败。所以可以确定ReentrantLock无论读操作还是写操作，添加的锁都是都是独享锁。\nArticle link： https://tqgoblin.site/post/csdn/java中的锁/  Author： Stephen  \n","slug":"csdn/java中的锁","date":"2022-10-14T15:40:19.000Z","categories_index":"Java","tags_index":"锁 java 面试","author_index":"Stephen"},{"id":"65befa2e81f1cf5d027b510d356d0eda","title":"简单介绍Kafka","content":"Kafka 概念Kafka 是一种高吞吐量、分布式、基于发布&#x2F;订阅的消息系统，最初由 LinkedIn 公司开发，使用Scala 语言编写，目前是 Apache 的开源项目。\n\nbroker：Kafka 服务器，负责消息存储和转发\n topic：消息类别，Kafka 按照 topic 来分类消息\npartition：topic 的分区，一个 topic 可以包含多个 partition，topic 消息保存在各个 partition 上\noffset：消息在日志中的位置，可以理解是消息在 partition 上的偏移量，也是代表该消息的唯一序号\n Producer：消息生产者\nConsumer：消息消费者\nConsumer Group：消费者分组，每个 Consumer 必须属于一个 group\n Zookeeper：保存着集群 broker、topic、partition 等 meta 数据；另外，还负责 broker 故障发现，partition leader 选举，负载均衡等功能\n\n\n一、Kafka 数据存储设计1.partition 的数据文件（offset，MessageSize，data）        partition 中的每条 Message 包含了以下三个属性：offset，MessageSize，data，其中 offset 表示 Message 在这个 partition 中的偏移量，offset 不是该 Message 在 partition 数据文件中的实际存储位置，而是逻辑上一个值，它唯一确定了 partition 中的一条 Message，可以认为 offset 是partition 中 Message 的 id；MessageSize 表示消息内容 data 的大小；data 为 Message 的具体内容。\n2.数据文件分段 segment（顺序读写、分段命令、二分查找）partition 物理上由多个 segment 文件组成，每个 segment 大小相等，顺序读写。每个segment 数据文件以该段中最小的 offset 命名，文件扩展名为.log。这样在查找指定 offset 的Message 的时候，用二分查找就可以定位到该 Message 在哪个 segment 数据文件中。\n3.数据文件索引（分段索引、稀疏存储）        Kafka 为每个分段后的数据文件建立了索引文件，文件名与数据文件的名字是一样的，只是文件扩 展名为.index。index 文件中并没有为数据文件中的每条 Message 建立索引，而是采用了稀疏存储的方式，每隔一定字节的数据建立一条索引。这样避免了索引文件占用过多的空间，从而可以将索引文件保留在内存中。\n\n三、生产者设计1.负载均衡（partition 会均衡分布到不同 broker 上）        由于消息 topic 由多个 partition 组成， 且 partition 会均衡分布到不同 broker 上，因此，为了有效利用 broker 集群的性能，提高消息的吞吐量 ，producer 可以通过随机或者 hash 等方式，将消息平均发送到多个 partition 上，以实现负载均衡。\n\n2.批量发送        是提高消息吞吐量重要的方式，Producer 端可以在内存中合并多条消息后， 以一次请求的方式发送了批量的消息给 broker ，从而大大减少 broker 存储消息的 IO 操作次数。但也一定程度上影响了消息的实时性，相当于以时延代价，换取更好的吞吐量。\n3. 压缩（GZIP 或 Snappy）        Producer 端可以通过 GZIP 或 Snappy 格式对消息集合进行压缩。Producer 端进行压缩之后，在Consumer 端需进行解压。压缩的好处就是减少传输的数据量，减轻对网络传输的压力，在对大数据处理上，瓶颈往往体现在网络上而不是 CPU（压缩和解压会耗掉部分CPU 资源）。\n三、消费者设计\n1. Consumer Group        同一 Consumer Group 中的多个 Consumer 实例，不同时消费同一个 partition，等效于队列模式。partition 内消息是有序的，Consumer 通过 pull 方式消费消息。Kafka 不删除已消费的消息 对于 partition，顺序读写磁盘数据，以时间复杂度 O(1)方式提供消息持久化能力。\nArticle link： https://tqgoblin.site/post/csdn/简单介绍Kafka/  Author： Stephen  \n","slug":"csdn/简单介绍Kafka","date":"2022-10-13T15:38:26.000Z","categories_index":"面试","tags_index":"kafka kafka","author_index":"Stephen"},{"id":"f00905d171c4323a2676ba94b49d2b6c","title":"java并发关键字","content":"一、CountDownLatch        CountDownLatch类位于java.util.concurrent包下，是一个同步工具类 ，允许一个或多个线程一直等待其他线程的操作执行完后再执行相关操作。        CountDownLatch 基于线程计数器来实现并发访问控制，主要用于主线程等待其他子线程都执行完毕后执行相关操作。其使用过程为：在主线程中定义CountDownLatch，并将线程计数器的初始值设置为子线程的个数，多个子线程并发执行，每个子线程在执行完毕后都会调用countDown函数将计数器的值减1，直到线程计数器为0，表示所有的子线程任务都已执行完毕， 此时countDownLatch上等待的主线程将被唤醒并继续执行。        我们利用CountDownLatch可以实现类似计数器的功能 比如有一个主任务，它要等待其他两个任务都执行完毕之后才能执行，此时就可以利用 CountDownLatch来实现这种功能。具体实现如下：\n\n \n        以上代码片段先定义了一个大小为2的CountDownLatch，然后定义了两个子线程并启动该子线程，子线程执行完业务代码后在执行latch.countDown()时减少一个信号量，表示自己已执行完成。 主线程调用latch.await()阻塞等待，在所有线程都执行完成并调用了countDown函数时， 表示所有线程均执行完成，这时程序会主动唤醒主线程井开始执行主线程的业务逻辑。\n二、CyclicBarrier        CyclicBarrier （循环屏障）是一个同步工具，可以实现让一组线程等待至某个状态之后再全部同时执行。 在所有等待线程都被释放之后，CyclicBarrier 可以被重用。CyclicBarrer的运行状态叫作Barrier状态，在调用 await方法后，线程就处于Barrier状态。        CyclicBarrier中最重要的方法是 await 方法，它有两种实现。\n\npublic int await()：挂起当前线程直到所有线程都为 Barrier 状态再同时执行后续的任务。\npublic int await(long timeout, Tim Unit unit)：设置一个超时时间，在超时时间过后，如果还有线程未达到 Barrier 状态，则不再等待，让达到 Barrier 态的线程继续执行后续的任务。\n\nCyclicBarrier的具体使用方法如下：\n\n        以上代码先定义了一CyclicBarrier， 然后循环启动了多个线程，每个线程都通过构造函数将 CyclicBarrier 传入线程 ，在线程内部开始执行第1阶段的工作，比如查询数据等；等第1阶段的工作处理完成后，再调cyclicBarrier.await 方法等待其他线程也完成第一阶段的工作(CyclicBarrier让一组线程等待到达某个状态再一起执行);等其他线程也执行完第1阶段的工作，便可执行并发操作的下一项任务，比如数据分发等。\n三、Semaphore        Semaphore指信号量，用于控制同时访问某些资源的线程个数，具体做法为通过调用acquire()获取一个许可，如果没有许可，则等待，在许可使用完毕后通过 release() 释放该许可，以便其他线程使用。        Semaphore常被用于多线程需要共享有限资源的情况，比如办公室有两台打印机，但是有5个员工需要使用， 一台打印机同时只能被一个员工使用，其他员工排队等候，且只有该打印机被使用完毕并释放后其他员工方可使用，这时就可以通过Semaphore来实现：\n        以上代码中首先定义了一个数量为2的Semaphore ，然后定义了一个工作线程Worker并通过构造函数将Semaphore传入到线程内部。在线程调用semaphore.acquire()时开始申请许可并执行业务逻辑，在线程业务逻辑执行完成后调用Semaphore.release()释放许可，以便其他线程使用。\nSemaphore 类中有以下几个比较重要的方法。\n\npublic void acquire ()：以阻塞方式获取一个许可，在有可用许可时返回该许可，在没有可用许可时阻塞等待，直到获得许可。\npublic void acquire(int permits)：同时获取permits个许可。\npublic void relase ()： 释放某个许可。\npublic void release(int permits)： 释放 permits 个许可。\npublic boolean try Acquire()：以非阻塞方式获取一个许可，在有可用许可时获取该许可并返回true，否则返回false不会等待。\npublic boolean tryAcquire(long timeout,Tim Unit unit）：如果在指定的时间内成功获取到可用许可，则返回true，否则返回false。\npublic boolean tryAcquire(int permits ）：如果成功获取 permits 个许可 则返回 true,否则立即返回false。\npublic boolean tryAcquire(int permits,long timeout,TimeUnit unit)：如果在指定的时间内成功获取permits个许可，则返回true，否则返回false。\navailablePermits()：查询可用的许可数量。\n\nCountDownLatch、CyclicBarrier、Semaphore的区别如下。\n\nCountDownLatch和CyclicBarrier 都用于实现多线程之间的相互等待，但二者的关注点不同。CountDownLatch主要用于主线程等待其他子线程任务均执行完毕后再执行接下来的业务逻辑单元，而CyclicBarrier主要用于一组线程互相等待大家都达到某个状态后，再同时执行接下来的业务逻辑单元。此外，CountDownLatch是不可以重用的，而CyclicBarrier是可以重用的。\nSemaphore和Java中的锁功能类似，主要用于控制资源的并发访问。\n\n四、volatile 关键字的作用        Java 除了使用了synchronized保证变量的同步，还使用了稍弱的同步机制，即volatile变量。 volatile也用于确保将变量的更新操作通知到其他线程。        volatile具备两种特性：一种是保证该变量对所有线程可见，在一个线程修改了变量的值后，新的值对于其他线程是可以立即获取的；一种是volatile禁止指令重排，即volatile 不会被缓存在寄存器中或者对其他处理器不可见的地方，因此在读取volatile类型的变量时总会返回最新写入的值。        因为在访问volatile变量时不会执行加锁操作，也就不会执行线程阻塞，因此volatile 变量是一种比synchronize关键字更轻量级的同步机制。volatile 主要适用于一个变量被多个线程共享，多个线程均可针对这个变量执行赋值或者读取的操作。        在有多个线程对普通变量进行读写时，每个线程都首先需要将数据从内存中复制变量到CPU缓存中，如果计算机有多个CPU，则线程可能都在不同的CPU被处理，这意味着每个线程都需要将同一个数据复制到不同的CPU Cache中，这样在每个线程都针对同一个变量的数据做了不同的处理后就可能存在数据不一致的情况。具体的多线程读写流程如图所示：\n\n        如果将变量声明为volatile, JVM就能保证每次读取变量时都直接从内存中读取，跳过CPU Cache这一步 ，有效解决了多线程数据同步的问题。具体的流程如图所示： \n\n        需要说明的是，volatile关键字可以严格保障变量的单次读、写操作的原子性，但并不能保证像i++这种操作的原子性，因为i++在本质上是读、写两次操作。 volatile在某些场景下可以代替synchronized，但是volatile不能完全取代synchronize的位置，只有在一些特殊场景下才适合使用volatile。比如，必须同时满足下面两个条件才能保证并发环境的线程安全。\n\n对变量的写操作不依赖于当前值（比如i++），或者说是单纯的变量赋值( boolean flag &#x3D; true)。\n该变量没有被包含在具有其他的不变式中，也就是说在不同的volatile变量之间不能互相依赖，只有在状态真正独立于程序内的其他内容时才能使用volatile。\n\n        volatile关键字的使用方法比较简单，直接在定义变量时加上volatile关键字即可：\njavavolatile boolean flag = false; Article link： https://tqgoblin.site/post/csdn/java并发关键字/  Author： Stephen  \n","slug":"csdn/java并发关键字","date":"2022-10-11T03:27:15.000Z","categories_index":"Java","tags_index":"Java 面试 java","author_index":"Stephen"},{"id":"7d9db4929e21c0d91333dbdc28b79909","title":"什么是AQS","content":"        AQS ( Abstract Queued Synchronizer ）是一个抽象的队列同步器，通过维护一个共享资源状态（ Volatile Int State ）和一个先进先出（ FIFO ）的线程等待队列来实现一个多线程访问共享资源的同步框架。\n一、AQS原理         AQS 为每个共享资源都设置一个共享资源锁，线程在需要访问共享资源时首先需要获取共享资源锁，如果获取到了共享资源锁，便可以在当前线程中使用该共享资源，如果获取不到，则将该线程放入线程等待队列，等待下一次资源调度，具体的流程如图 -14所示。许多同步类的实现都依赖于AQS ，例如常用的 ReentrantLock、Semaphore、CountDownLatch。\n\n \n二、state：状态        Abstract Queued Synchronizer 维护了 volatile int 类型的变量，用于表示当前的同步状态。volatile虽然不能保证操作的原子性，但是能保证当前变量state的可见性。        state的访问方式有三种： getState()、setState()和 compareAndSetState()，均是原子操作，其中，compareAndSetState的实现依赖于 Unsafe的compareAndSwaplnt() 具体的。JDK 码实现如下：\n\n三、AQS共享资源的方式：独占式和共享式        AQS 定义了两种资源共享方式 ：独占式 (Exclusive)和共享式(Share)\n\n独占式:只有一个线程能执行，具体的 Java 实现有 ReentrantLock。\n共享式：多个线程可同时执行，具体的 Java 实现有 Semaphore和CountDownLatch。\n\n        AQS只是一个框架 ，只定义了一个接口，具体资源的获取、释放都 由自定义同步器去实现。不同的自定义同步器争用共享资源的方式也不同，自定义同步器在实现时只需实现共享资源state的获取与释放方式即可，至于具体线程等待队列的维护，如获取资源失败入队、唤醒出队等， AQS 已经在顶层实现好，不需要具体的同步器再做处理。自定义同步器的主要方法如表 3-4 所示:\n\n        同步器的实现是 AQS的核心内存。 ReentrantLock对AQS的独占方式实现为：ReentrantLock中的state初始值为0表示无锁状态。在线程执行 tryAcquire()获取该锁后ReentrantLock中的state+1，这时该线程独占ReentrantLock锁，其他线程在通过tryAcquire() 获取锁时均会失败，直到该线程释放锁后state再次为0，其他线程才有机会获取该锁。该线程在释放锁之前可以重复获取此锁，每获取一次便会执行一次state+1, 因此ReentrantLock也属于可重入锁。 但获取多少次锁就要释放多少次锁，这样才能保证state最终为0。如果获取锁的次数多于释放锁的次数，则会出现该线程一直持有该锁的情况；如果获取锁的次数少于释放锁的次数，则运行中的程序会报锁异常。        CountDownLatch对AQS的共享方式实现为：CountDownLatch 将任务分为N个子线程去执行，将 state 初始化为 N, N与线程的个数一致，N个子线程是井行执行的，每个子线程都在执行完成后 countDown()1次， state 执行 CAS 操作并减1。在所有子线程都执行完成( state&#x3D;O)时会unpark()主线程，然后主线程会从 await()返回，继续执行后续的动作。\n        一般来说，自定义同步器要么采用独占方式，要么采用共享方式 ，实现类只需实现tryAcquire、tryseAcquireShared、tryReleaseShared 中的一组即可。但AQS也支持自定义同步器同时实现独占和共享两种方式，例如 ReentrantReadWriteLock 在读取时采用了共享方式，在写入时采用了独占方式。\nArticle link： https://tqgoblin.site/post/csdn/什么是AQS/  Author： Stephen  \n","slug":"csdn/什么是AQS","date":"2022-10-10T15:35:06.000Z","categories_index":"Java","tags_index":"面试 java 面试","author_index":"Stephen"},{"id":"2a388c0d390bb47b986af930ed26f977","title":"什么是 CAS","content":"一、CAS 的概念：比较并交换        CAS (Compare And Swap）指比较并交换。CAS算法 CAS(V,E,N)包含3个参数，V表示要更新的变量，E表示预期的值，N表示新值。 在且仅在V值等于E值时，才会将V值设为N，如果V值和E值不同，则说明已经有其他线程做了更新，当前线程什么都不做。 最后CAS返回当前V的真实值。\n二、CAS 的特性：乐观锁        CAS操作采用了乐观锁的思想，总是认为自己可以成功完成操。在有多个线程同时使用CAS操作一个变量时，只有一个会胜出并成功更新，其余均会失败。失败的线程不会被挂起，仅被告知失败，并且允许再次尝试，当然，也允许失败的线程放弃操作。基于这样的原理，CAS 操作即使没有锁，也可以发现其他线程对当前线程的干扰，并进行恰当的处理。\n三、CAS 自旋等待        在JDK的原子包java.util.concurrent.atomic 里面提供了一组原子类，这些原子类的基本特性就是在多线程环境下，在有多个线程同时执行这些类的实例包含的方法时，会有排他性。其内部便是基于CAS算法实现的,即在某个线程进入方法中执行其中的指令时，不会被其它线程打断；而别的线程就像自旋锁一样，一直等待该方法执行完成才由JVM从等待的队列中选择另一个线程进入。        相对于synchronized阻塞算法，CAS是非阻塞算法的一种常见实现。由于CPU的切换比CPU指令集的操作更加耗时，所以 CAS 的自旋操作在性能上有了很大的提升。JDK具体的实现源码如下：\n        在以上代码中， getAndlncrement采用了 CAS操作，每次都从内存中读取数数据然后将此数据和加1的结果进行CAS操作，如果成功，则返回结果，否则重试值到成功为止。 \n四、ABA 问题        对CAS算法的实现有个重要的前提：需要取出内存中某个时刻的数据，然后在下一时刻进行比较、替换，在这个时间差内可能数据已经发生了变化，导致产生ABA问题。        ABA问题指第1个线程从内存的V位置取出A，这时第2个线程也从内存中取A,并将V位置的数据首先修改为B，接着又将V位置的数据修改为A，这时第1个线程在进行CAS操作时会发现在内存中仍然是A，然后第1个线程操作成功。尽管从第1个线程的角度来说CAS操作是成功的，但在该过程中其实V位置的数据发生了变化，只是第1线程没有感知到罢了，这在某些应用场景下可能出现过程数据不一致的问题。        部分乐观锁是通过版本号（version）来解决 ABA 问题的，具体的操作是乐观锁每次在执行数据的修改操作时都会带上一个版本号，在预期的版本号和数据的版本号一致时就可以执行修改操作，并对版本号执行加1操作，否则执行失败。因为每次操作的版本号都会随之增加，所以不会出现 ABA 问题，因为版本号只会增加，不会减少。\nArticle link： [https://tqgoblin.site/post/csdn/什么是 CAS&#x2F;](https://tqgoblin.site/post/csdn/什么是 CAS&#x2F;)  Author： Stephen  \n","slug":"csdn/什么是 CAS","date":"2022-10-10T14:58:41.000Z","categories_index":"面试","tags_index":"Java java 面试","author_index":"Stephen"},{"id":"32c93abe435d8f794f6c98ee11259476","title":"Java阻塞队列","content":"概述        队列是种只允许在表的前端进行删除操作，而在表的后端进行插入操作的线性表。阻塞队列和一般队列的不同之处在于阻塞队列是“阻塞”的，这里的阻塞指的是操作队 列的线程的一种状态。在阻塞队列中，线程阻塞有如下两种情况：\n\n消费者阻塞：在队列为空时·，消费者端的线程都会被自动阻塞（挂起），直到有数据放入队列，消费者线程会被自动唤醒并消费数据，如图所示\n\n\n\n生产者阻塞：在队列已满且没有可用空间时，生产者端的线程都会被自动阻塞（挂起），直到队列中有空的位置腾出，线程会被自动唤醒并生产数据，如图所示\n\n\n一、阻塞队列的主要操作阻塞队列的主要操作有插入操作和移除操作。插入操作有 a dd(e) 、o ffer(e)、 put(e)、offer(e,time,unit), 移除操作有 remove()、 poll ( )、take() 、poll(time,unit) ，具体介绍如下\n1.插入操作\npublic abstract boolean add(E paramE): 将指定的元素插人队列中，在成功时返回 ture,如果当前没有可用的空间，则抛出IllegalStateException。如果该元素是null， 则抛出NullPointerException异常。Jdk源码的实现如下：\n\n  \n \n\npublic abstract boolean offer(E paramE): 将指定的元素插入队列中，在成功时返回true,如果当前没有可用的空间，则返回false。jdk源码的实现如下：\n\n\n\noffer(E o, long timeout, TimeUnit unit）： 将指定的元素插入队列中，可以设定等待的时间，如果在设定的等待时间内仍不能向队列中加入元素， 则返回 false。\n\npublic abstract void put(E p aramE) throws InterruptedException 将指定的元素插入队列中，如果队列已经满了， 则阻塞、等待可用的队列空间的释放，直到有可用的队列空间释放且插入成功为止，JDK 源码的实现如下\n\n\n\n2.获取数据操作\npoll(）取走队列队首的对象，如果取不到数据，则返回null。jdk源码的实现如下：\n\n\n\npoll(long timeout, T imeUnit unit )：取走列队首的对象，如果在指定的时间内队列有数据可取，则返回队列中的数据，否则等待一定时间 ，在等待超时并且没有数据可取时，返回null。\n\ntake ()取走列队首的对象,如果队列为空，则进入阻塞状态等待，知道队列有新的数据被加入，再及时取出新加入的数据。jdk源码的实现如下\n\ndrainTo(Collection collection） 一次性从队列中批量获取所有可用的数据对象，同时可以指定获取数据的个数，通过该方法可以提升获取数据的效率， 避免多次频繁操 作引起的队列锁定。\n\n\n \n二、Java中的阻塞队列实现Java 中 的阻塞 队列有： ArrayBlockingQueue 、LinkedBlockingQueue 、PriorityBlockingQueue 、DelayQueue、SynchronousQueue、LinkedTransferQueue、LinkedBlockingDeque。具体功能如下：\n\n\nArrayBlockingQueue\n\n        ArrayBlockingQueue 是基于数组实 现的有界阻塞 队列。ArrayBlockingQueue 队列按照 先进先出原则对元素进行排序，在 默认情 况下不保证元 素操 作的 公平性。\n        队列操作的公平性指在生 产者线程或消费者线程发生 阻塞 后再次被 唤醒时，按照 阻塞的先 后顺序操作队列， 即先阻塞 生产者线程 优先向队列中插入元素，先阻塞的消费 者线程 优先从队列中 获取元素 。 因为保证公平性会降低吞吐 ，所 以如果要 处理的 数据 没有先后顺序， 则对 其可 以使用非 公平处理 的方式。我 们可以通 过以下代码创建一 个公平或者非公平的阻塞 队列：\n\n\n LinkedBlockingQueue\n\nLinkedBlockingQueue是基于链表实现的阻塞队列，同ArrayBlockingQueue类似，此队列按照先进先出原则对元素进行排序，LinkedBlockingQueue对生产者端和消费者端分别采用两个独立的锁来控制数据同步，我们可以将队列头部的锁理解为写锁，尾部的锁理解成读锁，因此生产者和消费者可以基于各自独立的锁并行的操作队列中的数据，队列的并发性能较高，具体用法如下：\n\nPriorityBlockingQueue\n\nPriorityBlockingQueue是一个支持优先级的无界队列。元素在默认情况下采用自然顺序升序排序。可以自定义实现compareTo方法来指定元素进行排序规则，或者在初始化PriorityBlockingQueue时指定构造参数Comparator来实现对元素的排序。注意：如果两个元素的优先级相同，则不能保证该元素的存储和访问顺序。具体用法如下：\n\n\n DelayQueue\n\n        DelayQueue是一个支持延时获取元素的无界阻塞队列，在队列底层使用PriorityQueue实现。DelayQueue队列中的元素必须实现Delayed接口，该接口定义了在创建元素时该元素的延迟时间，在内部通过为每个元素的操作加锁来保障数据的一致性。只有在延迟时间到后才能从队列中提取元素。我们可以将 DelayQueue运用于以下场景中\n         缓存系统的设计：可以用 DelayQueue保存缓存元素的有效期，使用一个线程循环查询DelayQueue一旦能从 DelayQueue 获取元素， 表示缓存的有效期到了。\n        定时任务调度：使用 DelayQueue 保存即将执行的任务和执行时间，一旦从 DelayQueue 中获取元素，就表示任务开始执行，Java 中的 TimerQueue 就是使用 DelayQueue 实现的。\n        在具体使用时，延迟对象必须先实现Delayed类并实现其getDelay方法和compareTo方法，才可以在延迟队列中使用：\n\n\n SynchronousQueue\n\nSynchronousQueue时一个不存储元素的阻塞队列。SynchronousQueue中的每一个put操作都必须等待一个take操作完成，否则不能继续添加元素。我们可以将SynchronousQueue看作一个快递员，它负责把生产者线程的数据直接传递给消费线程，非常适用于传递型场景，比如将在一个线程中使用的数据传递给另一个线程使用。SynchronousQueue的吞吐量高于ArrayBlockingQueue 、LinkedBlockingQueue，具体的使用方法如下：\n \n\n LinkedTransferQueue\n\n        LinkedTransferQueue是基于链表结构实现的无界阻塞TransferQueue队列。相对于其他阻塞队列，LinkedTransferQueue多了个transfer、tryTransfer和tryTransfer(E e,long timeout,TimeUint unit)方法。\n        transfer方法：如果当前有消费者正在等待接收元素， transfer 方法就会直接把生产者传入的元素投递给消费者并返回 true。如果没有消费者在等待接收元素，transfer 方法就会将元素存放在队列的尾部（ taiI ）节点，直到该元素被消费后才返回。\ntryTransfer方法:首先尝试能否将生产者传入的元素直接传给消费者，如果没有消费者等待接收元素，则返回 false。和transfer方法的区别是，无论消费者是否接收元素，tryTransfer方法都立即返回，而 transfer 方法必须等到元素被消费后才返回。\ntryTransfer(E e,long timeout,TimeUint unit)方法：首先尝试生产者传入的元素直接传给消费者，如果没有消费者，则等待指定的时间，在超时后如果元素还没 有被消费，则返回 false ，否则返回true。\n\nLinkedBlockingDeque\n\n        LinkedBlockingDeque是基于链表结构实现的双向阻塞队列，可以在队列的两端分别执行插入和移出元素操作。这样，在多线程同时操作队列时，可以减少一半的锁资源竞争，提高队列的操作效率。\n        LinkedBlockingDeque 相比其他阻塞 队列， 多了 addFirst、 addLast、 offer First、offerLast、 peekFirst 、 peekLast 等方法 First 结尾的方法 表示在 队列头部执行插入 (add ）、获取（ peek ）、移除（ offer ）操 作；以 Last 结尾的方法 表示在 队列的尾部执行插 入、获取、移除操作。\n        在初始化LinkedBlockingDeque 时，可以设置队列的大小以防止内存溢出，双向阻塞 队列也常被用于工作窃取模。\nArticle link： https://tqgoblin.site/post/csdn/Java阻塞队列/  Author： Stephen  \n","slug":"csdn/Java阻塞队列","date":"2022-10-10T09:52:53.000Z","categories_index":"Java","tags_index":"Java","author_index":"Stephen"},{"id":"f003f641d410fd391760b84b742d0677","title":"如何排查java程序导致的cpu和内存过高异常","content":"引言Java程序在实际生产过程中经常遇到CPU或内存使用率高的问题，那么应该如何排查问题的原因呢，本文大概描述一下排查方法。\n一、命令方式分析1.排查占用CPU的进程\n使用top命令，在大写打开的情况下按P键或者在大写没有打开的情况下按 shift+P键，会按照CPU使用率的高低进行排序（在大写打开的情况下按M键或者在大写没有打开的情况下按 shift+M键，会按照内存使用率的高低进行排序），查找使用率最高的进程获取进程PID。\n\n2.查找实际占用最高的线程\n使用命令top -H -p PID,此处PID就是上一步获取的进程PID，通过此命令可以查看实际占用CPU最高的的线程的ID，此处几位TID\n\n3.获取对应线程的线程栈信息\n使用命令printf “%x\\n” tid，将线程ID转换为16进制\n使用命令jstack PID(进程pid) |grep tid(线程pid16进制) -A 50，此处tid为上一步转换后的16进制，使用此命令可以查看到对应线程的线程栈信息，从对根据线程栈对对应的代码进行分析\n\n二、dump文件分析1.dump文件输出\n输出dump文件方式一：jmap -dump:live,format&#x3D;b,file&#x3D;aaa.dump pid\n输出dump文件方式二：jcmd pid GC.heap_dump smpo1.dump\n内存溢出输出dump:     -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath&#x3D;路径\n\n注：命令直接复制可能执行报未找到命令的话就手敲\n2.可视化工具分析之后用dump分析工具打开分析堆栈信息，比如jdk自带的jvisualvm\n\nArticle link： https://tqgoblin.site/post/csdn/如何排查java程序导致的cpu和内存过高异常/  Author： Stephen  \n","slug":"csdn/如何排查java程序导致的cpu和内存过高异常","date":"2022-10-09T06:32:40.000Z","categories_index":"Java","tags_index":"Java linux jvm","author_index":"Stephen"},{"id":"80846fe2badad166d0ce9fff99b0e646","title":"Kafka中怎么保证消息不会丢失和不重复消费？","content":"从两个方面分析：消息推送和消息消费。也就是生产者和消费者两方面。首先我们需要知道topic的概念。\n\nTopic ： 话题，可以理解为一个队列， 生产者和消费者面向的都是一个 topic。\n 一个topic又有多个partition，而每个分区都有若干个副本：一个 leader 和若干个 follower。\n\n生产者在推送消息时，会确定topic和topic中的那个partition。\n一个消费者组内每个消费者负责消费 一个topic中不同分区的数据，同一个分区同时只能由一个组内消费者消费\n一、生产者推送消息时怎么保证消息不丢失和不重复对于生产者，在推送消息的时候，有以下几种方式来确定topic、topic中的partition。\n将 producer 发送的数据封装成一个 ProducerRecord 对象。\n1.指明 partition 的情况下，直接将指明的值直接作为 partiton 值；2.没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值；3.既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition值，也就是常说的 round-robin 算法。\n1.1、总体概况 \n为保证 producer 发送的数据，能可靠的发送到指定的 topic， topic 的每个 partition 收到producer 发送的数据后，并等待该分区中全部的follower同步完成，该分区的leader才向 producer 发送 ack（acknowledgement： 确认收到），如果producer 收到 ack， 就会进行下一轮的发送，否则重新发送数据。\n而为了处理follower在同步数据时发生故障，导致leader一直等待下去的情况，新增了ISR的机制。\n1.1.2、什么是ISR呢？Leader 维护了一个动态的 in-sync replica set (ISR：同步副本)，意为和 leader 保持同步的 follower 集合。当 ISR 中的 follower 完成数据的同步之后，leader 就会给 producer 发送 ack。如果 follower长时间未向leader同步数据，则该 follower 将被踢出 ISR，该时间阈值由replica.lag.time.max.ms参数设定。而如果Leader 发生故障，就会从 ISR 中选举出新的 leader。\n1.1.3、ACK机制对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等 ISR 中的 follower 全部接收成功，才返回ack。\n所以 Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。\nacks 参数配置：\n\n0： producer 不等待 broker（或者说是leader）的 ack，这一操作提供了一个最低的延迟， broker 一接收到还没有写入磁盘的数据就已经返回，当 broker 故障时有可能丢失数据；\n1： producer 等待 broker 的 ack， partition 的 leader 落盘成功后返回 ack，如果在 follower同步成功之前 leader 故障，那么将会丢失数据；\n-1（all） ： producer 等待 broker 的 ack， partition 的 leader 和 ISR 里的follower 全部落盘成功后才返回 ack。但是如果在 follower 同步完成后， broker 发送 ack 之前， leader 发生故障，那么会造成数据重复。（假如ISR中没有follower，就变成了 ack&#x3D;1 的情况）\n\n1.1.4、三种语义\nAt Most Once 语义：\n\n将服务器 ACK 级别设置为 0，可以保证生产者每条消息只会被发送一次，即 At Most Once 语义。\n此语义可以保证数据不重复，但是不能保证数据不丢失。\n\nAt Least Once 语义：\n\n将服务器的 ACK 级别设置为-1（all），可以保证 Producer 到 Server 之间不会丢失数据，即 At Least Once 语义。\n此语义可以保证数据不丢失，但是不能保证数据不重复。\n\nExactly Once 语义：\n\nAt Least Once + 幂等性 &#x3D; Exactly Once\n幂等性：所谓的幂等性就是指 Producer 不论向 Server 发送多少次重复数据， Server 端都只会持久化一条。\n要启用幂等性，只需要将 Producer 的参数中 enable.idempotence 设置为 true 即可（此时 ack&#x3D; -1）。 Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。原理：开启幂等性的 Producer 在初始化的时候会被分配一个 PID，发往同一 Partition 的消息会附带 Sequence Number。而Broker 端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时， Broker 只会持久化一条。\n但是 PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以幂等性无法保证跨分区、跨会话的 Exactly Once。（也就是说它只解决单次会话、单个分区里的消息重复问题）\n\n1.2、过程总结总结以上，可以得知生产者在推送消息时，依靠的是ISR、ACK机制、以及三种语义来达到不同情况的消息准确性。\n所以总的过程应该是这样的： producer 向指定的 topic和partition发送数据， topic 的每个 partition 收到producer 发送的数据后，（下一步是等待ISR的follower同步完成，这一步会根据ack的参数配置[0，1，-1]，确定具体的ack返回时机），该分区的leader向 producer 发送 ack（acknowledgement： 确认收到），如果producer 收到 ack， 就会进行下一轮的发送，否则重新发送数据。\n而如果要保证生产者推送到服务器里的消息数据即不重复又不丢失，就要使用Exactly Once语义：将ack参数配置为-1，并开启幂等性(enable.idempotence&#x3D; true)。\n1.3、follower与leader出故障，怎么保证数据的一致性follower 和 leader 发生故障了，该怎么处理。\n\n\nLEO：（Log End Offset）每个副本的最后一个offset；\nHW：（High Watermark）高水位，指的是消费者能见到的最大的 offset， ISR 队列中最小的 LEO；\n\nfollower 故障和 leader 故障：\n\nfollower 故障：follower 发生故障后会被临时踢出 ISR，待该 follower 恢复后， follower 会读取本地磁盘记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。等该 follower 的 LEO 大于等于该 Partition 的 HW，即 follower 追上 leader 之后，就可以重新加入 ISR 了。\nleader 故障：leader 发生故障之后，会从 ISR 中选出一个新的 leader，之后，为保证多个副本之间的数据一致性， 其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader同步数据。\n\n注意： 这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。ack是负责数据丢失的\n二、消费者丢失消息和重复消费消息的情况consumer 采用 pull（拉） 模式从 broker 中读取数据。这个过程只涉及到了服务器和消费者两方，那消费者是怎么保证不丢失和不重复的获取消息呢？\n关键在于consumer会维护一个offset，该offset实时记录着自己消费的位置。同时消费者能见到的最大的 offset，是HW， 是ISR 队列中最小的 LEO【这一点看1.3】，所以只要保证offset不出错，那消息就不会丢失或者重复消费。但是offset的维护并不是那么简单，它分为好几种方式。\noffset的维护方式：\n自动提交\nenable.auto.commit：是否开启自动提交 offset 功能，消费者只在启动的时候去访问offset的值，如果将该值配置为false，就要手动提交offset，否则offset就不会更新。auto.commit.interval.ms：自动提交 offset 的时间间隔手动提交\ncommitSync（同步提交）commitAsync（异步提交）两者的相同点是：都会将本次 poll 的一批数据最高的偏移量提交；不同点是：commitSync 阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而 commitAsync 则没有失败重试机制，故有可能提交失败。无论是同步提交还是异步提交 offset，都有可能会造成数据的漏消费或者重复消费。先提交 offset 后消费，有可能造成数据的漏消费；而先消费后提交 offset，有可能会造成数据的重复消费。自定义存储offsetoffset 的维护是相当繁琐的， 因为需要考虑到很多东西，例如消费者的 Rebalace。\n三、总结在业务的运用中。\n对于消息重复，这个影响不是很严重，无论是生产者重复推送数据，还是消费者重复拉取数据，只要在消费端落库时，手动做去重就可以了。\n对于消息丢失：\n\nconsumer端丢失消息的情形比较简单：如果在消息处理完成前就提交了offset，那么就有可能造成数据的丢失。由于Kafka consumer默认是自动提交位移的，所以在后台提交位移前一定要保证消息被正常处理了，因此不建议采用很重的处理逻辑，如果处理耗时很长，则建议把逻辑放到另一个线程中去做。为了避免数据丢失，可以采用手动提交offset：（1）enable.auto.commit&#x3D;false 关闭自动提交位移、（2）在消息被完整处理之后再手动提交位移\n生产者丢失消息是最复杂的情形了。生产者(Producer) 使用 send 方法发送消息实际上是异步的操作，我们可以通过 get()方法获取调用结果，但是这样也让它变为了同步操作，但是一般不推荐这么做！可以采用为其添加回调函数的形式。这个回调函数会在 Producer 收到 ack 时调用，此处就和acks参数配置[1、0、-1]密切相关了\n如果消息发送失败的话，我们检查失败的原因之后重新发送即可！另外这里推荐为 Producer 的retries（重试次数）设置一个比较合理的值，一般是 3 ，但是为了保证消息不丢失的话一般会设置比较大一点。设置完成之后，当出现网络问题之后能够自动重试消息发送，避免消息丢失。另外，建议还要设置重试间隔，因为间隔太小的话重试的效果就不明显了，网络波动一次，你3次一下子就重试完了。\n\n————————————————版权声明：本文为CSDN博主「努力的布布」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/qq\\_37200262/article/details/125267714\nArticle link： https://tqgoblin.site/post/csdn/Kafka中怎么保证消息不会丢失和不重复消费？/  Author： Stephen  \n","slug":"csdn/Kafka中怎么保证消息不会丢失和不重复消费？","date":"2022-10-09T02:53:10.000Z","categories_index":"mq","tags_index":"kafka","author_index":"Stephen"},{"id":"44f7c35823ef1132fd633487c5c5e40e","title":"golang验证Etherscan上的智能合约","content":"golang验证Etherscan上的智能合约在阅读此文章前，您需要掌握一定的基础知识，如golang与以太坊交互，此篇文章是对其的补充，提供利用代码自动验证智能合约，减少不太必要的人工操作，如果由于Etherscan接口的更新，导致代码不适用，请随时与我联系。\n为什么要验证智能合约出于多种原因，您可能希望在公共区块浏览器上开源（验证）您的智能合约。\n在Etherscan或其他类似的区块链浏览器上验证智能合约具有以下几个重要的用途：\n\n透明度和信任：通过验证智能合约，你向社区展示了你的合约代码是公开的、可审查的。这增加了用户和其他开发者对你项目的信任，因为他们可以查看代码，确认合约行为的逻辑和功能。\n安全性审查：验证后的智能合约会吸引更多人的关注，特别是智能合约专家和安全研究人员。他们可以帮助发现潜在的安全漏洞或问题，并提供改进建议。这有助于提升你的合约的安全性和可靠性。\n抵抗黑客攻击：通过让更多人审查你的合约代码，可以提前发现和修复可能存在的漏洞，从而降低黑客攻击的风险。黑客往往会寻找未经审查或有漏洞的智能合约来进行攻击，而经过验证的合约能够减少这种风险。\n开发者社区增长：开源并验证的智能合约能够鼓励更多开发者参与到你的项目中来。他们可以基于你的代码进行二次开发、添加新功能或者将你的合约作为基础构建新的应用程序，从而推动生态系统的发展和扩展。\n\n总结来说，通过在Etherscan上验证智能合约，你不仅增加了透明度和安全性，还能吸引更多开发者和用户参与到你的区块链项目中来，推动项目的发展和采纳。\n如何使用golang去验证合约如果你是通过在线工具如Remix或OpenZeppelin的合约向导部署了你的合约，请考虑使用合约验证页面进行验证。Verify &amp; Publish Contract Source Code | Etherscan\n如果你是通过开发工具如Hardhat、Foundry、Truffle等部署了你的合约，请考虑使用插件来自动化验证过程。Contract Verification Plugins | Etherscan\n如果你是一名使用golang去开发Web3应用的开发者或学习者，我们可以查看Etherscan提供的api接口，然后根据需要，自己造轮子。Contracts | Etherscan\n这里我们找到验证合约所必要的两个api接口，Verify Source Code和Check Source Code Verification Status，意如其名，一个是将合约源代码提交给类似 Etherscan 的浏览器进行验证，一个是返回合同验证请求的成功或错误状态。\n只要这两个接口都返回成功，我们的智能合约就已经在Etherscan上进行了验证。\n获取EtherscanAPI密钥注册登录-&gt;仪表盘-&gt;API-KEYs-&gt;Add-&gt;copy，具体请看链接Getting an API key | Etherscan。\nVerify Source Code接口将合约源代码提交给类似 Etherscan 的浏览器进行验证。\n\n\n\n根据官方提供的api接口详情，我们可以造出以下的代码，但是略有不同，上面表单中没有提供OptimizationUsed这个参数（0没有，1有），但是我在使用postman测试的时候，得到必须添加这个参数的结果，此外，这个接口必须使用POST请求，尽管它更像是GET请求，没有Body，只有Params。\ngopackage utils\n\nimport (\n    &quot;encoding/json&quot;\n    &quot;fmt&quot;\n    &quot;io&quot;\n    &quot;math/big&quot;\n    &quot;net/http&quot;\n    &quot;net/url&quot;\n    &quot;reflect&quot;\n)\n\ntype VerifyRequest struct &#123;\n    ApiKey               string   `json:&quot;apiKey&quot;`                          // API密钥\n    ChainId              *big.Int `json:&quot;chainId&quot;`                         // 链ID\n    CodeFormat           string   `json:&quot;codeformat&quot;`                      // 代码格式\n    SourceCode           string   `json:&quot;sourceCode&quot;`                      // 合约源代码\n    ConstructorArguments string   `json:&quot;constructorArguements,omitempty&quot;` // 构造函数参数\n    ContractAddress      string   `json:&quot;contractaddress&quot;`                 // 合约地址\n    ContractName         string   `json:&quot;contractname&quot;`                    // 合约名称\n    CompilerVersion      string   `json:&quot;compilerversion&quot;`                 // 编译器版本\n    OptimizationUsed     int      `json:&quot;OptimizationUsed&quot;`                // 是否使用了优化\n&#125;\n\ntype VerifyResponse struct &#123;\n    Status  string `json:&quot;status&quot;`  // 状态\n    Message string `json:&quot;message&quot;` // 详细信息\n    Result  string `json:&quot;result&quot;`  // 具体结果\n&#125;\n\nfunc Verify(apiKey string, chainId *big.Int, codeFormat, sourceCode, constructorArgs, contractAddress, contractName, compilerVersion string, optimizationUsed int) error &#123;\n    // 构造请求数据\n    requestData := VerifyRequest&#123;\n        ApiKey:               apiKey,\n        ChainId:              chainId,\n        CodeFormat:           codeFormat,\n        SourceCode:           sourceCode,\n        ConstructorArguments: constructorArgs,\n        ContractAddress:      contractAddress,\n        ContractName:         contractName,\n        CompilerVersion:      compilerVersion,\n        OptimizationUsed:     optimizationUsed,\n    &#125;\n\n    // 创建一个 Client 实例\n    client := &amp;http.Client&#123;&#125;\n\n    // 准备查询参数\n    params := url.Values&#123;&#125;\n\n    // 使用反射获取requestData结构体中的字段和值\n    val := reflect.ValueOf(requestData)\n\n    // 如果是指针类型，则获取其指向的值\n    if val.Kind() == reflect.Ptr &#123;\n        val = val.Elem()\n    &#125;\n\n    for i := 0; i &lt; val.NumField(); i++ &#123;\n        field := val.Type().Field(i)\n        value := val.Field(i).Interface()\n        // 获取标签的值\n        tag := field.Tag.Get(&quot;json&quot;)\n        if tag == &quot;&quot; &#123;\n            // 如果没有标签，默认使用字段名\n            tag = field.Name\n        &#125;\n        // 将字段名和值添加到查询参数中\n        params.Set(tag, fmt.Sprint(value))\n    &#125;\n\n    // 构建POST请求的URL\n    apiURL := &quot;https://api.etherscan.io/api?module=contract&amp;action=verifysourcecode&amp;&quot; + params.Encode()\n\n    // 创建POST请求\n    req, err := http.NewRequest(&quot;POST&quot;, apiURL, nil)\n    if err != nil &#123;\n        return fmt.Errorf(&quot;创建POST请求失败：%v&quot;, err)\n    &#125;\n\n    // 发送POST请求到Etherscan API\n    resp, err := client.Do(req)\n    if err != nil &#123;\n        return fmt.Errorf(&quot;POST请求失败：%v&quot;, err)\n    &#125;\n    defer resp.Body.Close()\n\n    // 读取响应体\n    body, err := io.ReadAll(resp.Body)\n    if err != nil &#123;\n        return fmt.Errorf(&quot;读取响应体失败：%v&quot;, err)\n    &#125;\n\n    // 解析JSON响应\n    var verifyResponse VerifyResponse\n    err = json.Unmarshal(body, &amp;verifyResponse)\n    if err != nil &#123;\n        return fmt.Errorf(&quot;解析JSON响应失败：%v&quot;, err)\n    &#125;\n\n    // 检查验证提交状态\n    if verifyResponse.Status != &quot;1&quot; &#123;\n        return fmt.Errorf(&quot;验证提交失败：%s，%s\\n&quot;, verifyResponse.Message, verifyResponse.Result)\n    &#125;\n\n    fmt.Printf(&quot;验证提交成功：%s，%s\\n&quot;, verifyResponse.Message, verifyResponse.Result)\n    return nil\n&#125;\n其实在这里，也可以把verifyResponse.Result返回出去，因为接下来的检查验证会用到它。总之根据自己所需，可以随意更改函数形式，代码的灵活性和趣味性不尽也如此了吧。\n参数列表：\n\n\n\n参数\n类型\n描述\n举例\n\n\n\napiKey\nstring\nEtherscan的Api密钥，注册账户免费获取\n**********************************\n\n\nchainId\n*big.Int\n提交验证的链，例如主网1\nbig.NewInt(int64(11155111))\n\n\ncodeFormat\nstring\n单个文件，使用solidity-single-file、使用JSON文件solidity-standard-json-input\nsolidity-single-file\n\n\nsourceCode\nstring\nSolidity 源代码\n// SPDX-License-Identifier……\n\n\nconstructorArgs\nstring\n可选，如果合约使用构造函数参数，则包括\nnil\n\n\ncontractAddress\nstring\n您的合约部署地址\n0x****************************************\n\n\ncontractName\nstring\n合同的名称，例如contracts/Verified.sol:Verified\nVerified\n\n\ncompilerVersion\nstring\n使用的编译器版本，例如v0.8.26+commit.8a97fa7a\nv0.8.26+commit.8a97fa7a\n\n\noptimizationUsed\nint\n是否使用了优化，否0，是1\n0\n\n\n补充：\n获取solc编译器版本：打开cmd，输入\nbashsolc --version\nCheck Source Code Verification Status接口返回合同验证请求的成功或错误状态。\n\n\n这个接口相对于上一个，就好写很多，只是简单的一个GET请求，我们很容易地写出以下代码。\ngopackage utils\n\nimport (\n    &quot;encoding/json&quot;\n    &quot;fmt&quot;\n    &quot;io&quot;\n    &quot;net/http&quot;\n)\n\ntype CheckVerificationStatusRequest struct &#123;\n    ApiKey string `json:&quot;apiKey&quot;` // API密钥\n    Guid   string `json:&quot;guid&quot;`   // 从验证请求收到的唯一值\n&#125;\n\ntype CheckVerificationStatusResponse struct &#123;\n    Status  string `json:&quot;status&quot;`  // 状态\n    Message string `json:&quot;message&quot;` // 详细信息\n    Result  string `json:&quot;result&quot;`  // 具体结果\n&#125;\n\nfunc CheckVerificationStatus(apiKey, guid string) error &#123;\n    requestData := CheckVerificationStatusRequest&#123;\n        ApiKey: apiKey,\n        Guid:   guid,\n    &#125;\n\n    // 创建一个 Client 实例\n    client := &amp;http.Client&#123;&#125;\n\n    // 构建GET请求的URL\n    apiURL := &quot;https://api.etherscan.io/api?module=contract&amp;action=checkverifystatus&amp;guid=&quot; + requestData.Guid + &quot;&amp;apikey=&quot; + requestData.ApiKey\n\n    // 创建GET请求\n    req, err := http.NewRequest(&quot;GET&quot;, apiURL, nil)\n    if err != nil &#123;\n        return fmt.Errorf(&quot;创建GET请求失败：%v&quot;, err)\n    &#125;\n\n    // 发送GET请求到Etherscan API\n    resp, err := client.Do(req)\n    if err != nil &#123;\n        return fmt.Errorf(&quot;GET请求失败：%v&quot;, err)\n    &#125;\n    defer resp.Body.Close()\n\n    // 读取响应体\n    body, err := io.ReadAll(resp.Body)\n    if err != nil &#123;\n        return fmt.Errorf(&quot;读取响应体失败：%v&quot;, err)\n    &#125;\n\n    // 解析JSON响应\n    var checkVerificationStatusResponse CheckVerificationStatusResponse\n    err = json.Unmarshal(body, &amp;checkVerificationStatusResponse)\n    if err != nil &#123;\n        return fmt.Errorf(&quot;解析JSON响应失败：%v&quot;, err)\n    &#125;\n\n    // 检查验证状态\n    if checkVerificationStatusResponse.Status != &quot;1&quot; &#123;\n        return fmt.Errorf(&quot;验证失败：%s，%s\\n&quot;, checkVerificationStatusResponse.Message, checkVerificationStatusResponse.Result)\n    &#125;\n\n    fmt.Printf(&quot;验证成功：%s，%s\\n&quot;, checkVerificationStatusResponse.Message, checkVerificationStatusResponse.Result)\n    return nil\n&#125;\n通过这个函数，我们可以查询我们提交的验证是否已经通过了。\n参数列表：\n\n\n\n参数\n类型\n描述\n举例\n\n\n\napiKey\nstring\nEtherscan的Api密钥，注册账户免费获取\n**********************************\n\n\nguid\nstring\n从验证请求收到的唯一值guid\npdresyk3uidtwtcn5qxp3gqyp4ifsughl9hr8xdt3t2iw7acug\n\n\n演示示例及注意事项然后我用我在上一篇博客，也就是文章最开始的链接中的这个样例合约（已经部署在Sepolia上，但未进行验证），进行演示。\n下面是调用Verify()函数的过程，（注意，因为网络问题，所以我对这个函数进行了小小的修改，所以叫VerifyZh()函数，后面会进行说明的），然后我们得到返回的guid标识。\n\n接着，下面是调用CheckVerificationStatus()函数的过程，（注意，因为网络问题，所以我对这个函数进行了小小的修改，所以叫CheckVerificationStatusZh()函数，后面会进行说明的），然后我们看到我们的验证已经完成了！\n\n当然也有可能失败，因为验证是需要排队的，等待一段时间，再次查询就好了。\n\n然后打开Etherscan，查看我们的合约，可以看到它的Contract上多了一个√，我们点击后，就可以看到我们合约的源代码以及其他信息了。\n\n\n注意\n无法验证合约，有很多原因，请根据报错信息自行摸索，或者在下方留言。\n此外Etherscan很聪明，如果别人已经验证过一个合约，然后你部署了跟他一模一样的字节码，就可能导致你部署的合约不用验证，就已经验证了。\n网络问题无法调用Etherscan接口（最重要的步骤）打开魔法软件面板，点击设置，查看代理端口。\n\n或者，打开windows设置，点击网络和Internet-&gt;代理-&gt;手动设置代理服务器-&gt;编辑，查看端口。\n\n然后将之前的代码中的如下代码\ngo    // 创建一个 Client 实例\n    client := &amp;http.Client&#123;&#125;替换为（注意端口号！！！）\ngo    // 创建一个自定义的 Transport 实例\n    transport := &amp;http.Transport&#123;\n        Proxy: func(req *http.Request) (*url.URL, error) &#123;\n            return url.Parse(&quot;http://127.0.0.1:7897&quot;) // 设置代理（注意端口号！！！）\n        &#125;,\n    &#125;\n\n    // 创建一个自定义的 Client 实例\n    client := &amp;http.Client&#123;\n        Transport: transport, // 设置 Transport\n    &#125;就可以了，我的加了Zh的函数就是修改了这段代码。\n上述方法适用于大部分因网络问题，无法调用第三方api接口的问题。\n这篇文章到这里就结束了，希望对您有所帮助，欢迎点赞、评论加收藏，您的支持是对我最大的帮助！\nArticle link： https://tqgoblin.site/post/csdn/golang验证Etherscan上的智能合约/  Author： Stephen  \n","slug":"csdn/golang验证Etherscan上的智能合约","date":"2022-07-07T13:33:39.000Z","categories_index":"区块链学习","tags_index":"go语言学习 golang 智能合约 开发语言 Etherscan 验证","author_index":"Stephen"},{"id":"85c9259013736ba6783892461c9c0cc9","title":"go语言--区块链学习（三）","content":"模块三：以太坊想要深入学习智能合约的小伙伴，一定不要错过的视频：\njavascript版：（32 小时最全课程）区块链，智能合约 &amp; 全栈 Web3 开发_哔哩哔哩_bilibili\npython版：【公开课】Python区块链开发教程 Solidity web3 区块链和智能合约（中英字幕）_哔哩哔哩_bilibili\n3.1 智能合约3.1.1 普通合约普通合约是指双方或多方在法律框架下，通过书面形式或口头协议达成的一种约定。它是一种约束双方行为的法律文件，规定了合同期限、支付方式、责任承担、违约处理等细节问题。普通合约通常需要法院或仲裁机构的介入来解决争议。\n3.1.1.1 特点\n法律约束力：普通合约是在法律框架下达成的约定，具有法律约束力。\n需要履行：普通合约规定了双方的权利和义务，需要各方按照约定履行。\n可变性：普通合约可以通过双方协商修改，但必须符合法律规定。\n争议解决：普通合约在履行过程中可能出现争议，需要法院或仲裁机构介入解决。\n\n普通合约广泛应用于商业活动、房地产交易、雇佣关系等各个领域。它为各方提供了一种明确的约定，保障了各方的权益，并促进了交易的顺利进行。\n3.1.2 智能合约智能合约简单来说就是部署在去中心化区块链上的一个合约或者一组指令，当这个合约或者这组指令被部署以后，它就不能被改变了，它会自动执行，每个人都可以看到合约中的条款。更深层次的理解是，这些代码会被去中心化地执行。\n3.1.2.1 特点\n自动执行：一旦满足了预设的条件，智能合约将自动执行相关的操作，无需人为干预。\n透明性：智能合约的代码和执行结果都会被保存在区块链上，任何人都可以查看和验证。\n不可篡改性：一旦部署到区块链上，智能合约的代码是不可更改的，确保了交易的可信性和安全性。\n去中心化：智能合约在区块链网络上的节点上执行，没有单一的控制机构，参与者共同验证和维护合约的执行。\n\n智能合约可以应用于各种领域，如数字货币交易、供应链管理、投票系统等。它提供了一种安全、高效、可信赖的方式来进行交易和合作，减少了中间环节和风险，提升了交易的效率和可靠性。\n3.1.2.2 步骤\n定义合约：首先，开发者需要定义智能合约的结构、功能和逻辑。这涉及到确定合约的变量、函数和事件，以及定义它们之间的关系和交互方式。\n编写代码：根据合约的定义，开发者使用合约编程语言（如Solidity）编写智能合约的代码。代码中包含了合约的变量、函数和事件的具体实现。\n编译合约：编写完智能合约代码后，开发者需要使用合约编译器将其编译成字节码。编译过程会检查代码的语法和逻辑错误，并生成可在区块链上执行的字节码文件。\n部署合约：部署合约是将合约字节码上传到区块链网络，并在区块链上创建合约实例的过程。开发者需要选择适当的区块链网络，并支付一定的交易费用来完成合约的部署。\n调用合约：一旦合约被部署到区块链上，其他用户可以通过调用合约的函数与之交互。调用合约需要构造交易并发送到区块链网络，以执行合约中定义的相应操作。\n执行合约：当交易被打包进区块并被确认后，合约的函数将被执行。合约的执行结果会被记录在区块链上，并对所有参与者可见。\n合约状态更新：合约的执行可能会导致合约的状态发生变化，例如更新合约的变量值或触发特定的事件。这些状态变化会被写入区块链，并对其他人可见。\n\n智能合约的步骤可以根据具体的区块链平台和开发工具有所不同，上述步骤仅为一般性的概述。开发者需要熟悉所使用的具体平台和工具的文档和指南，以确保正确地编写、部署和调用智能合约。\n3.1.2.3 应用DApp（去中心化应用）：常见的DApp包括去中心化金融（DeFi）应用、数字身份验证应用、去中心化交易所（DEX）、游戏等。随着区块链技术的不断发展，DApp将继续在各个领域推动创新，并改变传统的中心化应用模式。\n3.2 以太坊介绍3.2.1 什么是以太坊以太坊（Ethereum）是一种基于区块链技术的开源平台，旨在构建和运行智能合约和去中心化应用程序（DApps）。以太坊于2015年由Vitalik Buterin等人提出，并于同年上线。\n以太坊的核心是一个分布式计算平台，它使用以太坊虚拟机（Ethereum Virtual Machine，EVM）来执行智能合约。智能合约是一种自动执行的计算机程序，可以在去中心化网络上进行部署和操作。以太坊的设计目标是提供一个灵活且高度可编程的平台，使开发者能够构建各种类型的应用程序，而不仅限于传统的货币交易。\n以太坊的原生加密货币称为以太币（Ether，简称ETH），它是以太坊网络中的内部计算资源，并且被用作支付交易费用和奖励矿工的费用。以太币也是其他基于以太坊平台上的代币发行的基础。\n以太坊的特点包括具有可扩展性、安全性和去中心化的特性。它提供了一个开放的平台，使开发者能够构建基于区块链的应用程序，并通过智能合约实现自动化和可编程的功能。\n3.2.2 官网地址英文版：Home | ethereum.org\n中文版：首页 | ethereum.org\n3.2.3 浏览器交易地址Ethereum (ETH) Blockchain Explorer (etherscan.io)\n以太坊环境准备\n3.2.4 私有链、公有链、联盟链私有链（Private Chain）是指基于区块链技术构建的仅限于特定组织或个人使用的链。\n特点：私有链的参与者和节点都由同一个实体控制，因此可以更好地控制链的权限、隐私性和性能。\n应用：私有链通常用于企业内部的区块链解决方案，旨在提高效率、安全性和数据保密性。\n公有链（Public Chain）是指开放给任何人参与的区块链网络。\n特点：公有链的参与者可以匿名，任何人都可以创建账号、交易和参与共识过程。公有链的特点是去中心化和透明，所有的交易和操作都可以被公开验证。\n应用：比特币和以太坊就是典型的公有链。\n联盟链（Consortium Chain）是介于私有链和公有链之间的一种形式。\n特点：它由一组特定的组织或实体共同管理和控制，这些组织之间达成共识并共享资源。\n应用：联盟链通常用于跨组织合作的场景，如供应链管理、金融行业等，可以提供更高的效率、可扩展性和安全性。\n3.2.5 比特币和以太坊区别：\n\n目的和设计思路不同：比特币最初是作为一种数字货币而设计的，旨在解决中心化货币系统中的问题，如中心化的发行机构、通胀等。以太坊则是为了实现智能合约而设计的，它不仅可以实现货币的交易，还可以创建分布式应用程序（DApp）和自动化的智能合约。\n\n区块时间和容量不同：比特币的区块时间为10分钟，而以太坊的区块时间为15秒。这意味着比特币网络处理交易速度较慢，而以太坊网络处理交易速度较快。此外，比特币区块容量为1MB，而以太坊没有固定的区块容量限制。\n\n挖矿算法不同：比特币使用SHA-256算法进行挖矿，而以太坊使用Ethash算法进行挖矿。这两种算法都需要大量的计算能力来完成，但是Ethash相对于SHA-256更加抗ASIC，因此以太坊网络上有更多的GPU挖矿。\n\n智能合约功能不同：以太坊的智能合约功能比比特币更加强大和灵活。以太坊的智能合约可以处理复杂的逻辑和条件，如创建新的代币、触发交易、实现去中心化金融应用等。与此同时，比特币只支持基本的交易类型和脚本语言。\n\n\n3.3 metamask钱包的使用MetaMask安装及使用（全网最全！！！）_metamask如何安装-CSDN博客\n3.4 remix的使用Remix-Desktop安装-CSDN博客\n3.5 solidity基础Solidity基础（详细易懂！！！）_solidity需要什么基础-CSDN博客\n\n因为博主近期忙于学习其他知识，以下两个知识点，只给出大体方向，实在抱歉！！！\n如果可以的话，也许以后可以补上。\n如果只是单纯的以太坊学习，做智能合约开发，而不是使用go语言去和以太坊做一个交互，也可以使用web3.js（主流）去做交互，具体的学习方式请看：\njavascript版：（32 小时最全课程）区块链，智能合约 &amp; 全栈 Web3 开发_哔哩哔哩_bilibili\n3.6 go调用以太坊golang与以太坊交互-CSDN博客\n3.7 go调用智能合约golang与以太坊交互-CSDN博客\nArticle link： https://tqgoblin.site/post/csdn/go语言--区块链学习（三）/  Author： Stephen  \n","slug":"csdn/go语言--区块链学习（三）","date":"2022-04-22T13:49:35.000Z","categories_index":"区块链学习","tags_index":"区块链 golang 学习","author_index":"Stephen"},{"id":"0f42ae74b0881a5537a75ba29aedf346","title":"在以太坊测试网上部署合约","content":"在以太坊测试网上部署合约前言在部署合约之前，您需要确保你的操作系统上已经安装了MetaMask，并熟练掌握其使用方法，以及掌握了部分Solidity基础语法知识。\n如果没有，请移步到下面链接，确保您以及做好了充足的准，开始接下来的合约部署。\nMetaMask的安装及使用：MetaMask安装及使用（全网最全！！！）_sepoliaeth水龙头-CSDN博客\nSolidity基础语法知识：Solidity基础（详细易懂！！！）-CSDN博客\n值得注意的是！！！\nRemix-Desktop连接MetaMask程序比较繁琐，所以我们将在Remix浏览器版进行我们的部署操作。\nRemix浏览器版：Remix - Ethereum IDE\nRemix-Desktop我们也讲一下，连接MetaMask的具体方式是，点击左下角的插件管理，搜索wallet connect，再带点击Activate，在弹出来的页面中点击connect to a wallet，然后找到我们的MetaMask钱包，连接上就行。\n正文首先打开浏览器，进入我们的Remix浏览器版：Remix - Ethereum IDE（确保你的浏览器已经安装了MetaMask扩展）\n同Remix-Desktop，操作步骤是一样的，我们首先创建一个名字叫SimpleStorage.sol的合约文件，然后粘贴如下代码。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.8.22;\n\ncontract SimpleStorage &#123;\n\n    // This gets initialized to zero!\n    // &lt;- This means that this section is a comment!\n    uint256 public favoriteNumber;\n\n    mapping(string =&gt; uint256) public nameToFavoriteNumber;\n\n    struct People &#123;\n        uint256 favoriteNumber;\n        string name;\n    &#125;\n\n    // uint256[] public favoriteNumbersList;\n    People[] public people;\n\n    function store(uint256 _favoriteNumber) public virtual  &#123;\n        favoriteNumber = _favoriteNumber;\n        retrieve();\n    &#125;\n\n    // view, pure\n    function retrieve() public view returns(uint256) &#123;\n        return favoriteNumber;\n    &#125;\n\n    // calldata, memory, storage\n    function addPerson(string memory _name, uint _favoriteNumber) public &#123;\n        people.push(People(_favoriteNumber, _name));\n        nameToFavoriteNumber[_name] = _favoriteNumber;\n    &#125;\n&#125;\n对于这个SimpleStorage合约。\n我们有一个全局变量favoriteNumber，通过使用store函数，可以往里面存储一个我们喜欢的数字，我们还有一个名为nameToFavoriteNumber的mapping，并且我们还拥有了一个新类型的数组，一种我们创建的名为People的新类型，我们还可以往这些数组和mapping中添加新内容，就用我们创建的这个addPerson函数，这样我们就既能够保存许多个不同的人的favorite number，又能够保存单独一个全局的favorite number，现在我们已经准备好把它发送到一个测试网络上，并让其他人与之互动了。\n当然，一般来说，直到写完测试前不要部署上去，或是直到你做一些非常简单的审核，不过现在让我们先往下走，学习怎么把合约部署到测试网络或者真实网络上，请牢记，测试网完全是人们出于好心而自发运行的，所以哪怕它有点异常，或者没有像我们这里演示的那样工作，没有关系，只要它能与Remix虚拟机（VM）一起工作就行了，但是学习一下如何将这些东西部署到实际的测试网上仍不失为是一次很好的实践活动。\n这里是我们的合约SimpleStorage.sol，编译好了，编译通过后，这里会有一个绿色的小对勾，进入部署选项，现在我们要改变下环境。\n我们正在使用的是Remix VM环境，这是一种虚假的模拟环境，我们现在要转移到lnjected Provider上。\n\n接着我们要把 MetaMask或其他Web3钱包，注入到浏览器中以便能够使用，就像我们使用水龙头时做的那样，我们选择一个想要使用的账户，这里我选择了第二个账户（哪个账户都无所谓，重要的是要有ETH测试币）。\n\n现在我们看到，账户出现在了Remix的ACCOUNT选项中。\n\nlnjected Provider意味着我们正在使用·MetaMask或其他 Web3钱包。\n无论lnjected Provider是哪个网络，或者这样说，我们的钱包连接到了哪个网络上就会部署到那个网络上，在本篇文章中，我们选择部署到Sepolia上（SepoliaETH获取方法在文章前言中介绍的MetaMask安装及使用文章里有）。\n当然，你可以根据你的具体情况，进行选择，如果你有其他测试网络的ETH，你可以选择部署在其他网络上。\n这里我们选择Sepolia，为了部署到测试网上，请记住，我们是需要gas的，所以我们需要一些测试用的以太币，或者你可以部署到以太坊主网上，但你最好别这样做。\n所以现在我们就使用Injected Provider，我们可以通过完全相同的步骤，把合约部署到测试网上，就和部署到虚拟机上一样，并且记住，如果你没有足够的gas来部署合约，那么一定要在部署前来查看一下水龙头，所以我们要做的就是相同的事，去点击这个Deploy按钮。\n\n但是这一次MetaMask会弹出来，并询问我们是否真的要部署这个合约，这就是我们之前在区块链示例中看到的，给交易签名，我们现在其实就在签署和发送这个交易。\n\n这个交易的数据量非常非常庞大，它们就代表着我们刚刚创建的合约，我们可以看到这次交易或者说部署这个合约的所有付款信息。\n\n我们可以看到，部署这个合约将花费大约这么多以太币，再次强调，我们现在处于Sepolia测试网络上，所以这些都是假的以太币，我们继续，点击确认（Confirm）。\n\n如果你弹出这个小控制台，在Remix里，你会在短暂的延迟后看到，它会用这个绿色的小对勾，表示合约已经确实是被部署上了。\n\n我们可以继续复制一下这个交易哈希，并在etherscan上查看这笔交易，稍作延迟后，我们就可以看到这里的交易详情。\nTESTNET Sepolia (ETH) Blockchain Explorer (etherscan.io)\n\n就和我们之前发送以太币的交易详情一样，我们有一个哈希（Hash），有一个状态（status），有区块（Block），有确认区块数（Block Confirmation），还有时间戳（Timestamp），From，就是我们自己，To，是我们刚刚创建的合约的地址，我们这次没有发送任何价值（Value），所以这里是0个以太币，然后当然，我们会看到交易费（Transaction Fee）还有gas价格（Gas Price）。\n\n因为把合约部署到区块链上会修改区块链的状态，所以我们必须为此支付gas，其余的部分在这里也都看得到，正如我们所见，Gas Limit和gas Usage比单纯发送以太币要高得多，这是因为我们把大量数据送到了链上，进而增加了大量的计算量，所以这个数字远远高于单纯发送以太币所需要的21000gas。\n\n现在让我们回到Remix拉到下面，可以看到我们的SimpleStorage合约就在这个地址上。\n\n\n温馨提示：\n如果，没有看到可以交互的合约，可以手动添加，输入合约地址即可，合约地址，可以看刚才etherscan的交易明细中的To，或者看Remix控制台小绿勾中的To，如果不小心退出后，找不到合约地址的话，可以在etherscan中查账户地址的交易记录，从而找到合约地址。\n\n\n紧接上面。\n\n如果我们点击这个copy按钮，然后去到Sepolia etherscan，把地址粘贴到搜索栏里，我们会得到我们刚刚部署的这个合约。\n\n并且可以看到它的第一笔交易，也就是这个合约的创建交易，所以这是我们刚刚创建的合约。有一笔交易，它是在合约创建时产生的。\n所以我们现在已经把这个合约创建出来了，并且拥有和使用Remix虚拟机时完全相同的功能，或者说我们的虚假环境，也可说是超级虚假环境，现在我们就可以做一些之前在Remix VM上做过的操作了，只不过这次是在测试网络上。\n你可以看到，如果我点击retrieve，MetaMask不会弹出，这是因为这个蓝色按钮是一个view函数，如果我们看一下people在0处的值，这也是一个view函数所以 MetaMask同样也没有弹出，nameToFavoriteNumber现在还是空的，所以如果我现在输入zhangsan，当然返回是空了，对吧？其实这里我得到了一个返回值0，这是因为mapping初始化每个值的时候使用都是相应的空值，对uint256来说就是0。\n\n现在我们可以尝试存储一个favorite number，这一操作会导致区块链被修改，所以MetaMask会弹出来让我们确认以及签署该笔交易，从而修改区块链的状态。\n现在我要存储一个我喜欢的数字749，点击store，MetaMask会弹出来，而接下来我们需要做的就是点击确认（Confirm），点击确认就意味着，我们要签署该交易并把它发送到区块链上以修改状态，所以我们继续，点击确认。\n\n我们现在应该能在etherscan上看到这笔交易了，它可能需要一点时间才能索引或者说实际开始工作，所以请耐心等待一下这些测试网络，这也就是为什么你在构建应用程序时，最好是尽量最后一步再部署到测试网上，因为你会在这一步上等待很长时间，并且这也为运行这些测试网络的人带来了负担，他们纯粹是出于好心才运行这些网络的，所以请尽量把这一步作为你实际搭建应用时的最后一步，对于在这里学习的我们来说，倒是关系不大。\n稍作等待后，我们刷新一下。看起来它正在etherscan上建立索引，etherscan网站仍然在确认这笔交易到底进行到哪里了。\n回来看看Remix，看起来在区块链上，交易其实已经通过了，所以如果我们现在去点击retrieve，可以看到，我们最喜欢的数字确实就是749，当然了，nameToFavoriteNumber和people还是空的。\n\n看起来这笔交易已经完成了，etherscan已经把它索引上了。\n现在让我们继续，添加一个人物zhangsan，最喜欢的数字是23，接着我们点people击addPerson，这是个橙色按钮，所以交易会弹出来，因为我们正在修改区块链的状态，我们继续点击确认。\n\n这里需要我们有一点耐心，等待我们的交易通过，届时我们应该就能看到nameToFavoriteNumber会更新，还有people也是，如果我点击nameToFavoriteNumber，输入的是zhangsan，我会得到23，如果点击people，输入参数是0，我会得到favoriteNumber是23，name是zhangsan。\n\n太棒了！你现在已经成功地把一个合约部署到了实际的测试网上，并且能够在etherscan上看到交易的具体细节。\n现在如果你想看着把它部署到其他网络上是什么样的，你只需要在MetaMask上切换到其他测试网就行了。\n顺便期待一下博主的go语言-区块链学习（三），文章正在编辑中。。。\nArticle link： https://tqgoblin.site/post/csdn/在以太坊测试网上部署合约/  Author： Stephen  \n","slug":"csdn/在以太坊测试网上部署合约","date":"2022-02-21T12:42:39.000Z","categories_index":"区块链学习","tags_index":"区块链 学习","author_index":"Stephen"},{"id":"1bb20f7814a6924889d5c5d79f293898","title":"Solidity基础（详细易懂！！！）","content":"Solidity基础官方文档：Home | Solidity Programming Language (soliditylang.org)（有中文译版）\n前言本来是准备把这一篇全部移入go语言–区块链学习（三）中的，考虑到排版问题，全放一起可能会显得十分拥塞，也不方便整体的可读性，影响阅读体验，所以博主将这一知识点的分享提取出来，单独成块，作为一篇博客发表。\n在进入这一篇的学习前，您需要确保你的操作系统上已经安装了Remix以及MetaMask，并熟练掌握其使用方法。\n如果没有，请移步到下面链接，确保您以及做好了充足的准备，迎接接下来的Solidity的学习。\nMetaMask的安装及使用：MetaMask安装及使用（全网最全！！！）_sepoliaeth水龙头-CSDN博客\nRemix-Desktop的安装：Remix-Desktop安装-CSDN博客\n此文章参考了课程：freeCodeCamp S2 - YouTube（ Lesson 2 Pt. 1~Lesson 4 Pt. 18）\n（如果可以的话，推荐您去看视频，如果不想看视频的话，那就请您继续看下去吧）\nRemix-Desktop介绍Remix-Desktop是Remix的桌面版应用程序，它提供了与Remix网页版类似的功能，但是可以在本地计算机上运行。与Remix网页版不同的是，Remix-Desktop不需要通过浏览器访问，而是可以通过下载和安装应用程序来使用。\nRemix-Desktop包含了Solidity编译器、交互式控制台、调试器等工具，可以帮助开发人员更加高效地进行以太坊智能合约的开发和调试。同时，Remix-Desktop还支持与以太坊客户端的连接，可以直接在本地计算机上与以太坊网络进行交互。\n开始使用Remix接下来我们开始使用Remix，首先打开我们的Remix，你会看到如下界面，如果你对中文情有独钟可以，可以在此设置语言。（博主推荐用英文模式，没有为什么）\n\n然后，与浏览器版的Remix很大的不同，在Desktop版中我们可以将写的Solidity代码存储到本地，不会出现下次打开可能会造成数据丢失的情况，这无疑对我们的用户体验来说十分友好。\n而我们初始化的界面，文件浏览器默认路径是Remix所在根目录路径，所以，为了可以更好地管理我们的代码，我们需要自己手动设置一个工作区，里面将用于存放我们的代码，再将浏览器目录切换至工作区。\n\n现在，我们已经配置好了一个空白的Remix，接下来，我们将完成你的第一个智能合约的编写。\n首先，我们创建一个合约的文件夹，名称为contracts，然后，点击合约文件夹，新建文件，会有小的输入栏跳出来，你可以输入文件名，输入SimpleStorage.sol，.sol就是Solidity文件的后缀名。（.sol可以不输，默认是.sol）\n\nSolidity是智能合约的主要编程语言，虽然还有一些别的编程语言，但是Solidity目前还是最主流的。\n然后右边会有SimpleStorage.sol文件，可以开始写Solidity代码。\n你可以先打开左边的Solidity的图标，可以看到编译Solidity代码所需的参数。\n\n好，接下来，我们正式写代码。\n在任何一个Solidity智能合约中，你首先需要的就是Solidity的使用版本，它应该被标注在Solidity代码的最上面，Solidity是一个更新频率很高的语言，和别的语言相比，它总会有新版本，所以我们需要告诉代码，要用哪个版本，我们通Pragma Solidity + 版本号;来约定版本号。（Solidity语句以 ; 结尾）\n版本号有这么一些书写方式：\n^0.8.0：表示支持0.8.0及以上的版本（同 &gt;&#x3D; 0.8.0）\n&gt;&#x3D; 0.8.0 &lt;&#x3D; 0.9.0：表示支持0.8.0及以上0.9.0及以下的版本\n0.6.22：表示只支持0.6.22版本\n同时，在代码最上方，你可以加入SPDX-License-Identifier，最然这个是可选的，但是没有的话，有些编译器会出现警告，这个会定义License和代码分享规则，这里不更多介绍，如果需要解释，可以阅览：What is a software license? 5 Types of Software Licenses Explained | Snyk\n为了标注License，需要在这里写SPDX-License-Identifier，我们这里选择MIT，MIT是限制最少的License之一，我们会在大多数的代码中使用MIT协议。\n接下来，我们输入contract，开始定义智能合约，contract是Solidity的关键字，它告诉编译器后面的代码是来定义智能合约的。（类似面向对象语言中的class关键字）\n然后我们给智能合约起一个名字，就叫它SimpleStorage，再加入一对{ }，在花括号中的代码就是SimpleStorage智能合约的内容。\n当我们写完这些的时候，就可以到编译界面（点击Solidity图标），点击编译（Compile），或者直接按Crtl+S，也是可以的。\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract SimpleStorage &#123;\n\n&#125;然后就可以看见Solidity图标的上面出现了绿色对勾，绿色的对勾表示我们的代码成功编译，并且没有错误。\n\n假设现在部署这个合约，它就会是一个有效的合约。\n数据类型Solidity中最基础的数据类型包括：\n\n布尔类型（bool）：布尔类型表示真或假的值。\n整数类型：整数类型分为有符号和无符号两种。有符号整数类型包括int8、int16、int32、int64等，而无符号整数类型包括uint8、uint16、uint32、uint64等。这些类型表示不同位数的整数。\n固定点数类型：固定点数类型用于处理小数。例如，fixed和ufixed表示固定点数，后面可以跟着小数点的位数。\n地址类型（address）：地址类型表示以太坊网络上的账户地址。\n字节类型（bytes）：字节类型表示一组字节数据。例如，bytes32表示32个字节的数据。\n动态字节数组类型（bytes）：动态字节数组类型与字节类型类似，但其长度可变。\n字符串类型（string）：字符串类型表示文本数据。\n枚举类型（enum）：枚举类型用于定义一组离散的可能取值。\n\n现在，我们只介绍其中几种。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract SimpleStorage &#123;\n    // boolean, uint, int, address, bytes\n    bool hasFavoriteNumber = true;\n    uint256 favoriteNumber = 5;\n    string favoriteNumberInText = &quot;Five&quot;;\n    int256 favoriteInt = -5;\n    address myAddress = 0x4b13f21791e60BBce7003c4315ec965e62c5a52F;\n    bytes32 favoriteBytes = &quot;cat&quot;;\n&#125;其中uint、int、bytes后面都可以跟上具体的数字，表示分配空间的大小，比如uint8，就是分配了8个bit（这里就不解释一些计算机的基础知识了，默认大家至少学过C&#x2F;C++、Java、Golang、Python中的一种），这个数字可以一直大到256，如果不知道被赋值的数字多大，默认就是uint256。\n通常，把分配空间显式地写出来是一个好习惯，所以这里不把uint256缩写为uint，其他数据类型同理。\n如果你想了解更多的数据类型和它们的特性，请查阅Solidity官方文档，跳转链接就在文章开头。\n在Solidity中，如果不给变量赋值，它就会有一个默认值是null的值，比如uint256在Solidity中的默认值是0。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract SimpleStorage &#123;\n\n    // This gets initialized to zero!\n    // &lt;- This means that this section is a comment!\n    uint256 favoriteNumber;\n&#125;补充：\n数据结构的默认值\n在 Solidity 中，变量和状态变量都有默认值。这些默认值取决于变量的类型。\n对于各种数据类型，默认值如下：\n\nbool类型的默认值是 false。\n整数类型（包括 uint 和 int）的默认值是0。\n地址类型（address）的默认值是0x0000000000000000000000000000000000000000。\n字符串类型（string）的默认值是空字符串””。\n动态数组（包括字符串数组）和映射（mapping）的默认值是一个空的、长度为0的集合。\n\n对于结构体和枚举类型，默认值是其成员变量类型的默认值。\n需要注意的是，局部变量在声明时不会自动被赋予默认值，而是由开发者手动初始化。而状态变量在合约部署时会被自动赋予默认值。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract MyContract &#123;\n    bool public myBool;        // 默认值为 false\n    uint public myUint;        // 默认值为 0\n    address public myAddress;  // 默认值为 0x0000000000000000000000000000000000000000\n    string public myString;    // 默认值为 &quot;&quot;\n    uint[] public myArray;     // 默认值为一个空的、长度为 0 的数组\n&#125;如果你希望在声明变量时为其指定一个非默认值，可以在声明时赋初值，或者在构造函数中进行初始化。\nSolidity中的注释\n&#x2F;&#x2F;：单行注释\n&#x2F;**&#x2F;：模块注释\n函数现在，让我们创建一个函数，函数或者方法，指的是独立模块，在我们调用的时候会执行某些指令。\nSolidity的函数和常见的编程语言类似，函数通过function关键字表示。\n这里我们创建一个名字叫store的函数，这个函数会把favoriteNumber改成一个新的值，要改变的数字，是传给store函数的参数，所以我们定义store函数，接受uint256的参数，参数名字是_favoriteNumber，定义为public函数，这个在后面讲到，然后在函数中，我们将favoriteNumber赋值为传入的参数。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract SimpleStorage &#123;\n\n    // This gets initialized to zero!\n    // &lt;- This means that this section is a comment!\n    uint256 favoriteNumber;\n\n    function store(uint256 _favoriteNumber) public &#123;\n        favoriteNumber = _favoriteNumber;\n    &#125;\n&#125;现在store函数会接受一个参数，然后将参数赋值给favoriteNumber。\n为了看看它实际运行结果，我们把合约部署在一个测试环境上，即部署在本地网络上，或者叫Remix VM（以前叫JavaScript VM，如果你看一些课程或者其他教程，如果看见的是JavaScript VM，则不用疑惑，它们是一个东西）。\n首先第一步就是Ctrl+S，或者点击Solidity图标，再点击Compile，编译一下我们的Solidity代码。\n等待编译成功后，点下面的按钮，部署和发送交易区域，在部署和发送交易部分，有很多设置选项。\n首先第一个环境选择Remix VM (Shanghai)，Remix VM表示合约将被部署到本地的Remix（JavaScript）虚拟机上，Remix VM是本地测试用的区块链，在上面可以快速模拟交易，不需要等待测试网的流程。在这里有一些账户，运行Remix VM的时候，我们有很多账户可以部署，每个账户中都有100个以太币，这些账户和MetaMask中账户的类似，区别是，这些是在Remix VM中的测试以太币。在部署的合约的交易中可以设置gas limit，同时可以选择要部署的合约，现在我们只有SimpleStorage合约，也正是要部署的。点击Deploy按钮来部署合约，滑到最下方，可以看到合约已经被部署了。\n\n有一个橘黄色的按钮store，参数是uint 256 _favoriteNumber。在测试环境中，每个合约都有一个地址，就像每个账户都有地址一样，这里的0XD91…39138表示的就是SimpleStorage合约的地址。\n将右下方的窗口调大，这里可以看到一些部署细节，点开下拉菜单可以看到很多信息，有一些你应该知道，是一些熟悉的关键字，像是状态、交易哈希、from、to等等（如果不熟悉，请转移到MetaMask安装及使用中查看，链接在文章最上方的前言中）。\n\n部署合约其实就是发送一个交易，我们在区块链上做任何事情，修改任何状态，就是在发送一个交易。部署一个合约就修改了区块链，让链上拥有这个合约，如果我们在Sepolia，Goerli或者以太坊主网上发送这个交易，我们要支付gas来部署合约。\n这些数据模拟在真正网络部署交易的数据，要支付多少gas、交易哈希、from、to和其他所有数据。但是现在这些数据是Remix VM伪造出来的。\n这个橘黄色的按钮。代表了我们刚创建的函数store。在这里输入一些数据，比如123，然后点击按钮，我们就在Remix VM测试环境中执行了一个交易，把123传入了favoriteNumber中。\n\n接下来往上滑，查看账户，可以看到账户的余额中稍微少了一点以太币。这是因为我们部署合约的时候以及调用合约的时候用了一些gas。\n然后我们可以多试几次，比如输入678，然后点击store可以看到发了一笔交易，在favoriteNumber中存储了678。这里你就可能会有疑惑，favoriteNumber改变了，但是我们看不到（请忽略右边的decoded input），怎么知道favoriteNumber变了呢？\n可见度标识符函数和变量有四种可见度标识符public、private、internal和external。\n\npublic：public 是最高级别的可见度标识符，表示变量、函数或合约对内外部都可见。公共状态变量可以被任何人读取，并且公共函数可以被外部调用者调用。公共函数和状态变量的访问可以通过合约地址直接进行。\nprivate：private 是最低级别的可见度标识符，表示只有当前合约内的其他函数才能访问该变量或函数。私有状态变量只能在当前合约中访问，私有函数只能被当前合约的其他函数调用。私有状态变量和函数对外部调用者是不可见的。\ninternal：internal 表示内部可见性，表示只有当前合约及其派生合约内的其他函数才能访问该变量或函数。内部状态变量和函数可以在当前合约及其派生合约中访问，但对外部调用者不可见。\nexternal：external 可以用于函数，表示该函数只能通过外部消息调用。外部函数只能被其他合约调用，而不能在当前合约内部直接调用。此外，外部函数不能访问合约的状态变量，只能通过参数和返回值进行数据交互。\n\n如果没有显式指定变量的访问修饰符，则默认为internal，而internal关键字表示，它只对本合约和继承合约可见。\n现在这个favoriteNumber变量被设置为了internal，所以我们看不到它。\n现在我们讨论如何看到它，为了让我们看到它，现在把它设置为public重新编译。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract SimpleStorage &#123;\n\n    // This gets initialized to zero!\n    // &lt;- This means that this section is a comment!\n    uint256 public favoriteNumber;\n\n    function store(uint256 _favoriteNumber) public &#123;\n        favoriteNumber = _favoriteNumber;\n    &#125;\n&#125;然后进入部署区域，点击下面的X，把旧的合约删掉。当然只是从这个窗口删掉，但是没有从区块链删掉，因为区块链是不可更改的。当然因为这个是测试环境，所以只是某种程度上不可更改。编译，然后重新部署。\n\n然后发现在新的合约中有两个按钮。\n其中一个橘黄色按钮是函数，还有一个favoriteNumber的按钮，这个按钮代表favoriteNumber变量，就像一个显示变量值的函数（就比如Java里面的getter函数），实际上确实就是一个getter函数，此章节后面将会详细说明。\n如果现在点击这个按钮，会显示什么？因为favoriteNumber初始化的默认值是0，点击一下，我们可以看到显示的是0。现在就是说这个uint256数据类型存储的数值是0。如果现在通过store函数把变量改为678，再点击favoriteNumber按钮，可以看到数值更新为678。\n\n补充：\npublic关键字\n当你在Solidity中使用public关键字修饰一个状态变量时，Solidity编译器会自动生成一个类似getter函数的方法来允许外部调用者读取该变量的值。\n这个自动生成的getter函数会使用与状态变量同名的函数名，并且没有参数。它的返回值类型与状态变量的类型相匹配。通过调用这个getter函数，外部调用者可以获取到状态变量的当前值。\n例如，当你声明一个public的状态变量uint256 public myNumber时，Solidity编译器将会自动生成一个名为myNumber()的函数，用于获取myNumber的值。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract MyContract &#123;\n    uint256 public myNumber;\n&#125;外部调用者可以通过合约地址调用这个自动生成的getter函数来获取myNumber的值。\n需要注意的是，public关键字只会生成一个getter函数用于读取状态变量，而不能直接修改状态变量的值。如果你希望外部调用者能够修改状态变量的值，你可以使用setter函数或将状态变量声明为可写入的。\n状态变量和局部变量在Solidity中，状态变量是指在智能合约中永久存储的变量。它们存储在以太坊区块链上，并且对于每个合约实例都有唯一的值。状态变量定义在合约的顶层范围内，可以在整个合约中被访问和修改。\n在 Solidity 中，局部变量是指在函数内部声明并且只在函数作用域内可见的变量。局部变量的作用域通常仅限于声明它们的函数内部，在函数执行完成后即被销毁，不会永久存储在区块链上。\n简单理解就是全局变量和局部变量，函数外面的是全局变量（状态变量），函数里面的是局部变量。\n作用域在 Solidity 中，变量和函数可以拥有不同的作用域，作用域决定了这些变量和函数的可见性和访问权限。\n以下是 Solidity 中常见的作用域：\n\n全局作用域：在合约的整个范围内可见的变量和函数属于全局作用域。这些变量和函数可以被合约内的任何地方访问。\n合约作用域：在合约内部声明的变量和函数具有合约作用域，它们只能在声明它们的合约内部访问。\n函数作用域：在函数内部声明的变量具有函数作用域，只能在该函数内部访问。这些变量通常被称为局部变量。\n事件作用域：在 Solidity 中，事件也有其特定的作用域。事件在声明它们的合约内部可见，可以被合约内的任何函数调用来触发。\n\n在 Solidity 中，作用域非常重要，因为它决定了变量和函数的可见性和生命周期。合理使用作用域可以提高代码的可读性，降低变量冲突的可能性，并帮助开发者更好地组织和管理代码。\nSolidity的作用域和其他高级编程语言的作用域类似，这里不做更多介绍，可以参考下面的错误代码。\n错误代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract SimpleStorage &#123;\n\n    // This gets initialized to zero!\n    // &lt;- This means that this section is a comment!\n    uint256 public favoriteNumber;\n\n    function store(uint256 _favoriteNumber) public &#123;\n        favoriteNumber = _favoriteNumber;\n        uint256 testVar = 5;\n    &#125;\n\n    function something() public &#123;\n        testVar = 6; // ??\n    &#125;\n&#125;gas消耗每次调用这个store函数，我们都会发送一个交易。因为每次在更改区块链状态的时候，我们都会发送交易，可以在右下角Remix的日志区域，查看交易细节。\n你可以看到交易消耗了多少gas，可以看到这里的数字比发送交易所用到的21000 gas要多的，那是因为这里的操作会消耗更多的计算量。\n\n实际上我们在这里存储了一个数字，现在如果我们在函数中做更多的操作会发生什么？除了在这里存储数据外，我们在存储变量后更新这个变量，让favoriteNumber加1。因为我们加了这样一个操作，这个函数将消耗更多的计算量。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract SimpleStorage &#123;\n\n    // This gets initialized to zero!\n    // &lt;- This means that this section is a comment!\n    uint256 public favoriteNumber;\n\n    function store(uint256 _favoriteNumber) public &#123;\n        favoriteNumber = _favoriteNumber;\n        favoriteNumber = favoriteNumber + 1;\n    &#125;\n&#125;然后我们现在重新编译删掉旧合约，然后重新部署，然后再次输入678，然后查看交易细节，我们可以发现执行交易的消耗的gas变得更多了。那是因为我们做的事情变多了，这个函数消耗的计算量变多了。\n43730 gas -&gt; 44127 gas\n\n每个区块链计算gas的方式不同，但最简单的理解是，做越多的操作，消耗更多的gas。\nview和pure我们给favoriteNumber加上public，就相当于给它创建了gettee函数。\n我们可以创建一个函数来返回favoriteNumber，模拟被自动创建的getter函数，函数名是retrieve，可见标识符是public，返回值为uint256，返回favoriteNumber。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract SimpleStorage &#123;\n\n    // This gets initialized to zero!\n    // &lt;- This means that this section is a comment!\n    uint256 public favoriteNumber;\n\n    function store(uint256 _favoriteNumber) public &#123;\n        favoriteNumber = _favoriteNumber;\n    &#125;\n\n    // view, pure\n    function retrieve() public view returns(uint256) &#123;\n        return favoriteNumber;\n    &#125;\n&#125;Ctrl+S来编译，或者在编译页面点击编译。然后在部署页面，删除上一个合约，部署新的合约。\n这里多了一个retrieve函数，会返回和favoriteNumber一样的数值。把变量更新为678，然后点击favoriteNumber和retrieve，它们都会返回678。正如你在这里看到的，这两个函数是蓝色的，但是store是橘黄色的。\n\n它们的区别是什么？关键在这里的view关键字。\nSolidity中有两个关键字，标识函数的调用不需要消耗gas。这两个关键字是view和pure。\n如果一个函数是view函数，意味着我们只会读取这个合约的状态。例如，retrieve函数只读取favoriteNumber的值。\nview函数不允许修改任何状态。你在view函数中，不能修改任何状态。\n（为了不影响阅读体验，关于pure的部分移到了此章节后面）\n接下来，我们接着前面的内容，在下面的控制台，如果我调用retrieve函数会有一个call。与我们调用store函数不同，store函数调用后会有绿色的对勾和交易哈希，但是在call中没有哈希和绿色对勾。\n因为点击蓝色按钮不发送交易，我们只是在链下读取，读取数值。\n然而当你查看call的细节信息时，有一个执行消耗。\n这是什么意思？这里写了消耗只有在被合约调用时才会计算在内。也就是如果一个要改变区块链状态的函数调用了类似retrieve这种view或者pure函数才会消耗gas。\n\n例如，store不是view函数，他在某处调用了retrieve，那它就要支付retrieve的gas。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract SimpleStorage &#123;\n\n    // This gets initialized to zero!\n    // &lt;- This means that this section is a comment!\n    uint256 public favoriteNumber;\n\n    function store(uint256 _favoriteNumber) public &#123;\n        favoriteNumber = _favoriteNumber;\n        retrieve();\n    &#125;\n\n    // view, pure\n    function retrieve() public view returns(uint256) &#123;\n        return favoriteNumber;\n    &#125;\n&#125;因为读取区块链的信息消耗了计算量和gas。\n调用view函数是免费的，除非你在消耗gas的函数中调用它。\n我们可以编译，删除，部署，调用store，输入678，执行，查看它的消耗，发现加了retrieve()这行代码后，store函数的gas消耗更多了。\n\n\n补充：\npure函数也不允许修改状态，我们也不能在pure函数中修改状态，但是pure也不允许读取区块链数据，所以我们也不能读取favoriteNumber的值。\n通过pure函数，你想做的事情可能是这样的。在pure函数中返回1+1的结果。返回值是uint256，类似于这样的东西。可能是常用的方法，或者某个不需要读取数据的算法。如果我们调用view或者pure函数，是不需要支付gas的。因为只是读取观念数据，记住，只有更改状态的时候才支付gas，发交易。\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract SimpleStorage &#123;\n    function add() public pure returns (uint256) &#123;\n        return(1 + 1);\n    &#125;\n&#125;结构体和数组现在我们的合约已经很不错了，他允许我们存储单一一个喜欢的数字，但如果我们想要存储一组喜欢的数字呢？或者我们想存储一大批不同的人物，他们都喜欢不同的数字，那我们该怎么做？\n方法之一是，我们可以创建一个关于人物的结构体（struct），或者说我们在Solidity中创建一个新的类型，我们可以创建一个人物对象，其中包含某人的名字以及他们喜欢的数字。\n我们创建一个被称为People的新类型。就像uint25、boolean或者string，就像我们创建的uint256 public favoriteNumber一样，我们可以对People做相同的事情。\n我们可以写下People public，把它命名为person。这样我们就创建了一个新的people，并将其分配给person这个变量，后面再加上()，表示我们在创建一个新的People类型的变量，里面添加{}，表示参数是一个结构体，然后赋值。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract SimpleStorage &#123;\n\n    // This gets initialized to zero!\n    // &lt;- This means that this section is a comment!\n    uint256 favoriteNumber;\n    People public person = People(&#123;favoriteNumber: 2, name: &quot;scc749&quot;&#125;);\n\n    struct People &#123;\n        uint256 favoriteNumber;\n        string name;\n    &#125;\n\n    function store(uint256 _favoriteNumber) public &#123;\n        favoriteNumber = _favoriteNumber;\n        retrieve();\n    &#125;\n\n    // view, pure\n    function retrieve() public view returns(uint256) &#123;\n        return favoriteNumber;\n    &#125;\n&#125;然后我们来编译部署一下，因为它是一个public变量，所以它会有一个被称为person的getter函数，我们点击一下person，会看到我们的person对象。\n\n它的favoriteNumber是2，name是scc749，你可以看到成员变量的左边有个0和1，它们表示不同变量的索引（index），在计算机科学中，计数通常从数字0开始，每当您往solidity的列表变量中放入对象，它们都会自动编上索引，所以在这里，favoriteNumber的索引是0，name的索引是1（感兴趣的小伙伴还可以自己搜寻一下solidity中存储槽（storage slot）的定义与作用）。\n现在，我们有了一个People结构体，将favoriteNuber和name关联起来，但如果我们想要一堆人呢？\n一种方法是重复声明变量，person1，person2等等，但是这样很麻烦。\n更好的创建列表的方法是使用一种被称为数组（array）的数据结构，数组是存储列表，或者说存储一系列对象的一种方式，创建一个数组和初始化其他类型没什么区别，先声明对象类型，然后是对象的可见性，最后是变量名，我们需要对数组做同样的事情。\n方括号表示我们想要一个包含People类型的数组，我们让它的可见性是public，然后命名为people。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract SimpleStorage &#123;\n\n    // This gets initialized to zero!\n    // &lt;- This means that this section is a comment!\n    uint256 favoriteNumber;\n\n    struct People &#123;\n        uint256 favoriteNumber;\n        string name;\n    &#125;\n\n    // uint256[] public favoriteNumbersList;\n    People[] public people;\n\n    function store(uint256 _favoriteNumber) public &#123;\n        favoriteNumber = _favoriteNumber;\n        retrieve();\n    &#125;\n\n    // view, pure\n    function retrieve() public view returns(uint256) &#123;\n        return favoriteNumber;\n    &#125;\n&#125;编译一下，删掉旧合约，然后重新部署。\n\n现在我们就有了一个蓝色的people按钮在这里，需要提醒的是，因为它是public变量，所以它会自动获得一个view函数，也就是这个蓝色的按钮，并且，现在它不仅仅是一个单独用于显示数值的按钮，它还有一个可以填写的表格，它需要一个uint256作为它的输入参数。\n现在，不管您往这个格子里面输入什么，它都不会有任何反应，这是因为我们的people数组或者说我们的people列表，现在还是个空列表，这里的输入值其实就是输入你想要获得的那个对象的索引。\n接下来我们来实现给这个数组添加内容，这种类型的数组就是所谓的动态数组（Dynamic Array），因为我们在初始化这个数组的时候并没有规定它的大小。\n如果我们给这个People数组的方括号里添加一个数字5（People[5] public people;），就意味着这个People列表，或者说数组，最多只能放进去五个People对象，如果我们不给它设定大小，那就表明它可以是任意大小，并且数组的大小会随着我们添加和减少People而增大和减小。\n所以这里我们使用动态数组，因为我们希望可以添加任意数量的People到这个数组中去，所以让我们来创建一个函数，用于往people数组中添加People。\n我们来写下这个函数，addPerson，我们让它接收string memory _name，作为输入参数（下一章节会解释这是什么意思），然后是uint _favoriteNumber，我们让它成为一个public函数，我们要做的就是对这个people对象调用push函数，所以我们写下people.push，然后我们要创建一个新的person，它将接收_favoriteNumber和_name。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract SimpleStorage &#123;\n\n    // This gets initialized to zero!\n    // &lt;- This means that this section is a comment!\n    uint256 favoriteNumber;\n\n    struct People &#123;\n        uint256 favoriteNumber;\n        string name;\n    &#125;\n\n    // uint256[] public favoriteNumbersList;\n    People[] public people;\n\n    function store(uint256 _favoriteNumber) public &#123;\n        favoriteNumber = _favoriteNumber;\n        retrieve();\n    &#125;\n\n    // view, pure\n    function retrieve() public view returns(uint256) &#123;\n        return favoriteNumber;\n    &#125;\n\n    function addPerson(string memory _name, uint _favoriteNumber) public &#123;\n        people.push(People(_favoriteNumber, _name));\n    &#125;\n&#125;这个大写的People表示的是这个名为People的结构体（struct），然后一个新的People获得了_favoriteNumber和_name，这里小写的people指的是这个的数组，所以我们写下的就是，我们的数组+点+push，push基本上就是添加的意思。\n还有一种方式，就是我们先创建一个People类型的变量，所以我们就可以这样写，People newPerson等于People，然后和之前的做法一样，把内容放进括号里。\nsolidity    function addPerson(string memory _name, uint _favoriteNumber) public &#123;\n        People memory newPerson = People(&#123;favoriteNumber: _favoriteNumber, name: _name&#125;);\n        people.push(newPerson);\n    &#125;如果您的代码中没有memory，试图保存，就会收到错误提示，变量的数据位置必须是”storage”，“memory”，或者”calldata”，需要在此处添加关键字memory，下一章我们将会解释这是什么意思。\nMemory，Storage &amp; Calldata在 Solidity 中，有以下几种数据存储位置：\n\n栈（Stack）：栈是一种临时存储区域，用于存储局部变量和函数参数。在函数执行期间，栈上的数据会被分配和释放，当函数执行完成时，栈上的数据也会被销毁。\n内存（Memory）：内存是一种临时存储区域，用于存储动态分配的数据，比如动态数组和字符串。与栈不同，内存中的数据不会随着函数执行的结束而销毁，需要手动清除。在函数调用期间，内存中的数据可以被读取和修改。\n存储（Storage）：存储是永久存储在区块链上的位置，用于存储合约的状态变量。存储中的数据会一直保存在区块链上，直到合约被销毁。存储是最昂贵的一种存储位置，因为它需要永久存储在区块链上，并且对存储操作收费。\n调用数据（Calldata）：调用数据是用于存储外部函数调用的参数和返回值的位置。在函数调用期间，输入参数会被复制到调用数据中，函数执行完成后，返回值也会被写入调用数据中。\n代码（Code）：代码用于存储合约本身的字节码，即合约的函数实现、逻辑等内容。\n日志（Logs）：日志用于记录合约的事件和状态变化，可以通过日志来实现合约的事件通知和审计功能。\n\n本章节三个最重要的，就是Calldata，Memory和Storage，这是一个稍微进阶的知识点，所以，如果你第一次没有完全掌握它，那也完全没关系。\n可以类似记忆：\ncalldata类比于其他语言中的只读参数，例如在C#中的const参数或在函数调用中传递的不可变数据；\nmemory可以类比于其他语言中的堆内存，例如在C&#x2F;C++中使用malloc()或new关键字分配的内存，或者在Python中的临时变量；\nstorage可以类比于其他语言中的持久化存储，例如在Java中的实例变量或数据库中的表字段。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract SimpleStorage &#123;\n\n    // This gets initialized to zero!\n    // &lt;- This means that this section is a comment!\n    uint256 favoriteNumber;\n\n    struct People &#123;\n        uint256 favoriteNumber;\n        string name;\n    &#125;\n\n    // uint256[] public favoriteNumbersList;\n    People[] public people;\n\n    function store(uint256 _favoriteNumber) public &#123;\n        favoriteNumber = _favoriteNumber;\n        retrieve();\n    &#125;\n\n    // view, pure\n    function retrieve() public view returns(uint256) &#123;\n        return favoriteNumber;\n    &#125;\n\n    function addPerson(string memory _name, uint _favoriteNumber) public &#123;\n        people.push(People(_favoriteNumber, _name));\n    &#125;\n&#125;storage变量存在于正在执行的函数之外，尽管我们在上面没有指定favoriteNumber，但我们的favoriteNumber被自动分配为一个存储（storage）变量，因为它没有在这些函数中明确定义。\ncalldata和memory意味着这个变量只是暂时存在，所以addPerson()函数参数中的_name变量仅在调用此函数的transaction交易期间暂时存在。\n因为在这个函数运行后，我们就不再需要这个_name变量了，我们可以把它作为memory，也可以把它作为calldata，如果你最终不修改这个_name，你可以把这个参数作为calldata。\nsolidity    // calldata, memory, storage\n    function addPerson(string memory _name, uint _favoriteNumber) public &#123;\n        _name = &quot;cat&quot;;\n        people.push(People(_favoriteNumber, _name));\n    &#125;上面的代码运行是没有问题的，如果把memory改成calldata，编译就会发生错误，因为calldata变量不可被修改的临时变量，memory是可以被修改的临时变量，而storage是可以被修改的永久变量。\n尽管上面说实际上有6个地方可以让我们访问和存储信息，但我们不能说一个变量是Stack、Code或Logs，我们只能说是Memory、Storage或Calldata。\n接下来，我们来分析另一个问题，为什么我们在_name前面加上memory，而_favoriteNumber前面不加上memory呢，如果你试了，你会发现，它报了一个错误。\n\n数据位置只适用于数组、结构体或映射类型，不能是uint256。\n这是因为数组、结构体和映射类型在solidity中被认为是特殊的类型，而solidity可以自动知道uint256的位置。Solidity知道，对于这个函数，uint256将仅仅存在于内存中。然而，它不能确定string会是什么？\nstring实际上是有点复杂的，从背后原理上来说，一个string实际上是一个bytes数组，由于string是一个数组，我们需要把这个memory字节加进去。因为我们需要让solidity知道数组、结构体或映射类型的数据位置。所以，这就是为什么我们需要告诉它，它是在memory内存中的。\n同时，我们也不能在此添加storage关键字，这个名字变量实际上并没有被存储到任何地方，你需要让它成为memory或calldata，这是它唯一接受的两种类型。\n所以，总结一下，结构体、映射类型和数组在作为参数被添加到不同的函数时，需要给定一个memory或calldata关键字。\nMappings现在这个列表很棒，但如果我们知道某人的名字，但不知道他最喜欢的数字，怎么办？\n那么我们可以做的是，我们可以在整个数组中寻找那个人。例如在我们的合约中，我们可以从索引0开始寻找，一直往后查询，直到找到我们想要的那个人的信息。\n不用说，这肯定很麻烦，假如人有很多，我们就得一直迭代到那个人所在的索引，这显然是非常低效的。有什么其他的方法来存储这些信息，使其更容易和更快的访问呢？\n我们可以使用另一种数据结构，是一种叫做mapping映射的东西。\n（类似C++的unordered_map或者Java的hashmap等等，但需要注意的是，Solidity的mapping有一些特殊的属性和限制，例如不能迭代访问所有键值对，只能通过键来访问对应的值。此外，Solidity 中的mapping可以存储任意类型的值，而在C++和Java中，需要指定键和值的具体类型）\n你可以把映射想象成一种字典，它是一组键值对，每个key键返回与该键关联的某个value值。\n我们创建一个映射变量的方式与创建所有其他变量的方式完全相同。所以这会是从string到uint256的mapping()类型，可见性关键字将是public，我们把它叫做nameToFavoriteNumber，现在我们有一个字典，每个名字都会映射到一个特定的数字。\n所以让我们给addPerson()函数添加一些能力，所以我们要把我们的People添加到我们的数组中，还要把它们添加到我们的映射中，我们要做的是nameToFavoriteNumber[_name] &#x3D; _favoriteNumber;。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract SimpleStorage &#123;\n\n    // This gets initialized to zero!\n    // &lt;- This means that this section is a comment!\n    uint256 public favoriteNumber;\n\n    mapping(string =&gt; uint256) public nameToFavoriteNumber;\n\n    struct People &#123;\n        uint256 favoriteNumber;\n        string name;\n    &#125;\n\n    // uint256[] public favoriteNumbersList;\n    People[] public people;\n\n    function store(uint256 _favoriteNumber) public &#123;\n        favoriteNumber = _favoriteNumber;\n        retrieve();\n    &#125;\n\n    // view, pure\n    function retrieve() public view returns(uint256) &#123;\n        return favoriteNumber;\n    &#125;\n\n    // calldata, memory, storage\n    function addPerson(string memory _name, uint _favoriteNumber) public &#123;\n        people.push(People(_favoriteNumber, _name));\n        nameToFavoriteNumber[_name] = _favoriteNumber;\n    &#125;\n&#125;因此，让我们继续编译它，来到我们的部署页面将它部署，点击，我们有一个名为nameToFavoriteNumber的新按钮。\n\n当你创建一个映射时，会把所有东西都初始化为空值，现在这里每一个可能的字符串，都有一个对应的初始值favoriteNumber为0。\n所以我们要手动添加值，就利用我们的addPerson()函数，添加一个人到我们的映射中，等待这个交易确实完成了，然后，让我们多添加几个人。\n\n现在，如果我们查找某个人最喜欢数字，将会立即得到结果，当然我们也可以在people数组中找到它们。\n在测试网上部署第一个合约在以太坊测试网上部署合约-CSDN博客\n引入其他合约回到Remix，我们已经有了SimpleStorage.sol，现在我们有了SimpleStorage合约，它允许我们存储一个最喜爱的数字，也允许通过mapping映射和数组来存储最喜欢的数字。\n假如我们想在这个方面做得更高级一些，可以使用一个合约来为我们部署其他合约，并进一步使用它的部署的这些合约进行交互。合约间的相互交互，是使用solidity和智能合约工作中必不可少的一部分。\n合约可以无缝地互相交互的能力，这是我们说的可组合性。智能合约是可组合的，因为他们可以轻易地互相交互。当我们构建Defi应用时，这些特性显得尤其重要。复杂的金融产品间就可以非常轻易的互相交互，因为所有的合约代码在链上都可用。\n我们会学习如何做到这些，保持SimpleStorage合约（上面代码块中的代码）原样不变，我们创建一个名为StorageFactory的新合约。\n点击新建按钮，输入StorageFactory.sol。\n\n接下来，我们开始编写StorageFactory合约的代码。\n第一件要做的事，是SPDX-License-Identifier: MIT许可，接下来时Solidity版本，输入pragma solidity ^0.6.22，输入合约名称StorageFactory，保存，在编译选项卡中点击编译，合约就配置好了。\n现在创建一个能够部署SimpleStorage合约的函数，创建名为createSimpleStorageContract的函数，设置为public，所以任何人都可以调用它，这个函数将部署一个SimpleStorage合约，并将其保存在一个全局变量中，但是做这些之前，StorageFactory合约如何知道SimpleStorage合约是什么样的呢？\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract StorageFactory &#123;\n\n    function createSimpleStorageContract() public &#123;\n        // How does storage factory know what simple storage looks like?\n    &#125;\n    \n&#125;如果我们的StorageFactory合约要部署SimpleStorage合约，就需要知道SimpleStorage合约的代码。\n一种方法是，复制SimpleStorage.sol中pragma solidity下方的所有代码，粘贴到StorageFactory.sol中，放置到pragma solidity下方。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract SimpleStorage &#123;\n\n    // This gets initialized to zero!\n    // &lt;- This means that this section is a comment!\n    uint256 public favoriteNumber;\n\n    mapping(string =&gt; uint256) public nameToFavoriteNumber;\n\n    struct People &#123;\n        uint256 favoriteNumber;\n        string name;\n    &#125;\n\n    // uint256[] public favoriteNumbersList;\n    People[] public people;\n\n    function store(uint256 _favoriteNumber) public &#123;\n        favoriteNumber = _favoriteNumber;\n        retrieve();\n    &#125;\n\n    // view, pure\n    function retrieve() public view returns(uint256) &#123;\n        return favoriteNumber;\n    &#125;\n\n    // calldata, memory, storage\n    function addPerson(string memory _name, uint _favoriteNumber) public &#123;\n        people.push(People(_favoriteNumber, _name));\n        nameToFavoriteNumber[_name] = _favoriteNumber;\n    &#125;\n&#125;\n\ncontract StorageFactory &#123;\n\n    function createSimpleStorageContract() public &#123;\n        // How does storage factory know what simple storage looks like?\n    &#125;\n    \n&#125;编译并保存StorageFactory.sol，一切正常，现在，我们的StorageFactory.sol文件，实际上拥有2个合约，分别是Simple合约，和StorageFactory合约。\n前往部署页面，往下滑，确保右边选中的是StorageFactory.sol，而不是SimpleStorage.sol，你会看到，可以选择要部署哪一个合约。\n\n单个solidity文件，可以拥有多个不同的合约。\n现在我们的StorageFactory中已经有了SimpleStorage，可以继续编写createSimpleStorageContract函数了。\n我们创建一个全局变量，就像创建其他全局变量一样。类型是SimpleStorage，可见性为public，变量名为simpleStorage，在函数中，为变量simpleStorage赋值，new关键字会让Solidity知道，我们需要部署一个新的SimpleStorage合约。\nsoliditycontract StorageFactory &#123;\n    SimpleStorage public simpleStorage;\n\n    function createSimpleStorageContract() public &#123;\n        simpleStorage = new SimpleStorage();\n    &#125;\n&#125;前往编译页面进行编译。前往部署页面，确保环境选择的是Remix VM，下拉到合约选项，选择StorageFactory，记住，你需要让StorageFactory.sol处于选中状态，这里会显示StorageFactory。\n点击部署，现在我们看到，StorageFactory合约有两个按钮。一个是调用createSimpleStorageContract函数。另一个是获取变量simpleStorage的值，我们现在点击它，返回的合约地址是0，是它默认的初始值，这表示当前还没有部署SimpleStorage合约。\n\n上拉控制台，点击createSimpleStorage，可以看到我们发起了一次函数调用，StorageFactory.createSimpleStorageContract()，通过这样做，我们创建并部署了一个新的SimpleStorage合约。\n\n来看看SimpleStorage合约的地址是什么，点击按钮，我们看到一个地址与其进行了关联，现在我们知道了，一个合约实际上可以部署另一个合约。\n\n现在的问题是，StorageFactory上方的这一大块代码，显得有点冗余，尤其是我何已经有了一个叫SimpleStorage.sol的文件，假如我们有一个合约，它包含了很多其他的合约，复制这些合约的代码会是一项巨大的工作，所以，我们可以使用import来进行优化。\n删除SimpleStorage的代码，输入import “.&#x2F;SimpleStorage.sol”，这样使用import，跟我们之前将代码复制过来会是同样的作用，这里可以指定其他文件的路径，也可以指定其他包或者GitHub的路径。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\nimport &quot;./SimpleStorage.sol&quot;;\n\ncontract StorageFactory &#123;\n    SimpleStorage public simpleStorage;\n\n    function createSimpleStorageContract() public &#123;\n        simpleStorage = new SimpleStorage();\n    &#125;\n\n&#125;像这样引入合约，比起复制代码要好上很多，如果我们想更新SimpleStorage的代码，我们只需要在一个文件中进行改动，而不是在多处改动，另外，你也注意到了这个pragma solidity，如果我们的合约在两个独立的文件中，我们可以有不同的solidity版本号（前提是需要兼容的版本，不然编译器无法一起编译）。\n我们的createSimpleStorageContract函数，每调用一次都会替换掉变量simpleStorage的值，我们来改进一下，将simpleStorage变量类型改为数组，变量名改为simpleStorageArray。\n现在我们每创建一个新的SimpleStorage合约，不能像原来这样赋值了，而是要将其存储为SimpleStorage类型的内存变量，并将这个simpleStorage变量，添加到simpleStorageArray数组当中，就像之前一样，使用simpleStorageArray.push(simpleStorage)。\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\nimport &quot;./SimpleStorage.sol&quot;;\n\ncontract StorageFactory &#123;\n    SimpleStorage[] public simpleStorageArray;\n\n    function createSimpleStorageContract() public &#123;\n        SimpleStorage simpleStorage = new SimpleStorage();\n        simpleStorageArray.push(simpleStorage);\n    &#125;\n&#125;编译，部署StorageFactory，在这里，我们看到查看simpleStorageArray的按钮，我们创建一个SimpleStorage，然后在索引0的位置可以看到合约地址，现在索引1的位置，还没有值，如果我们再创建一个SimpleStorage，索引1的位置现在能看到合约地址了。\n\n与其他合约交互现在我们可以追踪所有的SimpleStorage的部署了，但我们要如何跟它们交互呢，比如我们想要在StorageFactory中调用SimpleStorage的store函数。\n你可以把StorageFactory当成是所有SimpleStorage的管理者，让我们来创建一个新的函数来完成这件事情，创建名为sfStore的函数，意思是StorageFactoryStore，参数是uint256类型的_simpleStoragelndex和uint256类型的simpleStorageNumber，它也是public的函数。\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\nimport &quot;./SimpleStorage.sol&quot;;\n\ncontract StorageFactory &#123;\n    SimpleStorage[] public simpleStorageArray;\n\n    function createSimpleStorageContract() public &#123;\n        SimpleStorage simpleStorage = new SimpleStorage();\n        simpleStorageArray.push(simpleStorage);\n    &#125;\n\n    function sfStore(uint256 _simpleStorageIndex, uint256 _simpleStorageNumber) public &#123;\n        // Address\n        // ABI - Application Binary Interface\n    &#125;\n&#125;要和其他的合约进行交互，你需要2样东西，一是需要合约地址，二是需要合约ABI。ABI指的是应用程序二进制接口，ABI会告诉我们的代码如何来跟合约进行交互，我们来深入了解一下ABl。\n前往编译洗项卡，点击编译，合约正在编译中，下拉页面，你会看到编译详情。这里会看到不同合约的大量信息，你可以看到合约名称是SimpleStorage，METADATA里包含编译器，语言，输出，配置等信息，然后是字节码，里面包含opcodes。\n\n你还可以看到ABI，它显示了所有不同的输入和输出，即这个合约能做的所有事情，比如，在我们的SimpleStorage，ABI中索引为0的位置，是函数addPerson。索引1，我们可以看到是favoriteNumber。索引2，nameToFavoriteNumber。索引3，people。索引4，retrieve。索引5，store。\n\n它告诉了我们可以和合约进行交互的所有的这些不同的方式。告诉我们可以调用的不同的函数，我们知道地址在哪，我们把它存储在了simpleStorageArray中，我们也获得了ABI。\n因为我们导入了SimpleStorage.sol，当你编译SimpleStorage.sol时，不论你何时编译它，正如在编译详情所见，它都与ABI预先打包好了，我们可以自动获得ABI，只需像这样导入它。将来我们会看到可以通过其他方式来轻松获得ABI。\n要调用我们合约中的store函数，首先要获得合约对象，我们如何做到这一点？\n定义一个变量，名字是simpleStorage，类型是SimpleStorage。它会是一个SimpleStorage类型的对象，不像上次我们是用new来创建对象，这回我们把地址放在括号里，地址可以从我们的数组获得。\n也就是说，调用这个函数，我们会传递一个数组的索引，这个地址有一个SimpleStorage合约，地址是simpleStorageArray[_simpleStoragelndex]。括号这个语法，可以让你访问数组中的不同元素，如果要获得列表中索引为0的元素，simpleStoragelndex参数应传递0，把它传递进来，它就会给返回 SimpleStorage的合约地址。\n因为这是一个SimpleStorage合约的数组，我们可以用索引来访问SimpleStorage合约，也就是使用simpleStorageArray[_simpleStoragelndex]。现在我们把SimpleStorage合约对象，赋值给了simpleStorage变量，我们可以在数组中找到地址。\n同时这里也自动获得了ABI，如果这是对象地址类型的数组，就需要像这样对地址进行包装，这个问题后面再谈。\nsolidity    function sfStore(uint256 _simpleStorageIndex, uint256 _simpleStorageNumber) public &#123;\n        // Address\n        // ABI - Application Binary Interface\n        SimpleStorage simpleStorage = simpleStorageArray[_simpleStorageIndex];\n        simpleStorage.store(_simpleStorageNumber);\n    &#125;现在我们已经有simpleStorage了现在我们可以通过simpleStorage调用store函数，现在通过simpleStorage.store()存储_simpleStorageNumber，这很完美。如果我们现在部署它，我们还不能读取store函数，让我们来创建另一个函数，可以在StorageEactory里读取SimpleStorage合约中的数据。\n创建名为sfGet的函数，意思是StorageFactoryGet，参数是uint256类型的_simpleStoragelndex，它是public view的函数，因为只从SimpleStorage合约中读取数据。它会返回uint256类型的数据。这里我们将SimpleStorage simpleStorage赋值为，我们使用跟上面同样的语法来获取合约对象，simpleStorageArray[_simpleStoragelndex]，使用simpleStorage.retrieve()，返回我们在这里存入的数字。\nsolidity    function sfGet(uint256 _simpleStorageIndex) public view returns(uint256) &#123;\n        SimpleStorage simpleStorage = simpleStorageArray[_simpleStorageIndex];\n        return simpleStorage.retrieve();\n    &#125;代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\nimport &quot;./SimpleStorage.sol&quot;;\n\ncontract StorageFactory &#123;\n    SimpleStorage[] public simpleStorageArray;\n\n    function createSimpleStorageContract() public &#123;\n        SimpleStorage simpleStorage = new SimpleStorage();\n        simpleStorageArray.push(simpleStorage);\n    &#125;\n\n    function sfStore(uint256 _simpleStorageIndex, uint256 _simpleStorageNumber) public &#123;\n        // Address\n        // ABI - Application Binary Interface\n        SimpleStorage simpleStorage = simpleStorageArray[_simpleStorageIndex];\n        simpleStorage.store(_simpleStorageNumber);\n    &#125;\n\n    function sfGet(uint256 _simpleStorageIndex) public view returns(uint256) &#123;\n        SimpleStorage simpleStorage = simpleStorageArray[_simpleStorageIndex];\n        return simpleStorage.retrieve();\n    &#125;\n&#125;然后我们编译部署一下。\n下拉页面，现在如果我们在sfGet函数传入0，我们什么也得不到。simpleStorage传入参数0，同样什么都没有。来创建一个SimpleStorage合约，现在我们能从索引0上获取合约地址了。\n\n为这个合约存入一个数字，这个合约的索引是0，索引参数传入0，存入数字7。调用sfStore函数。如果一切正常，数字7将会存入这个合约。调用sfGet函数传入0，确实返回了7。如果传入1，什么也没发生。事实上这里有一个revert的错误。\n\n我们再创建一个SimpleStorage合约，调用sfGet函数传入1，返回了默认值0。调用sfStore函数，索引传入1，存入数字16，调用sfGet函数传入1，返回了16。\n\nStorageFactory合约允许我们创建SimpleStorage合约，并把它们存入simpleStorageArray数组，然后我们在下面的函数可以使用这个数组，在StorageFactory合约里我们可以存入数据，也可以读取我们所创建的所有的SimpleStorage合约中的数据，这非常强大。\n我们可以进一步的优化这两个函数，我们可以在这里直接调用retrieve，我们在通过这个数组返回了一个SimpleStorage对象，我们可以删除这部分代码，这里直接调用retrieve函数，删掉下面这行，这里加上return，保存编译，显示了绿色对勾，只要这里是 SimpleStorage变量，就可以调用retrieve。\nsolidity    function sfGet(uint256 _simpleStorageIndex) public view returns(uint256) &#123;\n        return simpleStorageArray[_simpleStorageIndex].retrieve();\n    &#125;上面这里我们也同样优化一下，删除这部分，直接调用store函数，传入_simpleStoragelndex，保存编译，它和之前一样可以正常工作。\nsolidity    function sfStore(uint256 _simpleStorageIndex, uint256 _simpleStorageNumber) public &#123;\n        // Address\n        // ABI - Application Binary Interface\n        simpleStorageArray[_simpleStorageIndex].store(_simpleStorageNumber);\n    &#125;代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\nimport &quot;./SimpleStorage.sol&quot;;\n\ncontract StorageFactory &#123;\n    SimpleStorage[] public simpleStorageArray;\n\n    function createSimpleStorageContract() public &#123;\n        SimpleStorage simpleStorage = new SimpleStorage();\n        simpleStorageArray.push(simpleStorage);\n    &#125;\n\n    function sfStore(uint256 _simpleStorageIndex, uint256 _simpleStorageNumber) public &#123;\n        // Address\n        // ABI - Application Binary Interface\n        simpleStorageArray[_simpleStorageIndex].store(_simpleStorageNumber);\n    &#125;\n\n    function sfGet(uint256 _simpleStorageIndex) public view returns(uint256) &#123;\n        return simpleStorageArray[_simpleStorageIndex].retrieve();\n    &#125;\n&#125;现在我们有了可以存储变量的SimpleStorage合约，有了StorageFactory合约，可以管理这些SimpleStorage合约，可以部署它们并和它们进行交互。\n继承和重载我们真的很喜欢SimpleStorage合约，但是它不能完成我们希望做到的所有事情。比方说，我们存储的时候，不是只存储最喜爱的数字，而是存储最喜欢的数字再加上5。有时候你会需要一个这样的合约，所有人的最喜爱的数字都比他们认为的数要大5，当你真的很喜欢这个合约里的其他函数时，来创建一个新的合约，命名为ExtraStorage，输入ExtraStorage.sol。\n跟平常一样先配置这个合约，设置SPDX-License-ldentifier: MIT，pragma solidity设置为^0.6.22，合约名为ExtraStorage，保存编译，看到了这个绿色对勾。\n\n我们可以做什么？第一件事是可以把这里的代码全部复制过来，然后视情况修改它，这元得非常冗余，工作量也很大。有什么其他方法可以让ExtraStorage合约像SimpleStorage合约一样吗，这就是我们要说的继承。\n你可以理解为ExtraStorage合约是SimpleStorage合约的子合约，使ExtraStorage继承SimpleStorage的全部函数，只需通过2行代码。\n首先，为了让ExtraStorage了解SimpleStorage，我们需要导SimpleStorage，输入import “.&#x2F;SimpleStorage.sol”，然后，使用is关键字指定继承关系。\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\nimport &quot;./SimpleStorage.sol&quot;;\n\ncontract ExtraStorage is SimpleStorage &#123;\n    \n&#125;保存编译，现在ExtraStorage和SimpleStorage一样了，它继承了SimpleStorage合约的全部功能。\n来验证一下，我们编译部署它。\n\n在这里可以看到，ExtraStorage拥有SimpleStorage的所有函数。如果你想让一个合约能继承另一个合约的所有功能，只需把它导入，并使用is指定继承关系。\n现在我们给ExtraStorage合约加一些额外的函数，同时它也包含SimpleStorage合约的所有功能。\n现在，ExtraStorage已经继承了SimpleStorage，然后，我们不太喜欢SimpleStorage合约中的一个函数，回到SimpleStorage合约，store函数把参数中传递进来的_favoriteNumber赋值给了全局变量。\nsolidity    function store(uint256 _favoriteNumber) public &#123;\n        favoriteNumber = _favoriteNumber;\n        retrieve();\n    &#125;在ExtraStorage合约，我们希望store函数做一些不同的事情，我们希望在传递过来的数字上加上5，要如何做到呢？我们会使用到重载，要用到2个关键字，virtual和override，现在，我们来实现store，看看会发生什么。\nfunction store(uint256_favoriteNumber)，它是一个public函数，不仅是存储_favoriteNumber，而是在这个变量上加上5。\nsoliditycontract ExtraStorage is SimpleStorage &#123;\n    // + 5\n    // override\n    // virtual override\n    function store(uint256 _favoriteNumber) public &#123;\n        favoriteNumber = _favoriteNumber + 5;\n    &#125;\n&#125;如果我们现在编译，会看到两个不同的错误，第一个是提示缺少override修饰符，如果父合约，在这里就是SimpleStorage合约，拥有同样的函数，我们要告诉Solidity需要重写store函数，而不是使用这个函数。另一个错误是提示我们正常尝试重写非virtual的函数，是忘了添加virtual修饰符了吗？\n\n为了使得这个函数可以被重写，需要加上virtual关键字，现在它可以被重写了，然后给store函数添加override关键字，现在可以正常编译了。\n\n\n代码：\nsolidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract SimpleStorage &#123;\n\n    // This gets initialized to zero!\n    // &lt;- This means that this section is a comment!\n    uint256 public favoriteNumber;\n\n    mapping(string =&gt; uint256) public nameToFavoriteNumber;\n\n    struct People &#123;\n        uint256 favoriteNumber;\n        string name;\n    &#125;\n\n    // uint256[] public favoriteNumbersList;\n    People[] public people;\n\n    function store(uint256 _favoriteNumber) public virtual  &#123;\n        favoriteNumber = _favoriteNumber;\n        retrieve();\n    &#125;\n\n    // view, pure\n    function retrieve() public view returns(uint256) &#123;\n        return favoriteNumber;\n    &#125;\n\n    // calldata, memory, storage\n    function addPerson(string memory _name, uint _favoriteNumber) public &#123;\n        people.push(People(_favoriteNumber, _name));\n        nameToFavoriteNumber[_name] = _favoriteNumber;\n    &#125;\n&#125;solidity// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\nimport &quot;./SimpleStorage.sol&quot;;\n\ncontract ExtraStorage is SimpleStorage &#123;\n    // + 5\n    // override\n    // virtual override\n    function store(uint256 _favoriteNumber) public override &#123;\n        favoriteNumber = _favoriteNumber + 5;\n    &#125;\n&#125;让我们部署它，先删除旧的合约，Remix VM环境，很好，选择ExtraStorage，点击部署，这里会看到部署好的ExtraStorage合约。\n现在调用retrieve函数，返回了0，在这之前，使用store函数存储数字，然后现在，我们存入5，再加上5，实际存储的应该是10，调用store函数，看起来这个交易通过了，现在调用retrieve函数，确实返回了10，这就是继承和重写函数。\n\n通过函数发送 ETH &amp; 返回接下来，我们将创建一个合约，用来学习后面的Solidity基础内容。\nFundMe.sol是一个智能合约，它可以人们发起一个众筹。\n所以人们可以向该智能合约发送ETH、Polygon、 Ava、Fantom或者其他区块链原生通证，然后这个智能合约拥有者可以提取这些通证来做他们想做的事。\n此时，在remix中，这里有一些合约，SimpleStorage.sol、StorageFactory.sol和 ExtraStorage.sol，我们将创建一个新的FundMe.sol合约。\n让我们开始创建FundMe合约吧，像之前说到的，我们想要这个合约能从用户处获得资金，然后能够提取资金，同时设置一个以eth（以太币）计价的最小资助额，这就是我们要让合约做到的事情。\n首先，让我们设置 SPDX-License-ldentifier: MIT，然后设置solidity版本为^0.6.22，然后我们合约起名FundMe，然后，点击Compile FundMe.sol，看看是否编译正常。\n代码：\nsolidity// Get funds from users\n// Withdraw funds\n// Set a minimum funding value in ETH\n\n// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract FundMe &#123;\n    \n&#125;编译正常，好的，那我们继续。\n在实现所有函数逻辑之前，让我们先把要实现的函数写下来，我们需要Fund函数，人们可以通过它来发送资金，我们还需要一个Withdraw函数，这样合约的owner(拥有者)可以提取不同的funder 发送的资金。\nsoliditycontract FundMe &#123;\n    \n    // function fund()&#123;&#125;\n    \n    // function withdraw()&#123;&#125;\n&#125;这些差不多就是我们要实现的函数，这就是我们希望这个合约能实现的两个主要函数，可能还会实现更多函数来帮助这两个函数更加完善。\n但是让我们先从Fund开始，先把 Withdraw注释掉，我们希望任何人都可以调用Fund函数，所以我们把它设为public，正如之前提到的，我们希望能够设置一个以ETH计算的最小金额，这里有很多事情要考虑，第一件就是如何向这个合约转ETH。\nsoliditycontract FundMe &#123;\n    \n    function fund() public &#123;\n        // Want to be able to set a minimum fund amount in ETH\n        // 1. How do we send ETH to this contract?\n    &#125;\n\n    // function withdraw()&#123;&#125;\n&#125;每当我们在任何一个兼容EVM的区块链上创建一笔交易时，这个value的值，代表我们将通过这笔交易发送多少ETH。\n\n例如，当我们在不同账户间转移ETH时，实际上就是在value中填充不同的ETH数量。\n事实上，以太坊上发送的每笔交易都包含一些必要字段，这些字段包括：\n\nNonce：这是发送地址的交易计数器。它确保交易按顺序执行，并防止重播攻击。\nGas Price：这是发送者愿意支付的 gas 单价，以太坊网络会根据 gas price 和 gas limit 来计算手续费。\nGas Limit：这是交易执行所需的最大 gas 数量。它决定了交易的复杂程度和成本。\nTo：这是接收以太币或者调用智能合约的目标地址。\nValue：这是发送的以太币数量。\nData：这是可选字段，用于向智能合约发送数据。\nv、r、s：这些字段用于交易的签名验证，确保交易的安全性和完整性。\n\n在转账的时候，我们可以填充其中一些字段，例如，gas limit中填充的21,000，data是空的，然后to是我们想要将交易发送到的地址，在函数调用的交易中，仍然可以以这种方式填写to，我们可以调用一个函数，并同时进行转账。\n在remix中，有一个下拉菜单中，有Wei，Gwei，Finney，Ether，我们这里先忽略Finney。\n每当我们在任何一个兼容EVM的区块链上创建一笔交易时，这个value的值，代表我们将通过这笔交易发送多少ETH。\n\n我们可以来到Wei， Gwei和Ether的计算器Ethereum Unit Converter | Ether to Gwei, Wei, Finney, Szabo, Shannon etc. (eth-converter.com)\n\n—个Eth值这么多Gwei，值这么多Wei。\n为了使函数可以被ETH或任何其它通证支付，我们首先将函数设为payable，payable这个关键字让Fund函数变红，而不是普通的橙色。\nsoliditycontract FundMe &#123;\n    \n    function fund() public payable &#123;\n        // Want to be able to set a minimum fund amount in ETH\n        // 1. How do we send ETH to this contract?\n    &#125;\n\n    // function withdraw()&#123;&#125;\n&#125;就像我们的钱包可以持有资金，合约地址也可以持有资金，每次部署合约时，可以获得一个合约地址，它与钱包地址几乎一致，所以钱包和合约都可以持有像ETH这样的原生区块链通证，稍后部署我们的合约后，你会看到这个地址的ETH余额。\n现在我们已经让这个函数成为payable了，然后可以在函数中用全局关键字msg.value，来知道某人转账的金额。\n现在假设我们想将msg.value赋予一个某个数值的以太币，假设我们希望人们在所有交易中至少转一个以太币，或者换个说法，他们至少要转一个以太币我们要如何实现呢，好吧，我们通过require实现，可以require msg.value &gt; 1e18。\n代码：\nsolidity// Get funds from users\n// Withdraw funds\n// Set a minimum funding value in ETH\n\n// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract FundMe &#123;\n    \n    function fund() public payable &#123;\n        // Want to be able to set a minimum fund amount in ETH\n        // 1. How do we send ETH to this contract?\n        require(msg.value &gt; 1e18, &quot;Didn&#39;t send enough!&quot;); // 1e18 == 1 * 10 ** 18 == 1000000000000000000\n    &#125;\n\n    // function withdraw()&#123;&#125;\n&#125;这里需要解释一下，1e18等于1*10**18，也就是1,000,000,000,000,000,000，这么多Wei等于1个ETH。\n如果要求msg.value 至少发送1个ETH（或者Polygon，ava等等），可以设置require (msg.value) &gt; 1，这个require关键字会检查msg.value是否大于1，如果不是，它将revert这个交易，revert会同时显示错误信息“didn’ t send enough”。\n现在在Remix VM部署这个合约，点击Deploy，部署FundMe合约，可以看到fund函数的按钮变红了，如果我们现在调用fund函数,可以看到控制台中，有一个错误。\n\n错误原因就是之前设置的“didn’t send enough”，所以需要做的是在Fund交易中转至少一个ETH，满足require中的设置条件。\n回到上面的value字段，修改数值为2，所以这里应该是大于1个ETH或者对应多的Wei或者Gwei，发送2个ETH，向下滚动，现在我们可以点击 fund。\n\n可以看到通过了require的检查，如果require第一部分是false，那么将revert并且显示这个错误。\nrevert是什么？revert可能有一点不好理解，revert是将之前的操作回滚，并将剩余的gas费返回，那么实际是什么样的呢？\n比如说，创建uint256数据类型的public变量number，在fund函数中，给number赋值5。\nsoliditycontract FundMe &#123;\n    uint256 public number;\n    \n    function fund() public payable &#123;\n        // Want to be able to set a minimum fund amount in ETH\n        // 1. How do we send ETH to this contract?\n        number = 5;\n        require(msg.value &gt; 1e18, &quot;Didn&#39;t send enough!&quot;); // 1e18 == 1 * 10 ** 18 == 1000000000000000000\n        // a ton of computation here\n\n        // What is reverting?\n        // Undo any action before, and send remaining gas back\n    &#125;\n\n    // function withdraw()&#123;&#125;\n&#125;先删掉旧的合约，再部署这个新合约，number现在为0。\n\n但如果我们调用fund函数，number就会被赋值为5，然而，如果我们调用fund函数，require条件没有满足，这个交易将会被revert，将numbere设置为5的操作会被撤销。\n我们查看一下日志，我们保持value为0，所以fund函数还是会revert，调用fund函数，可以看到交易失败，因为require条件没有满足，所以number仍为0。\n\n还有个问题，我们是否真的花费了gas，我们花费gas来使number变为5，剩余的gas应该被这个require返回，例如，如果我们在require后面有需要大量计算资源的操作，调用fund函数时会花费大量的gas，当这个交易在这里被revert后，所有后续的gas将被返回给原用户。\n如果这里有一些不好理解，别担心，我们在后续还会学习它。现在你所需要知道的是，当有一个require时，如果条件没有满足，交易将被取消，任何之前的操作都将被撤销，并且会发送一条错误信息。\n好的，我们先在全局作用域中删除number，这里其实还有一种方法可以让交易revert，将在这个合约的后面学习中讨论。\n数组和结构体（续）接着上面的内容，然后我们将代码稍微修改一下（将ETH限制发送的数量作为全局变量）。\n代码：\nsolidity// Get funds from users\n// Withdraw funds\n// Set a minimum funding value in ETH\n\n// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract FundMe &#123;\n\n    uint256 public minimumETH = 0.01 * 1e18; // 1 * 10 ** 18\n    \n    function fund() public payable &#123;\n        // Want to be able to set a minimum fund amount in currency\n        // 1. How do we send ETH to this contract?\n        require(msg.value &gt;= minimumETH, &quot;Didn&#39;t send enough!&quot;);\n    &#125;\n\n    // function withdraw()&#123;&#125;\n&#125;接下来，我们想要在funding合约中做什么呢?\n当人们给这个合约发送资金的时候，我们想要记录下来这些人，所以我们创建一个数据类型来记录他们。创建一个名为funders的地址数组，记录所有发送资金的funder，这是一个地址数组，或者地址列表，我们把它定位为public。\n任何实际时间，有人给我们发送资金，通过require以后，我们就会把地址加入funder列表，写funder.push(msg.sender)，就如同msg.value。msg.sender也是一个全局关键字，msg.value代表有多少ETH或者其他原生通证被发送了，msg.sender是调用这个函数的人的地址，如果现在使用的是Sepolia，msg.sender就是这个调用函数的地址，因为我们的地址发送了ETH，我们将会把自己的地址加入到funder数组中，通过这个方法，我们可以追踪所有给我们合约捐助的人。\n然后我们可能想要创建一个address到uint256的mapping，记录每个地址发送资金的数量，所以我们的mapping是address映射到uint256，public addressToAmountFunded。当有人给合约发送资金的时候，就通过addressToAmountFunded[msg.sender] &#x3D; msg.value存储。\n代码：\nsolidity// Get funds from users\n// Withdraw funds\n// Set a minimum funding value in ETH\n\n// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract FundMe &#123;\n\n    uint256 public minimumETH = 0.01 * 1e18; // 1 * 10 ** 18\n\n    address[] public funders;\n    mapping(address =&gt; uint256) public addressToAmountFunded;\n    \n    function fund() public payable &#123;\n        // Want to be able to set a minimum fund amount in currency\n        // 1. How do we send ETH to this contract?\n        require(msg.value &gt;= minimumETH, &quot;Didn&#39;t send enough!&quot;);\n        funders.push(msg.sender);\n        addressToAmountFunded[msg.sender] = msg.value;\n    &#125;\n\n    // function withdraw()&#123;&#125;\n&#125;现在有了人们可以发送资金的函数，也在合约中记录了发送资金的人，这很棒！\nFor loop很棒，到目前为止，我们的fund方法已经完成了，现在任何人都可以来直接为这个合约提供资金，将以太或任何原生区块链货币转给这个合约。\n现在，接下来在如果所有的funder都已经开始资助我们，然后我们要做什么呢，我们会希望项目方能够从合约中提取资金，这样他们就可以直接使用这些资金为这个项目去购买东西，所以让我们接下来来创建一个资金提取函数。\n因此我们将创建一个withdraw函数，并将其设置为public，因为我们将从该合约中提取所有资金，所以还需要将重置funders 数组以及地址对应的资助金额，因为我们将提取所有资金，因此这些资金金额应该要重置为零，所以让我们接着开始遍历funders数组并更新我们的mapping object，使得这些funder的余额现在都为零，因为我们马上从他们那里取出所有的钱，为了做到这一点，我们将使用一种叫做for循环，所以什么是for循环？\nfor循环是一种将某种类型的索引对象进行循环的方式或者将某些范围内的数字进行循环的方法，或者只是将一项任务重复执行某个次数，例如，我们有一个数组或一个列表，在该列表上我们有一二三四，如果我们想获取此数组或此列表中的所有元素（即一二三四），我们如何很好地获取此列表中的所有元素？我们可以将使用for循环遍历这些对象中的每一个。\n因此，第零个索引将是1，第一个索引将是2，而在第二个索引上的将是3，在最后一个索引上的是4，所以我们会将索引0到3进行循环来获取所有这些元素，或者另一个例子是如果这是abcd，那么a是索引零，b是第一个索引，c是第二个，d是第三个，我们将遍历零到三的索引来得到这些元素。接下来我们将做同样的事情，但是会使用funders数组，我们具体如何做呢？\n我们首先从for这个关键字开始，for这个关键字表示我们即将开始一个循环，而在这些括号内，我们会定义要如何循环遍历它。\n&#x2F;*和*&#x2F;有点像注释的括号，这两者之间的任何内容都是注释。\n所以在for循环中，首先我们要给它一个起始索引，然后我们要给它一个结束索引，然后我们规定每次增加的值。例如，也许我们想从0开始，然后我们想到达10，以及我们想每次增加1，所以我们会像这样012345678910走上去，或者我们从0开始我们想在10结束，我们每次增加2，所以我们会像这样0246810这样加上去，或者我们想从0增加到5，我们想从2增加到5，每次增加1，我们会像这样2345遍历下去等等。\nsolidity    function withdraw() public &#123;\n        /* starting index, ending index, step amount */\n        for (uint256 funderIndex = 0; funderIndex &lt; funders.length; funderIndex++) &#123;\n            // code\n        &#125;\n    &#125;这就是这里for 循环的内容，所以对于我们的起始索引来说，让我把它放在这上面，这样你就可以直接引用它，所以我们的起始索引将是uint 256变量，我们将其称为funder索引，我们将从funder索引等于零开始，所以我们从零开始，我们将在funder数组的长度最大值处结束。\n因为我们想要遍历所有funder，所以我们要限制funder资助者小于funders.length，所以我们的结束索引将是，当资助者funder index不再小于funders.length的时候。\n然后最后我们要funder index &#x3D; funder index +1，这意味着每次循环内的代码完成时，我们都会将funder index加一。\n这就是我们如何从0到1到2到3到4到5等等，另一种方式你可以输入funder index &#x3D; funder index +1，或者你可以只做funder index++，这个++语法意味着funder index等于它本身加一。\n所以让我们开始循环遍历我们的funders数组，以访问第零个元素或第一个元素，我们将说资助者索引里的资助者，所以我们说我们想要访问funders的第零个元素，这将返回一个地址。\n我们将认为funder的地址等于资助者索引中的funder，所以现在我们有了这个funder地址，我们想用它来重置我们的mapping，所以addressToAmountFunded这个mapping在key是funder所对应的 value赋值为0。\nsolidity    function withdraw() public &#123;\n        /* starting index, ending index, step amount */\n        for (uint256 funderIndex = 0; funderIndex &lt; funders.length; funderIndex++) &#123;\n            // code\n            address funder = funders[funderIndex];\n            addressToAmountFunded[funder] = 0;\n        &#125;\n    &#125;因为在fund函数中，只有在合同被提供资金时，我们才会更新金额，当我们从合约中提取资金时，我们将把它重置为零，我们将得到第零个funder，我们将从索引为零处获取该funder的信息，并将该funder在addressToAmountFunded对应的value重置为零。\n然后用这个for循环会增加1它将从零移动到一，然后它会检查funder index是否小于数组长度。假设资助者有10人，如果资助者有10人，这个funder index就仍然是小于其数组长度的。\n所以现在funder index现在是1，其地址现在就是funder index为1的信息，而不是index为0的信息，我们将获取该地址，并将该地址的资助金额重置为零，然后我们将继续从2到3到4，一直到这个数字等于我们的funder数组的长度，这就是我们如何用循环的方式去遍历对象。\n所以要是你说这个中间这部分就是结束的index，这也并不完全正确，因为我们会通过检查布尔值来看其是否仍然为真。\n代码：\nsolidity// Get funds from users\n// Withdraw funds\n// Set a minimum funding value in ETH\n\n// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract FundMe &#123;\n\n    uint256 public minimumETH = 0.01 * 1e18; // 1 * 10 ** 18\n\n    address[] public funders;\n    mapping(address =&gt; uint256) public addressToAmountFunded;\n    \n    function fund() public payable &#123;\n        // Want to be able to set a minimum fund amount in currency\n        // 1. How do we send ETH to this contract?\n        require(msg.value &gt;= minimumETH, &quot;Didn&#39;t send enough!&quot;);\n        funders.push(msg.sender);\n        addressToAmountFunded[msg.sender] = msg.value;\n    &#125;\n\n    function withdraw() public &#123;\n        /* starting index, ending index, step amount */\n        for (uint256 funderIndex = 0; funderIndex &lt; funders.length; funderIndex++) &#123;\n            // code\n            address funder = funders[funderIndex];\n            addressToAmountFunded[funder] = 0;\n        &#125;\n        // resret the array\n        // actually withdraw the funds\n    &#125;\n&#125;所以我们已经重置了mapping的余额。\n但是我们还有两件事要完成，我们仍然需要重置数组以使资助者成为空数组，然后我们还需要真正提取资金，因为当我们给合约发送资金，我们在fund函数中发送了message.value，然而，我们实际上并没有提取资金。\n所以要去重置数组的话，我们可以遍历一遍它并从这个地址数组中删除里面的对象，或者我们可以整个重置这个变量。\n重置数组这一次我们不再通过循环遍历的方式删除所有数组内的对象，我们可以这样写funders &#x3D; new address array，这样做就会完全重置整个funders 数组。\nsolidity    function withdraw() public &#123;\n        /* starting index, ending index, step amount */\n        for (uint256 funderIndex = 0; funderIndex &lt; funders.length; funderIndex++) &#123;\n            // code\n            address funder = funders[funderIndex];\n            addressToAmountFunded[funder] = 0;\n        &#125;\n        // resret the array\n        funders = new address[](0);\n        // actually withdraw the funds\n    &#125;它现在是一个全新的address 类型数组了，并且里面没有任何（有0个）对象。\nsolidityfunders = new address[](1);如果我们把这里改成1，这个数组就会有1个初始元素在里面，改成2就是有两个，改成3就是有三个，以此类推。不过我们在这里想要的就只是一个全新的，空白的数组，如果这部分不太理解，不要纠结。\n很好，我们已经重置了这个数组。但我们现在究竟该如何实际地从这个合约中提取资金呢？或者说我们该如何把资金发送给合约的调用者？\ntransfer，send，和call现在，要想发送以太币或者其他区块链原生货币的话，有三种不同的方式可供使用，这三种方式我们都会看看，并且会说一下它们之间有什么区别。\n这三种方式分别是transfer，send，和call。\n让我们从transfer开始，因为transfer是最简单的，使用起来也是最直观的，所以如果我们想要转移资金，给调用这个withdraw函数的人，我们该这样写，msg.sender.transfer。\n然后我们要得到这个合约的余额，写下address(this).balance，关键字this指的就是这整个合约本身。这样我们就可以获得这个地址的区块链原生货币或者说以太币余额了。\n只用做这些就够了，我们唯一要再做的一件事就是需要做一下类型转换，我们要把msq.sender 从address类型转换到payable address类型，所以说msg.sender是一个address类型，而payable(msg.sender)是payable address类型。\nsolidity    function withdraw() public &#123;\n        /* starting index, ending index, step amount */\n        for (uint256 funderIndex = 0; funderIndex &lt; funders.length; funderIndex++) &#123;\n            // code\n            address funder = funders[funderIndex];\n            addressToAmountFunded[funder] = 0;\n        &#125;\n        // resret the array\n        funders = new address[](0);\n        // actually withdraw the funds\n\n        // transfer\n        // send\n        // call\n\n        // msg.sender = address\n        // payable(msg.sender) = payable address\n        payable(msg.sender).transfer(address(this).balance);\n    &#125;在solidity 中要想发送区块链原生货币比如以太币，你必须使用payable address类型才能做到，所以我们把它放到这个payable类型转换器里。\n这就是我们发送以太币的第一种方法，这种方法也可以用于不同合约之间互相发送代币，我们只需要把想要发送到的目标地址放到payable关键字里，然后写下.transfer，并且在这里告诉它我们到底要转移多少资金，但是transfer 还存在一些自身的问题。\n现在我们来到这个solidity-by-example网址的sending-ether页面：\nSending Ether (transfer, send, call) | Solidity by Example | 0.8.20 (solidity-by-example.org)\n另外，这个网址是一个非常好的参考资料，如果你有疑惑的话。\n\n我们刚刚看过的方法就是这个transfer方法，在之前的文章中我们已经知道了，如果我要把以太币，从一个地址发送到另一个地址，这笔交易大约消耗2100gas，我们的transfer函数的上限是2300gas，如果超出这个上限，它就会报错。\n下一个我们要使用的方法是send，它的消耗上限同样也是2300gas，而如果它运行失败了，则会返回一个布尔值。\n所以对于transfer来说，如果这一行运行失败，它会直接报错并且回滚交易，但使用send，就不会直接报错，而是会返回一个表示运行是否成功的布尔值。\n要想使用send，我们写下payable(msg.sender).send(address(this).balance)，但是我们不能就这么结束这一语句，如果这一行运行失败了，合约不会回滚交易，我们就只是单纯没拿到钱而已。\n所以我们这里还得写下bool sendSuccess等于整个这一部分，然后写下require sendSuccess，如果这里发送失败了就会报一个”Send failed”错误。\n这样做的话，如果这里运行失败，通过require语句我们还是可以回滚交易，如果transfer运行失败它会自动回滚交易，而send要想回滚交易，我们就必须在这里添加require语句。\nsolidity    function withdraw() public &#123;\n        /* starting index, ending index, step amount */\n        for (uint256 funderIndex = 0; funderIndex &lt; funders.length; funderIndex++) &#123;\n            // code\n            address funder = funders[funderIndex];\n            addressToAmountFunded[funder] = 0;\n        &#125;\n        // resret the array\n        funders = new address[](0);\n        // actually withdraw the funds\n\n        // transfer\n        payable(msg.sender).transfer(address(this).balance);\n        // send\n        bool sendSuccess = payable(msg.sender).send(address(this).balance);\n        require(sendSuccess, &quot;Send failed&quot;);\n        // call\n    &#125;很好，那么第三种发送以太币或者其他原生货币的方式是什么呢？\n就是这个call命令，call将会是我们接触的第一个在solidity 中实际使用的较为底层的命令，我们可以用它来调用几乎所有solidity的函数，甚至不需要依赖ABI，我们会在以后更深入的学习如何使用这一高级功能，但是现在，我们只学习一下如何使用它来发送ETH或者其他区块链原生货币。\ncall和send其实也很相似，我们写下payable(msg.sender).call，这个地方就是我们填写任何函数信息，或者说，任何我们想调用的某个合约的函数信息，我们现在并不打算调用哪个函数，所以我们在这里留白就行了，在这里填上两个引号以表明我们在此处留白。\nsoliditypayable(msg.sender).call(&quot;&quot;);现在我们反而像是要发送一笔交易一样来使用它了，正如我们在部署界面看到的，这里总是有一个用来提供 msg.value的位置。\n\n所以我们要把这个call函数当做一个普通交易来使用，并且要为它添加msg.value这样的东西，所以在这里，我们要添加一组花括号，这里我们写value: address(this).balance。\nsoliditypayable(msg.sender).call&#123;value: address(this).balance&#125;(&quot;&quot;);这个call函数实际上有两个返回值，当一个函数有两个返回值时，我们可以把它们放到左边的括号里来表示，这两个返回值的其中之一是布尔值，我们把它称为callSuccess，另外一个则是bytes对象，称之为dataReturned。\nsolidity(bool callSuccess, bytes memory dataReturned) =  payable(msg.sender).call&#123;value: address(this).balance&#125;(&quot;&quot;);因为call允许我们调用不同的函数，如果那个函数本身就返回一些数据或者说有返回值，我们就得把这个返回值给保存下来，它还返回callSuccess，当我们的函数被call成功调用时，这个值就是true，反之，就是false，并且因为 bytes对象是数组，所以dataReturned还需要放到memory中。\n现在对于我们这里的代码来说，我们实际上没有调用某个函数，所以我们其实不用关心dataReturned，我们只需要把这里删掉留下逗号，以此来告诉solidity，是的，我知道这个函数有两个返回值，但我们只关心其中的一个。\n然后就和上面send一样，我们要补上require，callSuccess，Call failed，意思就是我们要求callSuccess必须为true,否则，我们将回滚交易并抛出一个”Call failed”错误。\nsolidity    function withdraw() public &#123;\n        /* starting index, ending index, step amount */\n        for (uint256 funderIndex = 0; funderIndex &lt; funders.length; funderIndex++) &#123;\n            // code\n            address funder = funders[funderIndex];\n            addressToAmountFunded[funder] = 0;\n        &#125;\n        // resret the array\n        funders = new address[](0);\n        // actually withdraw the funds\n\n        // transfer\n        payable(msg.sender).transfer(address(this).balance);\n        // send\n        bool sendSuccess = payable(msg.sender).send(address(this).balance);\n        require(sendSuccess, &quot;Send failed&quot;);\n        // call\n        (bool callSuccess, ) =  payable(msg.sender).call&#123;value: address(this).balance&#125;(&quot;&quot;);\n        require(callSuccess, &quot;Call failed&quot;);\n    &#125;如果你现在还不太能搞懂它们三个之间的区别，不要过于纠结它们以至于拖累你的学习进度，等之后再回过头来重新审视这里，可以是在你学习了更多关于底层函数的工作机制后，也可以是学习了更多gas工作机制后。\nsolidity-by-example非常好的解释了这三者之间的区别，transfer最大允许2300 gas并且运行失败时会直接报错，send最大也允许2300 gas它运行失败时会返回一个布尔值，call转移所有gas所以它没有gas上限，并且和send一样返回一个布尔值，表示其运行成功，或者失败。\n目前来说，call是最推荐的发送和接收以太币或其他区块链原生货币的方式。\n现在，如果你对这部分还是有些疑惑，那就只看这里就行，这就是我们发送和转移 ETH或其他区块链原生通证的方式。\nsolidity// call\n(bool callSuccess, ) =  payable(msg.sender).call&#123;value: address(this).balance&#125;(&quot;&quot;);\nrequire(callSuccess, &quot;Call failed&quot;);代码：\nsolidity// Get funds from users\n// Withdraw funds\n// Set a minimum funding value in ETH\n\n// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract FundMe &#123;\n\n    uint256 public minimumETH = 0.01 * 1e18; // 1 * 10 ** 18\n\n    address[] public funders;\n    mapping(address =&gt; uint256) public addressToAmountFunded;\n    \n    function fund() public payable &#123;\n        // Want to be able to set a minimum fund amount in currency\n        // 1. How do we send ETH to this contract?\n        require(msg.value &gt;= minimumETH, &quot;Didn&#39;t send enough!&quot;);\n        funders.push(msg.sender);\n        addressToAmountFunded[msg.sender] = msg.value;\n    &#125;\n\n    function withdraw() public &#123;\n        /* starting index, ending index, step amount */\n        for (uint256 funderIndex = 0; funderIndex &lt; funders.length; funderIndex++) &#123;\n            // code\n            address funder = funders[funderIndex];\n            addressToAmountFunded[funder] = 0;\n        &#125;\n        // resret the array\n        funders = new address[](0);\n        (bool callSuccess, ) =  payable(msg.sender).call&#123;value: address(this).balance&#125;(&quot;&quot;);\n        require(callSuccess, &quot;Call failed&quot;);\n    &#125;\n&#125;构造函数如果我们点击编译FundMe.sol，我们确实看到它通过了编译。\n不过，这里还有一个小问题，现在，无论是谁都可以从这个合约提款，任何人都可以出资，这是我们想要的。但我们不希望随便谁都能提款，我们只想让那个募集资金的人能够真正提取资金。\n所以我们该怎么设置，才能够让这个withdraw函数只能被合约的owner调用呢？\n为此，我们要创建一组新函数，所以，当我们部署这个合约时，我们想让它自动进行设置，这样，无论是谁部署了这个合约都将成为合约的owner，然后我们就可以用一些参数，设定只有合约的拥有者才能调用withdraw函数，所以我们该怎么做呢？\n也许我们可以创建一个这样的函数，称为callMeRightAway，然后在我们部署合约后，立刻调用这个callMeRightAway函数，这样我们就能成为合约的拥有者了，不过这样就必须发起两次交易（部署一次，调用函数一次），如果我们不得不这样做的话，那未免也太麻烦了。\n因此，solidity 提供了一种称为构造函数（constructor）的东西，如果你熟悉其它编程语言的话，这个构造函数与其它编程语言的完全相同，构造函数的调用机制是这样的，它会在你部署合约后立即调用一次，所以，如果我要部署FundMe.sol，并且在这里写下minimumETH &#x3D; 2，minimumETH就不再是0.01乘10的18次方了，它会立即更新为2，这是因为构造函数在创建合约的那笔交易中被立刻调用了一次。\nsoliditycontract FundMe &#123;\n    uint256 public minimumETH = 0.01 * 1e18; // 1 * 10 ** 18\n    constructor() &#123;\n        // minimumETH = 2;\n    &#125;\n&#125;这个构造函数对我们来说有很大的用处，因为它允许我们以我们想要的方式设置合约，以我来们创建一个全局变量address public owner，然后在我们的构造函数内，我们让owner等于msg.sender，这个构造函数内的msg.sender，就是部署这个合约的人。\nsoliditycontract FundMe &#123;\n\n    uint256 public minimumETH = 0.01 * 1e18; // 1 * 10 ** 18\n\n    address[] public funders;\n    mapping(address =&gt; uint256) public addressToAmountFunded;\n    \n    address public owner;\n\n    constructor() &#123;\n        owner = msg.sender;\n    &#125;\n\n    function fund() public payable &#123;\n        // Want to be able to set a minimum fund amount in currency\n        // 1. How do we send ETH to this contract?\n        require(msg.value &gt;= minimumETH, &quot;Didn&#39;t send enough!&quot;);\n        funders.push(msg.sender);\n        addressToAmountFunded[msg.sender] = msg.value;\n    &#125;\n\n    function withdraw() public &#123;\n        /* starting index, ending index, step amount */\n        for (uint256 funderIndex = 0; funderIndex &lt; funders.length; funderIndex++) &#123;\n            // code\n            address funder = funders[funderIndex];\n            addressToAmountFunded[funder] = 0;\n        &#125;\n        // resret the array\n        funders = new address[](0);\n        (bool callSuccess, ) =  payable(msg.sender).call&#123;value: address(this).balance&#125;(&quot;&quot;);\n        require(callSuccess, &quot;Call failed&quot;);\n    &#125;\n&#125;Modifier现在我们已经设置好了合约的拥有者，现在我们就可以来修改我们的withdraw函数，使得只有拥有者才有权调用这个withdraw函数，所以在withdraw函数的顶部，或许我们想要添加一部分，也许可以这样，require(msg.sender &#x3D;&#x3D; owner)。\n关于双等号与等号这里说明一下，你可以把这个单等号视作一个设置参数，所以构造函数中的&#x3D;就是把owner 设置为msg.sender，双等号则是用来检查这两个变量是否等价，所以这里&#x3D;&#x3D;就是在问msg.sender和owner是否是一样的，所以&#x3D;&#x3D;是检查是否等价，&#x3D;是设置。\n所以我们写下的就是require msg.sender是否等于owner，不是的话，我们抛出一个错误，就说”Sender is not owner!”完美！\nsolidity    function withdraw() public &#123;\n        require(msg.sender == owner, &quot;Sender is not owner!&quot;);\n        /* starting index, ending index, step amount */\n        for (uint256 funderIndex = 0; funderIndex &lt; funders.length; funderIndex++) &#123;\n            // code\n            address funder = funders[funderIndex];\n            addressToAmountFunded[funder] = 0;\n        &#125;\n        // resret the array\n        funders = new address[](0);\n        (bool callSuccess, ) =  payable(msg.sender).call&#123;value: address(this).balance&#125;(&quot;&quot;);\n        require(callSuccess, &quot;Call failed&quot;);\n    &#125;我们通过这个简单的方法保证了withdraw函数只能被这个合约的拥有者所调用。\n现在我们假设这个合约有很多函数，它们都要求只能由合约的拥有者来调用，或者也可能是这个合约有大量函数，它们有着很多各不相同的要求，我们肯定不想反复复制这一行到所有的函数中去，所以我们该怎么办？\n这个时候就需要用到修饰器（modifier）了。\n现在我们要删掉这一行，并且在下面，创建一个modifier，modifier是一个可以直接在函数声明中添加的关键字，从而修饰（modify）给函数某些功能。\n我们要创建一个modifier并且称之为onlyOwner，然后把我们刚刚在withdraw里写的那一行粘贴过来，在下面我们还要添加一个下划线，现在我就可以把这个叫onlyOwner的修饰器，插入到我们的withdraw函数的函数声明中。\nsolidity    function withdraw() public onlyOwner &#123;\n        /* starting index, ending index, step amount */\n        for (uint256 funderIndex = 0; funderIndex &lt; funders.length; funderIndex++) &#123;\n            // code\n            address funder = funders[funderIndex];\n            addressToAmountFunded[funder] = 0;\n        &#125;\n        // resret the array\n        funders = new address[](0);\n        (bool callSuccess, ) =  payable(msg.sender).call&#123;value: address(this).balance&#125;(&quot;&quot;);\n        require(callSuccess, &quot;Call failed&quot;);\n    &#125;\n\n    modifier onlyOwner &#123;\n        require(msg.sender == owner, &quot;Sender is not owner!&quot;);\n        _;\n    &#125;所以这个修饰器到底有什么用？这个函数声明中的onlyOwner，我们可以这么理解，嘿，你这个withdraw函数，在你读取所有内部代码之前，先看看这个修饰器onlyOwner并且优先运行这里的代码，然后再运行下划线里的东西，这里的下划线表示运行余下的代码，此处指的就是调用这个withdraw函数。\n实际上，我们就是先运行了这个require语句，然后调用余下的代码。如果把这个require语句，放到下划线的下面，这将告诉我们的函数，先运行withdraw函数这里的全部代码，然后再运行这个require，不过我们还是想把这个require 放到前面。\n所以这就是修饰器（modifier）的工作机制。\n代码：\nsolidity// Get funds from users\n// Withdraw funds\n// Set a minimum funding value in ETH\n\n// SPDX-License-Identifier: MIT\npragma solidity ^0.6.22;\n\ncontract FundMe &#123;\n\n    uint256 public minimumETH = 0.01 * 1e18; // 1 * 10 ** 18\n\n    address[] public funders;\n    mapping(address =&gt; uint256) public addressToAmountFunded;\n    \n    address public owner;\n\n    constructor() &#123;\n        owner = msg.sender;\n    &#125;\n\n    function fund() public payable &#123;\n        // Want to be able to set a minimum fund amount in currency\n        // 1. How do we send ETH to this contract?\n        require(msg.value &gt;= minimumETH, &quot;Didn&#39;t send enough!&quot;);\n        funders.push(msg.sender);\n        addressToAmountFunded[msg.sender] = msg.value;\n    &#125;\n\n    function withdraw() public onlyOwner &#123;\n        /* starting index, ending index, step amount */\n        for (uint256 funderIndex = 0; funderIndex &lt; funders.length; funderIndex++) &#123;\n            // code\n            address funder = funders[funderIndex];\n            addressToAmountFunded[funder] = 0;\n        &#125;\n        // resret the array\n        funders = new address[](0);\n        (bool callSuccess, ) =  payable(msg.sender).call&#123;value: address(this).balance&#125;(&quot;&quot;);\n        require(callSuccess, &quot;Call failed&quot;);\n    &#125;\n\n    modifier onlyOwner &#123;\n        require(msg.sender == owner, &quot;Sender is not owner!&quot;);\n        _;\n    &#125;\n&#125;现在你可以尝试在测试网上部署这个合约了！！！\n后记到这里，博主的Solidity基础部分算是整理完了，但实际上，这并不完善，有些复杂的知识点，博主将之进行了省略。\n接口 &amp; 喂价、从GitHub中引入&amp; NPM、Solidity 中的浮点数计算、库、SafeMath库，溢出检查，和“unchecked”关键词\n此外，还有些许进阶知识，也没有写出来。\n概念型知识、Immutable &amp; Constant、Custom Error、Receive &amp; Fallback\n正如博主开头所叙，此文章参考了课程：freeCodeCamp S2 - YouTube（ Lesson 2 Pt. 1~Lesson 4 Pt. 18），如果你想完整地学习Solidity基础知识，请您去看这份视频，这份视频讲的十分完善和详细，耐心看下去，会有很大收获的。\nArticle link： https://tqgoblin.site/post/csdn/Solidity基础（详细易懂！！！）/  Author： Stephen  \n","slug":"csdn/Solidity基础（详细易懂！！！）","date":"2022-02-21T07:48:31.000Z","categories_index":"区块链学习","tags_index":"区块链 学习","author_index":"Stephen"},{"id":"8066bb1c5266753321f59d17d184c565","title":"MetaMask安装及使用（全网最全！！！）","content":"MetaMask安装及使用这应该是全网最详细的MetaMask安装及使用教程，如果您觉得它对你有帮助，请您记得点赞、收藏加关注哟，博主后面将会给您分享与区块链相关的知识文章。\n什么是MetaMask像以太坊这样的公共区块链是互联网数据库的下一个进化，而 MetaMask 是浏览器的下一个进化。\nMetaMask 是一个网页浏览器扩展版和移动版应用程序，用于管理以太坊私钥。这样，MetaMask 可以充当以太币和其他代币的钱包，也可以与去中心化应用程序 (dapps) 交互。\nMetaMask安装以Edge浏览器举例。（Chrome、Firefox、Brave都可以安装，这里仅以Edge举例）\n打开Edge浏览器，点击右上角 · · · ，点击扩展，并打开扩展网站。\n\n在左边搜索栏中搜索metamask，找到拓展，并点击右边的获取，选择添加扩展。\n\n接着我们便成功进入MetaMask的界面，勾选我同意，点击创建新钱包，如果你已经有了一个钱包，你可以通过助记词导入。（注意，这里需要连接以太坊主网络，即此处需要魔法，不知道魔法的小伙伴可以自行查阅）\n\n点击我同意。\n\n设置密码，创建新钱包。\n\n接下来进入最关键的环节，这里一定要认真对待，我们选择保护我的钱包，记住这里最重要的一件事，永远不要分享你的助记词！\n\n点击显示私钥助记词，并写下你的助记词，可以拿笔拿纸记录你的助记词，也可以复制粘贴到你的电脑上某一记事本内。但是记在txt文件里也是十分危险的行为，保不齐哪天电脑被入侵，你的助记词就被别人获取，相当于拿到了你的钱包，并可以对钱包里的以太币进行任意操作。所以最好的做法是，拿笔和纸记录你的助记词，并妥善保管好，或者使用密码管理软件，记录你的助记词。\n\n\n\n接着通过你记录下来的助记词，完成确认操作。\n\n这个时候，你的钱包就创建好了，当然你真的要往账户里存钱的时候，请务必要阅读来自MetaMask的提示，一定要备份密码或助记词。\n\nMetaMask会为你自动创建一个账户地址。\n\n现在，我们将MetaMask钉在右上方，可以获得更好的用户体验。\n\n可以看到右上方打开的页面和界面是一样的。\n\n区块链浏览器的使用区块链浏览器（Blockchain Explorer）是一种用于查看和分析区块链上交易和区块信息的工具。它提供了对区块链网络的实时数据和透明性的访问，能够帮助用户更好地了解和分析区块链上的数据，并增加对区块链网络的透明度和信任。\n区块链浏览器可以让用户查看所有的交易记录、区块高度、地址余额以及其他与区块链相关的信息。它通常会显示交易的发送方和接收方地址、交易金额、交易时间戳等信息。用户可以通过区块链浏览器来验证交易的有效性，并跟踪特定地址的余额和交易历史。\n区块链浏览器还提供了对区块链上智能合约的访问，可以查看智能合约的源代码、执行状态和交易历史。\n不同的区块链网络可能会有自己的区块链浏览器，例如比特币的区块链浏览器有Blockchain.com、Blockchair、Block Explorer等，以太坊的区块链浏览器有Etherscan、Ethplorer等。\n点击你的账户地址，就可以将它复制到剪切板。\n\n在地址栏中输入https://etherscan.io，就可以到一个叫区块链浏览器（这里特指Etherscan）的工具中，我们可以通过Etherscan在以太坊主网上看到刚创建的地址，在地址栏里粘贴你刚复制的账户地址即可。\n\n可以看到还没有任何交易发生，没有分析数据，没有评论，没有余额，没有资产价值，因为这是一个全新的钱包。\n查询的地址代表着我们的唯一地址，独一无二的钱包，唯一可以确定我们身份的东西。\n\n此外，我们可以点击这里，创建更多的账户，这里我创建了一个叫Account 2的账户。\n\n可以看到，这个账户有一个不同的地址，我们也可以复制这个地址，去Etherscan中查询，这个地址又是我们一个独一无二的身份。\n\nMetaMask中助记词、私钥和账户的关系我们可以通过这里，切换我们的账户，现在我们的钱包中有两个不同的账户。\n\n同一个助记词能够让我们创建多个账户，通过这个助记词，我们可以进入Account 1和Account 2和其他通过这个创建账户按钮所创建的账户，因为它能够让你进入MetaMask中所有的账户。\n这两个账户的地址是我们公开的身份，但是它们也有一个独一无二的私有身份，就像助记词一样，我们不能给他人分享和暴露这些私有身份，这就是账户的私钥，助记词可以让你进入多个账户，私钥只能让你进入这些账户中的一个。\n我们可以通过点击这里的 · · · （竖着）来查看私钥，查看账户明细，然后导出私钥，你只需要在这里输入密码，然后你就可以看到你的私钥，这个就是你账户的私钥，你可以认为私钥就是账户的密码，它可以让你创建交易，在正常情况下，不要共享你的私钥和助记词，如果某个人拥有这个私钥，那么他就可以进入Account 1，然而他不能进入Account 2，如果他们有我们的助记词，那么这两个账户，他们都可以进入，所以一定要保证你的助记词和私钥安全！\n\n拥有私钥，才拥有比特币和以太坊这些资产。\n通过水龙头（Faucets）获取测试币你可以看到账户的旁边，写着以太坊主网，这是我们的网络表单，点击它就能看到我们可以接入的所有网络。\n在以太坊主网上，可以发送交易来使用真正的钱，但是接下来，我们不用以太坊主网，相反我们会使用一个叫测试网的东西，因为我们作为工程师，我们经常需要测试代码，与代码交互，看看他们是否正常运行，我们将使用测试网和本地环境测试我们的代码，测试智能合约，我们主要是使用本地网络环境，但是我们也稍微看下MetaMask默认的测试环境。\n点击网络，我们把账户切换到这些测试网中的一个，比如我们可以点Sepolia，我们可以看到在Sepolia测试网上，我们也没有以太币，什么都没有，只有一个空钱包，除了不使用真实的钱，这些测试网和以太坊主网一样，它们使用测试币，让我们可以学习与合约交互，并且了解不同的合约如何进行协作。\nSepolia是当下最适合的测试网（博主写这篇博客的时候），因为测试网不是商业服务，所以服务不会太稳定，有可能这个网明天就挂了，你需要选择当下最适合的测试网。\n接下来，我们选择将网络切换到Sepolia测试网。\n\n现在，我们要做的是去Sepolia的Etherscan，只需要在etherscan.io前添加sepolia.即可。（懒人必备：https://sepolia.etherscan.io）\n\n同样的，我们可以输入账户的地址，来查询你的账户地址，然后我们就可以看到这个地址在Sepolia的具体信息，当然，什么都没有。\n\n总结一下，测试网是免费的，用来测试智能合约，主网会有费用，部署后就被认为是上线了，此外，测试网是非盈利的，所以不要滥用它，我们只用它来学习，所以不要在这些测试网上无限发送交易。\n现在，让我们在Sepolia测试网上发送一个交易，这将会给我们展示主网上交易是什么样的，为了能够模拟这个交易，我们会去叫一个水龙头（Faucets）的地方，以获取测试币。\n测试网水龙头（Testnet Faucets）\n主水龙头：Get Testnet LINK Tokens | Chainlink Faucets\n备用水龙头：Ethereum Sepolia Faucet\n现在，我们在faucets.chain.link下，获取一些测试的ETH代币，点击Connect wallet。（记得让MetaMask用Sepolia测试网）\n\n勾选接受，并选择MetaMask。\n\n点了之后，MetaMask会弹出来，询问你是否想要连接到这个网站，点击下一步，然后选择我们想要连接的账户，选择Account 1，点连接。\n\n连接之后，我们可以在右上角看到我们的账户，我们的账户地址也会自动填入。\n\n然后，我们来创建第一个测试交易，现在我们不需要LINK测试币，所以点掉它，现在我们只需要0.25个以太坊测试币，然后登录github账户，完成安全测试，点击发送交易。\n这个实际上就是请求水龙头给我们发0.25个以太坊测试币，测试网水龙头就是在测试网上获取测试币的地方，测试币不是真的钱，因为我们可以免费得到。（在主网上没有水龙头，你不能免费获取主网以太币）\n\n点击发送交易后，首先耐心等待一会儿。\n\n这里弹出来的是交易哈希，交易会被发送，等待确认，这意味着另一个钱包正在给我们发0.25个以太坊测试币，交易哈希就是该操作的交易，我们只需要等待交易完成验证和其他流程。\n\n最后完成我们的请求，当然，我们可以点击这个交易哈希。\n\n然后就会自动跳转到我们的交易信息界面，此刻可以看见状态是Indexing。\n\n如果上面的交易哈希没有出现，比如我们关掉它，然后我们可以复制一下我们的账户地址，然后回到Etherscan，粘贴地址，查询我们的账户。\n\n然后我们可以看到余额中有0.25个以太币，下面也可以看到一个发送到钱包的交易和其相关信息。\n\n点击左边的交易哈希。\n\n我们也可以看到交易信息界面，这个时候状态已经是Success了。\n\n区块链浏览器中的交易明细接下来，我们来介绍交易信息的细节，理解交易中发生了什么，是成为合约工程师必备的，也有助于了解整个生态。\n交易哈希（Transaction Hash）：这个是在这个区块链上这笔交易的唯一ID，这个交易哈希标识了发送了0.25个以太币到我们的地址的操作。\n状态（Status）：我们可以看到交易状态是成功的，它没有因某种情况而失败。\n区块（Block）：我们可以看到这个交易所在的区块高度。\n时间戳（Timestamp）：这里是时间戳，代表这个交易是何时发生的。\n发送者（From）：我们可以看到这个交易是由谁发送的，当然我们可以在新的标签页打开它，你就能看到发送交易的账户信息。\n接受者（To）：接受了交易的用户，这里就是我们自己。\n价值（Value）：交易的资产价值是0.25个以太币。\n交易手续费（Transaction Fee）：交易的手续费，付给矿工处理这笔交易的费用。\nGas价格（Gas Price）：Gas的价格，Gas价格是交易中每个执行单元的花费（用ether和gwei做单位），Gas价格越高，被写到区块中的机会越大。\n\n点击更多，我们可以在这里看到更多其他信息。\n\nGas Limit（Gas上限）：这里我们可以看一下，Gas上限是60000，实际上用了21000。\n所以这个交易使用了21000个Gas，对于发送以太币这样简单的交易，Gas比较便宜，然后我们可以算一下\nGas使用量 * Gas价格 &#x3D; 21000 * 0.000000049540425765 &#x3D; 0.001040348941065（交易手续费）\n我们得到了完全一样的交易手续费，给我们转账的人，除了发1个以太币以外，还支付了0.001040348941065个以太币作为手续费，每一个区块链都有不同的方式计算Gas费用，那些比较抽象，我们现在只关注这里的交易手续费和Gas价格。\n\n使用MetaMask进行转账这个时候，我们查看MetaMask，有2个账户，Account 1有0.25个Sepolia以太币，Account 2没有Sepolia以太币，如果我从Account 1发送0.1个以太币到另一个账户，我们会剩余多少以太币？\n让我们试一下，这将是你创建，并且会花手续费的第一个交易，我们点击发送，在我们的账户间转账，选择Account 2，输入0.1，下面可以看到Gas费估算，下面会有一个总计，是我们一共要为这个交易所花费的，0.1是我们发送的剩余的是需要的Gas费，最后点击确认，发送交易，然后我们点击这个交易记录，可以点击在区块链浏览器上查看。\n\n这样，你就成功实现了完全由你自己创建的第一笔交易了！\n\n有了这些基础知识，你知道了如何与链上应用交互，怎样发送交易和一些非技术细节，怎么与区块链交互，怎样与协议交互。\n这无疑是令人兴奋的！ 接下来就开始你的MetaMask之旅吧！\n顺便期待一下博主的go语言-区块链学习（三），里面将会介绍以太坊智能合约的编写和部署，文章正在编辑中。。。\nArticle link： https://tqgoblin.site/post/csdn/MetaMask安装及使用（全网最全！！！）/  Author： Stephen  \n","slug":"csdn/MetaMask安装及使用（全网最全！！！）","date":"2022-01-25T22:00:00.000Z","categories_index":"区块链学习","tags_index":"区块链 学习","author_index":"Stephen"},{"id":"e959118947d9361794da5ec67ee18329","title":"Remix-Desktop安装","content":"Remix-Desktop安装这篇文章主要介绍windows系统下Remix-Desktop如何安装，以及解决初次使用时，一直加载在Find Release : Latest页面，无法使用的问题。\nRemix-Desktop下载及安装Remix IDE（集成开发环境）适用于所有知识水平的用户在智能合约开发过程中的整个过程。它不需要任何设置，促进快速开发周期，并具有直观GUI的丰富插件集。该IDE有两种版本（Web应用程序或桌面应用程序），并可作为VSCode扩展。\n这里介绍桌面应用程序版的下载。\n下载：Releases · ethereum&#x2F;remix-desktop (github.com) - Remix Desktop 的发布页面。\n\n下载Lasted版本下的Remix-IDE-1.3.6-win.zip或者Remix-IDE-Setup-1.3.6.exe。（这里的1.3.6仅限于此文章发布期间，只要无脑选Lasted版本的下载就行）\nRemix-IDE-1.3.6-win.zip只需要解压到存放该软件的文件目录即可，打开后右建目录下的Remix IDE.exe创建快捷方式，再移动到桌面就行。\n\nRemix-IDE-Setup-1.3.6.exe无脑式安装，可以自己选择安装路径，安装过程中将自动创建快捷方式。\n双击打开Remix IDE。\n如果你的网络可以自动连上github，等待下载必要资源即可。\n否则，将一直卡在这个页面，等十个小时都没有用，问就是博主试过。\n\nFind Release : latest解决方案首先Win + R输入cmd并回车，然后ping github.com，看你的网络是否能ping通github.com\n\n如果和上图一样ping不通，那我们采用通用的修改host地址的方式，连上github.com（科学上网好像行不通，浏览器正常能访问github.com，但是控制台ping不通）\n修改host文件，访问github.com第一步：打开下面的链接，往下翻，找到下图所在位置，获得ip（记得自己查，时间隔的久，ip可能无效）\nGitHub.com - GitHub: Let’s build from here · GitHub (ipaddress.com)\n\n第二步：打开文件资源管理器，找到C:\\Windows\\System32\\drivers\\etc的路径位置，找到host文件\n\n第三步：双击host文件，选择用记事本打开\n\n第四步：在文件末尾添加 [你自己获取的ip] github.com，并Ctrl + S保存\n\n第五步：重新ping一下github.com试试\n\n开始使用Remix IDE重新打开Remix IDE，这个时候不出意外的话，就可以Loading了（如果还是炸了，多试几次，重启电脑，更换网络，清除缓存，Ctrl + R刷新界面等等，博主第二次安装也试过好几遍才成功）\n\n清除缓存，点击Help下的Clear the cache and restart Remix\n\n最后开始使用你的Remix IDE吧！\n\n附录remix官方文档：Welcome to Remix’s documentation! — Remix - Ethereum IDE 1 documentation (remix-ide.readthedocs.io)\n中文版本：欢迎使用 Remix 文档！ — Remix - Ethereum IDE 1 文档 (remix-ide.readthedocs.io)\n顺便期待一下博主的go语言–区块链学习（三），里面将介绍Remix IDE的使用和Solidity语言的学习，文章正在编辑中。。。\nArticle link： https://tqgoblin.site/post/csdn/Remix-Desktop安装/  Author： Stephen  \n","slug":"csdn/Remix-Desktop安装","date":"2022-01-24T04:33:19.000Z","categories_index":"区块链学习","tags_index":"区块链 学习","author_index":"Stephen"},{"id":"61ef945e5d20aa41bcaecf2484e095db","title":"go语言--区块链学习（二）","content":"模块二：区块链和比特币前记此模块为纯理论介绍，只是简单地介绍了区块链和比特币的关系，有关理论也是写的特别浅显，也没有内容要点总结归纳之类的，想要更加理解区块链的相关理论知识需要大家自己查阅资料，仔细研究。\n感兴趣的小伙伴可以看看这个视频系列：区块链之新 _ 纪录片 _ bilibili _ 哔哩哔哩弹幕视频网\n事实上，比特币网络的设计初衷是为了作为一种点对点的支付系统，因此其功能相对较为有限，主要用于进行价值转移和存储。\n然而，比特币的底层技术——区块链技术，为许多其他应用程序提供了基础设施，这些应用程序可以在区块链上构建，并且可以拥有更广泛的用途。\n例如，以太坊就是一种基于区块链的智能合约平台，它允许开发人员创建和部署各种类型的智能合约应用程序。与比特币不同，以太坊的设计目标是提供一个可编程的区块链，使开发人员能够在其上构建更灵活、更复杂的应用程序。\n对于我们程序员，因为比特币这一章没有具体开发的这一块的内容，所以博主只是在这里简单地介绍了点部分理论知识。\n大家也不需要知道比特币怎么去买卖，也不需要太过于关注比特币跌了还是涨了，想着怎么去一夜暴富。我们是学习技术的人，那是比用程序的人而更为高级的写程序的人，是互联网时代不可或缺的伟大缔造者之一，咱不用多么妄自菲薄，看轻自己，当你亲手写出来的程序得到大部分使用人的认可时，你将会有巨大的成就感和满足感，所以大家老老实实敲代码，为万恶的资本添砖加瓦吧。\n2.1 比特币介绍2.1.1 货币发展\n物物交换：最早期的人类社会中，人们通过直接交换商品来满足彼此的需求。\n实物货币：随着时间的推移，人们选择某些特定的贵重物品作为交换媒介，这些物品具有普遍接受和稀缺性，例如贵金属和贝壳。\n人工货币：为了方便交换，人们开始将贵重物品制成标准化的形状和重量，并加上政府或统治者的印记，这样的硬币可以更容易地被广泛接受并用于交换。随着经济的发展和贸易的扩大，纸币开始出现。纸币是由政府或中央银行发行的信用工具，其价值由政府或中央银行的信誉支撑。\n电子货币：随着科技的进步，电子货币逐渐流行起来。这种形式的货币可以通过电子设备进行交易和转移，例如银行卡、支付宝等。\n数字货币：近年来，随着区块链技术的发展，加密货币（如比特币）成为一种新兴的数字货币。加密货币基于分布式账本技术，去中心化并且具有高度安全性。\n\n2.1.2 诞生背景2.1.2.1 纸币的风险\n假币风险：纸币容易被伪造，尤其是高面值的纸币。伪造者可以通过技术手段制造假币，这对经济稳定和信任造成威胁。\n货币贬值：中央银行发行纸币时，过量的货币供应可能导致通货膨胀和货币贬值。当市场上流通的纸币数量超过实际需求时，购买力可能下降，人们的储蓄价值受到侵蚀。\n政治干预：纸币的发行受到政府和中央银行的控制，政府可能滥发货币以满足自身需求，或者通过调整利率和汇率等手段来影响经济运行。这种政治干预可能引发金融危机或不稳定。\n\n2.1.2.2 比特币2008年9月15日：美国雷曼兄弟宣布破产，引发全球金融危机，人们对传统金融机构和中央银行的信任受到了动摇。\n2008年10月31日：中本聪发表《比特币：一种点对点的电子现金系统》的论文，提出了比特币的概念，比特币被设计为一种去中心化的数字货币，旨在提供一种不依赖于传统金融机构的交易和储值方式。\n2009年1月3日：比特币网络正式开始运行，中本聪采用挖矿的方式来产生新的比特币。\n2009年1月中旬：中本聪挖出了比特币的创世区块（Genesis Block）。\n2.1.2.3 比特币是什么？比特币是一种数字货币，也被称为加密货币或虚拟货币。\n比特币基于区块链技术，采用去中心化的点对点网络进行交易，没有任何中央机构或政府控制。区块链是一个公共账本，记录着比特币交易的所有信息，确保交易的透明性和安全性。\n比特币的发行是通过矿工运算解决复杂的数学问题来完成的，这个过程被称为挖矿。挖矿不仅用于创建新的比特币，也用于验证和确认交易的有效性。\n2.2 去中心化2.2.1 中心化2.2.1.1 什么是中心化中心化（Centralization）是指权力、资源、决策等方面集中于某个中心，由这个中心来统一管理和控制的一种组织形式。在中心化组织中，权力和资源通常由某个特定的机构或个人掌控，其他人需要遵循其决策和规定进行行动。\n在计算机网络中，中心化也常用于描述某些应用程序或服务的架构方式。在中心化网络中，所有的节点都连接到一个中央服务器或节点，并通过该中央节点进行通信和数据传输。例如，传统的客户端-服务器模式就是一种中心化的网络架构方式。在这种模式中，客户端向服务器发送请求，服务器对请求进行处理并返回结果。\n2.2.1.2 优点和缺点中心化的优点包括决策效率高、资源使用效率高等，但缺点也十分明显，例如容易产生单点故障、安全性较低、透明度不足等。相比之下，去中心化的组织结构更加民主、自由，能够避免单一机构掌控所有权力和资源的问题，但也需要解决分散决策、资源利用效率低等问题。\n2.2.2 去中心化2.2.2.1 什么是去中心化去中心化（Decentralization）是指将权力、资源、决策等分散到多个节点或参与者之间，而不是集中在单一中心机构或个人手中的一种组织形式。在去中心化的组织结构中，权力和控制权被分散到各个节点，并通过协议、算法等机制来实现合作和决策。\n在技术领域，去中心化常用于描述一种网络架构或应用程序的设计模式。在去中心化网络中，参与者之间通过点对点的连接进行通信和交互，没有中央服务器或中介机构。每个节点都可以拥有相同的权限和能力，共同参与网络的管理和维护。\n总体而言，去中心化是一种追求权力分散、参与者自治和系统公正的组织形式，它在技术、经济和社会领域都有广泛的应用和影响。\n比特币和区块链技术是去中心化的典型例子。比特币是一种去中心化的数字货币，没有中央银行或政府机构掌控和发行。比特币网络由全球的矿工节点组成，他们通过计算生成新的区块并验证交易的有效性。这种去中心化的设计使得比特币具有抗审查、去信任、安全性高等特点。\n2.2.2.2 优点和缺点去中心化的优点包括降低单点故障的风险、提高系统的可靠性和韧性、增加参与者的权力和自主性等。去中心化还可以促进创新和合作，减少对中央权威的依赖，并提供更多的透明度和公正性。\n然而，去中心化也面临一些挑战和限制。例如，分散决策可能导致效率降低和协调困难，数据共享和隐私保护也需要更加细致的设计和解决方案。此外，去中心化的系统可能面临规模扩展和性能问题，需要通过技术创新来克服。\n2.2.2.3 如何实现去中心化\n区块链技术：区块链是一种去中心化的分布式账本技术，可以用于记录和验证交易或数据。区块链通过共识算法和分布式节点的协作来实现去中心化的数据存储和管理。比特币就是基于区块链技术构建的去中心化的数字货币。\nP2P网络：点对点网络（Peer-to-Peer，P2P）是一种直接连接参与者的网络架构，没有中央服务器或中介机构。P2P网络中的节点可以直接相互通信和交换数据，从而实现去中心化的信息传输和资源共享。\n分布式计算：将计算任务分配给多个节点进行并行处理，避免单一中心节点的瓶颈和单点故障，从而实现去中心化的计算。\n去中心化自治组织（DAO）：去中心化自治组织是一种利用智能合约和区块链技术实现组织自治和决策的方式。DAO的规则和运作由代码和协议来约束和执行，参与者可以通过投票和贡献来影响组织的发展和决策。\n\n2.2.2.4 比特币和去中心化的关系\n去中心化的发行和管理：比特币的发行和管理不依赖于任何中央银行或政府机构，而是通过去中心化的共识算法和分布式节点来实现。\n去中心化的交易验证：比特币网络中的矿工节点通过计算和验证交易来维护网络的安全和稳定，而没有一个单一的中心机构来控制和验证交易。\n去中心化的参与者：任何人都可以成为比特币网络的参与者，无需获得特殊权限或许可。每个节点都有相同的权力和能力，在网络中平等参与。\n\n总的来说，比特币的去中心化体现了一种基于分布式节点和共识算法的组织形式，使得比特币具有抗审查、去信任和安全性高的特点。但同时也需要注意到，去中心化并不意味着没有任何中心化的结构和问题，它需要通过技术和设计来平衡权力和效率的考量。\n2.3 区块链介绍2.3.1 什么是区块链区块链（Blockchain）是一种分布式账本技术，用于记录和验证交易或数据。它通过将交易按时间顺序组成一个不可篡改的数据块链，并通过加密和共识算法确保数据的安全性和一致性。\n区块链的核心概念是分布式、去中心化和不可篡改。在传统的中心化系统中，数据由中央机构或第三方机构掌控和管理，而在区块链中，数据由网络中的多个节点共同维护和验证，没有一个单一的中心机构控制。\n区块链的主要组成部分包括：\n\n数据块（Block）：数据块是区块链中的基本单位，用于存储一组交易或数据记录。每个数据块都包含一个唯一的标识符（哈希值）、时间戳和前一数据块的引用，形成一个链式结构。\n\n分布式网络（Distributed Network）：区块链网络由多个节点组成，每个节点都存储着完整的区块链副本。节点之间通过对等连接进行通信和数据传输。\n\n共识算法（Consensus Algorithm）：区块链网络中的节点通过共识算法来达成一致，并验证和记录新的数据块。常见的共识算法包括工作量证明（Proof of Work，PoW）、权益证明（Proof of Stake，PoS）等。\n\n加密算法（Cryptography）：区块链使用加密算法来确保数据的安全性和隐私性。每个数据块都通过哈希函数进行加密，而交易数据则经过公钥加密和签名来验证身份和完整性。\n\n\n2.3.2 记账记账（Accounting）指的是将交易或数据记录添加到区块链中的过程。在区块链中，每个交易都被封装成一个数据块，并通过共识算法获得网络节点的确认。一旦获得确认，该数据块就会被添加到区块链的末尾，成为最新的账本记录。记账的过程通常包括验证交易的有效性、加密数据以确保安全性，并根据共识算法确定数据块的顺序。\n\n提交交易：参与区块链网络的用户将要进行的交易提交到网络中。交易可以是转账、存储数据或执行智能合约等操作。\n验证交易：网络中的节点开始验证交易的有效性，包括检查交易的签名、余额是否足够等。验证的规则取决于具体的区块链协议和网络的共识算法。\n创建区块：一旦交易通过验证，节点将这些交易打包成一个新的数据块。数据块通常包含多个交易记录和其他元数据，如时间戳和前一数据块的哈希值。\n完成工作量证明（Proof of Work）：某些区块链网络使用工作量证明机制来确定哪个节点有权创建新的数据块。这个过程需要节点通过计算复杂的数学问题来寻找一个特定的哈希值，以证明他们完成了特定的工作量。\n数据块广播：一旦节点完成工作量证明，他们会将新的数据块广播给整个网络。其他节点接收到数据块后，会对其进行验证。\n共识达成：节点通过共识算法对收到的数据块进行验证，并决定是否接受该数据块。常见的共识算法包括Proof of Work（工作量证明）、Proof of Stake（权益证明）等。\n添加到链上：一旦数据块被大多数节点接受和验证，它将被添加到区块链的末尾，成为新的账本记录。数据块的哈希值将成为下一个数据块的前一数据块的引用，形成了一个不可篡改的链式结构。\n更新本地账本：每个节点在添加新数据块后会更新本地的账本副本，以反映最新的交易记录和状态变化。\n\n2.3.3 账本账本（Ledger）是区块链中存储和记录所有交易和数据的地方。它是一个按时间顺序组织的数据块链，每个数据块都包含一组交易记录。每个数据块都有一个唯一的标识符（哈希值），并包含前一数据块的引用，形成了一个不可篡改的链式结构。通过查阅账本，可以追溯到区块链的创始状态，并验证每个交易的完整性和合法性。\n区块链的账本具有以下特点：\n\n去中心化：账本由区块链网络中的多个节点共同维护，没有一个单一的中心机构控制。\n不可篡改：一旦数据被记录在区块链上，几乎不可能被修改或删除。每个数据块都包含前一数据块的引用，形成一个链式结构，保证了数据的完整性和可追溯性。\n透明和公开：区块链的账本是公开可见的，任何人都可以查看和验证数据，增加了透明度和公正性。\n高安全性：区块链使用加密算法和共识算法来保证数据的安全性和网络的稳定性，防止篡改和双重支付等问题。\n\n2.4 挖矿介绍2.4.1 钱包钱包是用于存储、管理和交换加密货币的工具或应用程序。它们提供了一个安全的数字环境，让用户可以管理他们的加密货币资产。钱包并不实际存储加密货币本身，而是存储用户的私钥和公钥，以便用户可以访问和控制他们的资产。\n2.4.1.1 私钥私钥是一串由数字和字母组成的字符串，是一种用于身份验证和数字签名的密码。私钥只有拥有者知道，并且不能被其他人访问或查看。私钥通常由随机数生成，可以用于签署交易和证明所有权。如果私钥丢失或被盗，用户将无法访问其加密货币。\n2.4.1.2 公钥公钥是与私钥相关联的另一个字符串，可以公开分享。它是通过对私钥进行数学运算而生成的。公钥用于验证数字签名的有效性和加密货币地址的生成。每个公钥对应唯一的加密货币地址，这是用于发送和接收加密货币的标识符。\n当用户使用钱包向其他人发送加密货币时，他们需要使用自己的私钥对交易进行数字签名。其他人可以使用用户的公钥来验证数字签名的有效性，并确认交易是否合法。如果数字签名有效，则交易得到确认并被添加到区块链上。\n2.4.2 节点在区块链中，节点是指网络中运行的计算机或设备，它们参与到区块链的维护和运行中。每个节点都具有独特的标识符，并且可以执行一些特定的任务，包括验证交易、创建新的区块、共识算法、存储数据等。\n2.4.2.1 全节点全节点（Full Node）：全节点是区块链网络中最重要的节点类型之一。它们完整地保留了整个区块链的副本，并可以验证和存储所有的交易数据。全节点不仅能够验证交易的有效性，还可以参与共识算法的运行和区块的创建。全节点对于区块链的安全性和去中心化特性至关重要。\n2.4.2.2 矿工节点矿工节点（Miner Node）：矿工节点是负责创建新的区块并将其添加到区块链上的节点。矿工节点通过执行复杂的计算任务来解决共识算法所需的工作量证明（Proof of Work）或其他共识机制。成功解决这些问题的矿工节点将获得一定数量的加密货币作为奖励。\n2.4.2.3 验证节点验证节点（Validator Node）：验证节点是专门用于验证交易和区块合法性的节点。它们参与共识算法，并通过验证交易的有效性来确保区块链的安全和一致性。验证节点通常需要满足一定的条件和资格要求，以确保它们具有足够的可信度。\n2.4.2.4 轻节点轻节点（Light Node）：轻节点是相对于全节点而言的一种轻量级节点。它们并不保存整个区块链的完整副本，而是仅保存一部分数据，如区块头或交易摘要。轻节点可以向全节点请求所需的信息，并依赖于全节点来验证交易和查询状态。\n2.4.2.5 超级节点超级节点（Super Node）：超级节点是在某些特定区块链网络中使用的一种节点类型。它们通常由具有较高性能和可靠性的计算机组成，并具有更大的存储容量和处理能力。超级节点在维护整个网络的稳定性和安全性方面扮演着重要角色。\n这些节点类型在区块链网络中相互协作，共同构建和维护整个区块链系统。每个节点的功能和角色可能因不同的区块链平台和协议而略有差异。\n2.4.2 挖矿2.4.2.1 什么是挖矿挖矿是指参与区块链网络维护和安全性保障的过程。在某些区块链网络中，比如比特币，挖矿是通过解决数学难题来创建新的区块并添加到区块链中的过程。挖矿是区块链网络中的节点为了获得奖励而进行的计算工作。\n以下是挖矿的一般过程：\n\n确认交易：在挖矿之前，矿工需要先确认待处理的交易。这些交易包括用户的转账请求、智能合约执行请求等。矿工会将这些交易打包成一个区块，并准备开始挖矿。\n工作量证明：挖矿的过程通常涉及到工作量证明机制，其中最常见的是Proof of Work（PoW）。矿工需要通过计算难题来找到一个符合特定条件的哈希值。这个计算过程需要不断尝试不同的数值，直到找到符合条件的哈希值为止。这个计算过程是高度计算密集的，需要大量的计算资源和能源消耗。\n验证与广播：一旦矿工找到了符合条件的哈希值，他们会将其与区块中的交易数据一起打包，并将这个新的区块广播到整个网络中。\n共识达成：其他节点接收到新的区块后，会对其进行验证。验证的过程包括检查工作量证明是否正确、交易的有效性等。如果大多数节点都接受并验证了这个新区块，那么就达成了共识，即这个区块被添加到区块链上。\n奖励与更新：作为挖矿的奖励，矿工获得一定数量的区块链网络代币（如比特币）。同时，他们的账本也会更新，记录他们创建了一个新的区块。\n\n挖矿的目的是通过计算和验证来确保区块链网络的安全性和完整性。通过工作量证明机制，挖矿抵制了恶意攻击者和操纵者对网络的攻击。挖矿参与者通过提供计算资源和维护网络的安全性来获得代币奖励，这也是激励机制的一部分，促使更多的人参与到区块链网络中。然而，值得注意的是，不同的区块链网络可能使用不同的共识算法和奖励机制，因此挖矿的具体细节会有所不同。\n2.5 比特币技术和结构2.5.1 算力、矿场、矿工\n算力：比特币的共识算法是工作量证明（Proof of Work，PoW），它要求矿工通过计算区块头的哈希值来找到符合一定条件的答案，从而获得新的比特币奖励。矿工所能提供的计算能力就是算力。\n矿场：指专门用于挖掘比特币或其他加密货币的设备集中的场所。矿场通常由大量矿机组成，它们利用高度优化的硬件和软件来提高算力和效率，以获得更多的比特币奖励。\n矿工：指参与比特币挖矿的个人或组织。矿工使用自己的计算机或专门设计的矿机来执行复杂的算法，并将解决方案提交给网络。矿工可以获得新的比特币奖励，同时也会收取一定的交易手续费。\n\n2.5.2 比特币参数\n区块链高度：指当前区块链中区块的数量，也就是区块链的长度。\n难度系数：难度系数代表挖矿的难度，它由比特币网络根据当前的总算力动态调整，以保持每10分钟产生一个新区块的稳定速率。\n区块奖励：指每个成功挖出新区块的矿工可以获得的比特币数量。创世区块的奖励是50个比特币，每隔210,000个区块奖励减半一次，目前的奖励是3.125个比特币。\n\n2.5.3 技术\n区块链：比特币是第一个应用区块链技术的加密货币。区块链是一个去中心化的分布式数据库，它将交易记录组成区块，并通过共识算法保证数据的安全性和一致性。\n共识算法：比特币的共识算法是工作量证明（PoW），它要求矿工通过计算区块头的哈希值来找到符合一定条件的答案，从而获得新的比特币奖励。除了PoW，还有其他共识算法，如权益证明（Proof of Stake，PoS）和权益共享（Delegated Proof of Stake，DPoS）等。\n分布式网络：比特币使用点对点（Peer-to-Peer，P2P）网络，没有中心化的服务器，所有节点都可以平等地参与到网络中来。这种分布式网络可以提高安全性和抗攻击能力。\n加密算法：比特币使用SHA-256算法进行加密，这是一种非对称加密算法，可以将数据转换为固定长度的哈希值，并保证不同的输入数据产生的哈希值是不同的。这种加密算法可以在保证数据安全的同时，避免了数据泄露。\n\n2.5.4 比特币地址生成规则\n比特币地址：比特币地址是一串由数字和字母组成的字符串，用于接收比特币的支付。比特币地址通常以1或3开头，长度为26-35个字符。\n私钥和公钥：比特币地址是通过私钥和公钥生成的。私钥是一个由随机数生成的256位二进制数，用于签名交易。公钥是由私钥生成的，它是一个由椭圆曲线加密算法（Elliptic Curve Cryptography，ECC）计算得出的256位二进制数。\n地址生成：比特币地址有一个版本号和校验和，其中校验和是通过对公钥哈希值进行两次SHA-256哈希运算得到的。然后将版本号、公钥哈希值和校验和合并为一个字符串，再对这个字符串进行Base58编码。编码后的结果就是比特币地址。\n\n2.5.5 比特币结构比特币的结构主要包括区块头和区块体两部分。\n2.5.5.1 区块头区块头是一个由80个字节组成的数据结构，包含了区块的元信息。具体来说，区块头包括以下内容：\n\n版本号：表示当前区块的版本。\n前一个区块的哈希值：指向前一个区块的哈希值，用于将区块串联起来。\nMerkle根：通过对区块中所有交易的哈希值进行两两配对，并再次进行哈希计算得到的根哈希值。\n时间戳：表示当前区块的生成时间。\n难度目标：表示当前区块链的难度系数。\n随机数：也称为Nonce，是矿工为了满足难度目标而不断尝试的参数。\n\n2.5.5.2 区块体区块体是一个由交易组成的数据结构，包含了区块的内容。具体来说，区块体包括以下内容：\n\n交易数量：表示当前区块中包含的交易数量。\n交易列表：即所有包含在区块中的交易记录，每条交易记录都包含了发送者、接收者、交易金额等信息。\n\n2.6 数字签名数字签名是一种密码学技术，用于验证一个文档、电子邮件或其他数据的真实性和完整性。数字签名可以确保数据未被篡改，并且只能由数据的拥有者进行签名。\n2.6.1 数字签名的作用数字签名的主要作用是保证数据的真实性和完整性。数字签名可以防止数据在传输或存储过程中被篡改，同时也可以保证数据的来源不被冒充。数字签名在电子商务、电子政务、数字版权等领域得到了广泛的应用。\n2.6.2 签名在进行数字签名之前，需要获取以下三个信息：\n\n原始数据：即要签名的数据。\n私钥：用于对原始数据进行加密，生成数字签名。\n数字签名算法：用于生成数字签名的算法，例如RSA、DSA、ECDSA等。\n\n2.6.3 验证在验证数字签名时，需要获取以下三个信息：\n\n原始数据：即已签名的数据。\n公钥：用于对数字签名进行解密，验证数字签名的真实性。\n数字签名算法：用于验证数字签名的算法，必须与签名时使用的算法相同。\n\n2.6.4 签名哪些数据数字签名通常只对原始数据进行签名，不包括任何附加信息或元数据。如果需要对多个数据进行签名，可以将这些数据合并成一个数据块，然后再进行签名。\n2.7 共识机制2.7.1 工作量证明（Proof of Work，PoW）\n工作量证明通过要求节点解决一个复杂的数学难题来获得记账权。这个难题通常需要大量的计算资源和能源来解决。\n解决问题的过程被称为挖矿，而解决问题并添加新区块的节点将获得一定数量的加密货币作为奖励。\n区块链系统中的所有节点都可以参与挖矿，但拥有更多计算能力的节点成功获得记账权的概率更高。\n\n2.7.2 权益证明（Proof of Stake，PoS）\n权益证明机制中，记账权与节点持有的加密货币数量相关。\n\n在PoS系统中，节点可以锁定一部分加密货币作为抵押品，从而增加获得记账权的机会。\n\n选择记账节点的过程是随机的，但持有更多加密货币的节点获得记账权的概率更高。\n\n\n2.7.3 权益证明+随机选择（Delegated Proof of Stake，DPoS）\nDPoS是基于PoS的共识机制，通过选举信任节点来进行记账。\n在DPoS系统中，持有加密货币的持币人可以投票选举一定数量的信任节点，这些节点负责验证交易和生成区块。\n选举产生的信任节点会周期性地轮流担任记账节点的角色，从而提高了交易速度和可扩展性。\n\n2.7.4 实用拜占庭容错（Practical Byzantine Fault Tolerance，PBFT）\nPBFT是一种面向拜占庭容错问题的共识机制，适用于具有较小网络延迟和较少节点数量的环境。\n在PBFT系统中，节点通过多轮消息交换和投票来达成共识。阈值设定为超过两个第三分之二的节点达成一致才能进行下一步操作。\n\n2.7.5 共享证明（Proof of Authority，PoA）\nPoA机制中，记账权授予特定的权威节点，这些节点被认为是可信任的。\n这些权威节点通常是由共识算法的设计者或网络管理员指定的，不依赖于计算工作量或加密货币持有量。\n由于权威节点的控制，PoA机制通常具有高吞吐量和低延迟的特点。\n\n未完待续。。。\nArticle link： https://tqgoblin.site/post/csdn/go语言--区块链学习（二）/  Author： Stephen  \n","slug":"csdn/go语言--区块链学习（二）","date":"2022-01-22T05:40:11.000Z","categories_index":"区块链学习","tags_index":"golang 区块链 学习","author_index":"Stephen"},{"id":"7f7afcc6a1e9fb63345e7b5f7e953c62","title":"go语言--区块链学习（一）","content":"模块一：密码学1.1 密码介绍1.1.1 为什么要加密 ？\n保护隐私和数据安全：在信息传输过程中，不加密的数据可以被未经授权的人员截获和查看，这可能导致个人隐私泄露、商业机密被窃取或者敏感数据被篡改。通过加密数据，可以确保只有授权的人员能够解密和访问数据，提高了数据的安全性。\n\n防止数据篡改：在信息传输过程中，数据可能会被篡改或修改，这可能导致信息内容的损坏或误导。通过加密数据，可以在接收方验证数据的完整性，确保传输的数据没有被篡改。\n\n防止重放攻击：重放攻击是指攻击者拦截并重放先前传输的数据，以达到欺骗或恶意目的。通过加密数据并使用防重放技术，可以防止攻击者复用先前的数据来进行攻击。\n\n遵守法律和合规要求：根据一些行业标准、法律法规和合规要求，某些类型的数据在传输过程中必须进行加密，以确保数据的安全性和保密性。加密数据可以帮助组织遵守相关的法律和合规要求。\n\n\n举例：\n小明–&gt;小红：\n\n原文：你好啊，可以加个好友吗\n密钥：+2\n密文：请问你儿童好雨哦啊怕是，地方可规划以接口加两种个形参好那么友请问吗\n密钥（密码）：-2\n解密：请问你儿童好雨哦啊怕是，地方可规划以接口加两种个形参好那么友请问吗\n你好啊，可以加个好友吗\n\n\n\n1.1.2 常见的几种加密算法\n编码解码：是将信息从一种形式转换为另一种形式的过程。编码通常用于将可读文本转换为二进制格式，以便在计算机之间传输或存储数据时使用。解码是将已编码的信息还原为原始形式的过程。\n哈希加密：也称为散列函数，是一种通过将任意长度的消息压缩为固定长度的摘要或哈希值的加密方法。哈希值是唯一的，并且由输入数据完全决定。但是，由于不同的消息可以生成相同的哈希值，因此哈希加密并不是完全安全的加密方式。\n对称加密：是指使用相同的密钥进行加密和解密的加密方式。这意味着发送方和接收方都必须知道密钥。对称加密速度快，但密钥需要在发送和接收之间传递。\n非对称加密：是指使用一对密钥（公钥和私钥）进行加密和解密的加密方式。公钥可公开，而私钥则必须保密。非对称加密安全性高，但速度较慢。\n数字签名：数字签名是一种加密技术，用于确保数字文档的完整性和身份验证。它使用非对称加密来创建一个数字签名，该签名包含文档的摘要和私钥。通过验证数字签名，可以确定文档是否被篡改，并且签名者是合法的。\n\n1.1.3 加密三要素\n明文&#x2F;密文：明文是未经过加密的数据，而密文是已经进行加密转换的数据。\n密钥：密钥是用于加密和解密数据的秘密值。密钥的安全性决定了数据的安全性。对称加密使用相同的密钥进行加密和解密，而非对称加密则使用一对密钥（公钥和私钥）进行加密和解密。\n加密算法&#x2F;解密算法：加密算法是用于将明文转换为密文的数学函数。解密算法是用于将密文转换回明文的逆函数。加密算法的安全性越高，破解难度就越大。\n\n加密三要素共同构成了加密系统，其中密钥的保护和管理非常重要，不当的密钥管理会导致加密系统的破解和信息泄露。\n1.2 编码解码1.2.1 常见的几种编码\nbase64：26个小写字母、26个大写字母、10个数字、&#x2F;、+\nbase58（区块链）：去掉6个容易混淆的，去掉0，大写的O、大写的I、小写的L、&#x2F;、+\n&#x2F;、+影响双击选择\n\n\n\n1.2.2 go实现base64编码、解码Go语言的encoding&#x2F;base64包提供了对base64编码和解码的支持。base64是一种将二进制数据转换为可打印ASCII字符的编码方式，常用于在网络传输中以文本形式表示二进制数据。\n该包中的主要函数有以下几个：\n\nfunc Encode(dst, src []byte)：将给定的字节切片src进行base64编码，并将结果存储在dst中。返回编码后的字节数。\nfunc Decode(dst, src []byte) (n int, err error)：将给定的base64编码的src进行解码，并将解码结果存储在dst中。返回解码后的字节数。\nfunc EncodeToString(src []byte) string：将给定的字节切片src进行base64编码，并返回编码后的字符串。\nfunc DecodeString(s string) ([]byte, error)：将给定的base64编码的字符串s进行解码，并返回解码后的字节切片。\n\n使用encoding&#x2F;base64包进行base64编码和解码非常简单，可以按照以下步骤进行操作：\n\n导入包：import “encoding&#x2F;base64”\n编码：调用base64包的EncodeToString函数，将要编码的数据作为参数传入，得到base64编码后的字符串。\n解码：调用base64包的DecodeString函数，将要解码的base64字符串作为参数传入，得到解码后的字节切片。\n\n通过encoding&#x2F;base64包，可以方便地进行base64编码和解码，常用于处理二进制数据在文本传输中的需求，如图像、音频等文件的传输与存储。\n\ntxtbase64编码\n\ngosrc := []byte(&quot;hello&quot;)\nbuf := base64.StdEncoding.EncodeToString(src)\n\n// 在url中使用\nsrc := []byte(&quot;http://127.0.0.1:8080/?name=hello&quot;)\nbuf := base64.URLEncoding.EncodeToString(src)\ntxtbase64解码\n\ngodbuf, err := base64.StdEncoding.DecodeString(buf)\nif err != nil &#123;\n    fmt.Println(err)\n    return\n&#125;\nfmt.Println(string(dbuf))代码：\ngopackage main\n\nimport (\n    &quot;encoding/base64&quot;\n    &quot;fmt&quot;\n)\n\nfunc main() &#123;\n    &#123;\n        src := []byte(&quot;hello&quot;)\n        buf := base64.StdEncoding.EncodeToString(src)\n        dbuf, err := base64.StdEncoding.DecodeString(buf)\n        if err != nil &#123;\n            fmt.Println(err)\n            return\n        &#125;\n        fmt.Println(string(dbuf))\n    &#125;\n    &#123;\n        src := []byte(&quot;http://127.0.0.1:8080/?name=hello&quot;)\n        buf := base64.URLEncoding.EncodeToString(src)\n        dbuf, err := base64.URLEncoding.DecodeString(buf)\n        if err != nil &#123;\n            fmt.Println(err)\n            return\n        &#125;\n        fmt.Println(string(dbuf))\n    &#125;\n&#125;\n1.2.3 go实现base58编码、解码base58编码表\n\n\n\nValue\nChar\nValue\nChar\nValue\nChar\nValue\nChar\n\n\n\n0\n1\n15\nG\n30\nX\n45\nn\n\n\n1\n2\n16\nH\n31\nY\n46\no\n\n\n2\n3\n17\nJ\n32\nZ\n47\np\n\n\n3\n4\n18\nK\n33\na\n48\nq\n\n\n4\n5\n19\nL\n34\nb\n49\nr\n\n\n5\n6\n20\nM\n35\nc\n50\ns\n\n\n6\n7\n21\nN\n36\nd\n51\nt\n\n\n7\n8\n22\nP\n37\ne\n52\nu\n\n\n8\n9\n23\nQ\n38\nf\n53\nv\n\n\n9\nA\n24\nR\n39\ng\n54\nw\n\n\n10\nB\n25\nS\n40\nh\n55\nx\n\n\n11\nC\n26\nT\n41\ni\n56\ny\n\n\n12\nD\n27\nU\n42\nj\n57\nz\n\n\n13\nE\n28\nV\n43\nk\n\n\n\n\n14\nF\n29\nW\n44\nm\n\n\n\n\n字符1代表0，字符2代表1，…，字符z代表57\n\nbase58编码：\n\n\n数据分块：首先将二进制数据划分为固定大小的块。每个块通常包含特定数量的位，通常为8位（1字节）大小。这些块一个接一个地进行处理。\n\n二进制转十进制转换：将每个二进制数据块转换为一个十进制值。此转换通过将二进制数据解释为基于256的整数来执行（因为每个字节有256个可能的值）。\n\n映射到base58字符集：然后将在上一步中获得的十进制值映射到base58字符集中对应的字符。此映射是通过反复将十进制值除以58并使用余数作为索引从base58集合中选择字符来完成的。\n\n构建编码字符串：从映射过程中获得的字符被连接在一起以形成base58编码字符串。\n\n处理前导零：在某些情况下，原始二进制数据中的前导零可能导致编码字符串中出现前导的 ‘1’ 字符。这些前导的 ‘1’ 字符通常被省略或替换为特殊字符（例如 ‘1’），以保持可读性。\n\n\n\ngoconst (\n    b58Alphabet = &quot;123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz&quot;\n)\n\nfunc Base58Encode(input []byte) string &#123;\n    var result strings.Builder\n\n    // 将待编码的字节数组input转换成大整数，如：hello(104 101 108 108 111) --&gt; 104*256^4 + 101*256^3 + 108*256^2 + 108*256^1 + 111*256^0 = 448378203247\n    x := new(big.Int).SetBytes(input)\n\n    base := big.NewInt(58)\n    zero := big.NewInt(0)\n\n    // 对大整数x进行遍历，每次将x除以58，得到商和余数。将余数映射到Base58字母表中，并将结果存储在字符串构建器result中。不断重复该过程，直到商为0\n    for x.Cmp(zero) &gt; 0 &#123;\n        mod := new(big.Int)\n        x.DivMod(x, base, mod)\n        result.WriteByte(b58Alphabet[mod.Int64()])\n    &#125;\n\n    // 处理前导0：对输入的字节数组进行遍历，统计前导的0的个数。根据Base58编码规则，在编码后的字符串中会添加相应数量的前导1\n    for _, b := range input &#123;\n        if b != 0 &#123;\n            break\n        &#125;\n        result.WriteByte(b58Alphabet[0])\n    &#125;\n\n    // 将result中的字符串反转，得到最终的Base58编码结果\n    reversedResult := result.String()\n    reversedResultBytes := []byte(reversedResult)\n    for i, j := 0, len(reversedResultBytes)-1; i &lt; j; i, j = i+1, j-1 &#123;\n        reversedResultBytes[i], reversedResultBytes[j] = reversedResultBytes[j], reversedResultBytes[i]\n    &#125;\n\n    return string(reversedResultBytes)\n&#125;\n\nbase58解码：\n\n\n字符转换为十进制：要解码base58编码的字符串，需要根据base58字符集将字符串中的每个字符转换回其对应的十进制值。\n\n十进制转二进制转换：从字符转换中获得的十进制值然后被转换回二进制数据。\n\n重建二进制数据：然后将上一步中的二进制数据连接起来以重建原始的二进制数据。现在可以将这些数据用于各种目的，例如数据反序列化或加密操作。\n\n\n\ngofunc Base58Decode(input string) []byte &#123;\n    result := big.NewInt(0)\n    base := big.NewInt(58)\n    zeroBytes := 0\n\n    // 处理前导0：对输入的字符串进行遍历，统计前导的0的个数，直到遇到第一个非0的字符为止\n    for _, b := range input &#123;\n        if byte(b) == b58Alphabet[0] &#123;\n            zeroBytes++\n        &#125; else &#123;\n            break\n        &#125;\n    &#125;\n\n    // 对输入字符串去除前导的0后，对其余字符进行遍历。通过在Base58字母表中查找当前字符所在的位置，得到该字符的数值，并将其乘以58的n次方（n为字符所在的位置），累加到result中\n    payload := input[zeroBytes:]\n    for _, b := range []byte(payload) &#123;\n        charIndex := strings.IndexByte(b58Alphabet, b)\n        if charIndex == -1 &#123;\n            return []byte&#123;&#125;\n        &#125;\n        result.Mul(result, base)\n        result.Add(result, big.NewInt(int64(charIndex)))\n    &#125;\n\n    // 将result转换成字节数组，并在前面添加zeroBytes个0，得到最终的解码结果\n    resultBytes := result.Bytes()\n    resultBytes = append(bytes.Repeat([]byte&#123;byte(0)&#125;, zeroBytes), resultBytes...)\n\n    return resultBytes\n&#125;\n代码：\ngopackage main\n\nimport (\n    &quot;bytes&quot;\n    &quot;fmt&quot;\n    &quot;math/big&quot;\n    &quot;strings&quot;\n)\n\nconst (\n    b58Alphabet = &quot;123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz&quot;\n)\n\nfunc Base58Encode(input []byte) string &#123;\n    var result strings.Builder\n\n    x := new(big.Int).SetBytes(input)\n    base := big.NewInt(58)\n    zero := big.NewInt(0)\n\n    for x.Cmp(zero) &gt; 0 &#123;\n        mod := new(big.Int)\n        x.DivMod(x, base, mod)\n        result.WriteByte(b58Alphabet[mod.Int64()])\n    &#125;\n\n    // Add leading 1&#39;s for zeros in the input\n    for _, b := range input &#123;\n        if b != 0 &#123;\n            break\n        &#125;\n        result.WriteByte(b58Alphabet[0])\n    &#125;\n\n    // Reverse the result\n    reversedResult := result.String()\n    reversedResultBytes := []byte(reversedResult)\n    for i, j := 0, len(reversedResultBytes)-1; i &lt; j; i, j = i+1, j-1 &#123;\n        reversedResultBytes[i], reversedResultBytes[j] = reversedResultBytes[j], reversedResultBytes[i]\n    &#125;\n\n    return string(reversedResultBytes)\n&#125;\n\nfunc Base58Decode(input string) []byte &#123;\n    result := big.NewInt(0)\n    base := big.NewInt(58)\n    zeroBytes := 0\n\n    for _, b := range input &#123;\n        if byte(b) == b58Alphabet[0] &#123;\n            zeroBytes++\n        &#125; else &#123;\n            break\n        &#125;\n    &#125;\n\n    payload := input[zeroBytes:]\n    for _, b := range []byte(payload) &#123;\n        charIndex := strings.IndexByte(b58Alphabet, b)\n        if charIndex == -1 &#123;\n            return []byte&#123;&#125;\n        &#125;\n        result.Mul(result, base)\n        result.Add(result, big.NewInt(int64(charIndex)))\n    &#125;\n\n    resultBytes := result.Bytes()\n    resultBytes = append(bytes.Repeat([]byte&#123;byte(0)&#125;, zeroBytes), resultBytes...)\n\n    return resultBytes\n&#125;\n\nfunc main() &#123;\n    input := []byte(&quot;hello&quot;)\n    encoded := Base58Encode(input)\n    fmt.Println(&quot;Encoded:&quot;, encoded)\n\n    decoded := Base58Decode(encoded)\n    fmt.Println(&quot;Decoded:&quot;, string(decoded))\n&#125;\n1.3 哈希算法1.3.1 特点\n固定长度输出：哈希算法将任意长度的输入数据映射为固定长度的输出，通常以固定位数的二进制数或十六进制字符串表示。这使得哈希算法在需要固定长度标识和比较数据的场景中非常有用。\n单向性（不可逆）：哈希算法是一种单向函数，也就是说，从哈希值无法推导出原始数据。给定一个哈希值，很难通过逆运算还原出原始数据。这种特性使得哈希算法在密码学中被广泛应用于加密、数字签名和身份验证等领域。\n高效性：哈希算法的计算速度通常很快。对于给定的输入数据，哈希算法能够迅速生成对应的哈希值。这使得哈希算法在大规模数据处理、密码校验和数据索引等方面得到广泛应用。\n雪崩效应（防篡改）：哈希算法具有雪崩效应，即输入数据的微小变化会导致输出哈希值的巨大变化。这意味着稍微改变输入数据的任何部分，都会导致生成的哈希值完全不同。这种特性使得哈希算法在校验数据完整性和防止冲突等方面非常有用。\n碰撞概率：由于哈希算法的输出空间是有限的，不同的输入数据可能会生成相同的哈希值，这被称为碰撞。好的哈希算法应该使得碰撞的概率非常低，即使在大量数据的情况下也能保持碰撞的概率较小。\n\n如：一般网站的账号密码会经过哈希加密后存储到数据库中，以保护用户的账号密码安全。\n1.3.2 常用的几种哈希算法\nMD4（Message Digest Algorithm 4）：由Ron Rivest于1990年设计的一种哈希算法，已被认为不安全，不推荐使用。\nMD5（Message Digest Algorithm 5）：也是由Ron Rivest设计的一种哈希算法，常用于校验数据完整性和密码存储。然而，由于其存在碰撞漏洞，不适合用于安全性要求较高的场景。\nSHA-1（Secure Hash Algorithm 1）：由美国国家安全局（NSA）设计的一种哈希算法，常用于校验数据完整性和数字签名。然而，由于其存在碰撞漏洞，安全性逐渐被破坏，不再推荐使用。\nSHA-224：SHA-224是SHA-256的一个变种，生成224位的哈希值。\nSHA-256：SHA-256是SHA-2系列中的一种哈希算法，生成256位的哈希值。目前被广泛使用，能够提供较高的安全性。\nSHA-384：SHA-384是SHA-2系列中的一种哈希算法，生成384位的哈希值。相比于SHA-256，SHA-384具有更大的输出长度，提供更高的安全性。\nSHA-512：SHA-512是SHA-2系列中的一种哈希算法，生成512位的哈希值。与SHA-256相比，SHA-512在安全性方面更强。\n\n需要注意的是，虽然MD5、SHA-1、SHA-224、SHA-256、SHA-384和SHA-512在某些场景下仍可用于非安全性需求，但对于安全性要求较高的应用，建议使用更强大的哈希算法，如SHA-256或SHA-512。\n1.3.3 go实现哈希算法举例1.3.3.1 go实现md4加密需要引入第三方包（失败，请配置环境变量GOPROXY&#x3D;https://goproxy.cn）\ngogo get golang.org/x/crypto/md4代码：\ngopackage main\n\nimport (\n    &quot;encoding/hex&quot;\n    &quot;fmt&quot;\n    &quot;golang.org/x/crypto/md4&quot;\n)\n\nfunc Md4Encrypt(str string) string &#123;\n    // 创建一个 md4.New() 对象，该对象用于计算 MD4 哈希值\n    hasher := md4.New()\n    // 将输入的字符串转换为字节数组，并将其写入哈希对象中\n    hasher.Write([]byte(str))\n    // 调用 hasher.Sum(nil) 完成哈希计算\n    hash := hasher.Sum(nil)\n    // 将哈希值转换为十六进制字符串\n    return hex.EncodeToString(hash)\n&#125;\n\nfunc main() &#123;\n    str := &quot;hello, world!&quot;\n    fmt.Println(Md4Encrypt(str)) // 输出：03cc95120b718b883b96bce3706d64b7\n&#125;\n1.3.3.2 go实现md5加密代码：\ngopackage main\n\nimport (\n    &quot;crypto/md5&quot;\n    &quot;encoding/hex&quot;\n    &quot;fmt&quot;\n)\n\nfunc Md5Encrypt(str string) string &#123;\n    // 创建一个 md5.New() 对象，该对象用于计算 MD5 哈希值\n    hasher := md5.New()\n    // 将输入的字符串转换为字节数组，并将其写入哈希对象中\n    hasher.Write([]byte(str))\n    // 调用 hasher.Sum(nil) 完成哈希计算\n    hash := hasher.Sum(nil)\n    // 将哈希值转换为十六进制字符串\n    return hex.EncodeToString(hash)\n&#125;\n\nfunc main() &#123;\n    str := &quot;hello, world!&quot;\n    fmt.Println(Md5Encrypt(str)) // 输出：3adbbad1791fbae3ec908894c4963870\n&#125;\n1.3.3.3 go实现sha256加密代码：\ngopackage main\n\nimport (\n    &quot;crypto/sha256&quot;\n    &quot;encoding/hex&quot;\n    &quot;fmt&quot;\n)\n\nfunc Sha256Encrypt(str string) string &#123;\n    // 创建一个 sha256.New() 对象，该对象用于计算 SHA-256 哈希值\n    hasher := sha256.New()\n    // 将输入的字符串转换为字节数组，并将其写入哈希对象中\n    hasher.Write([]byte(str))\n    // 调用 hasher.Sum(nil) 完成哈希计算\n    hash := hasher.Sum(nil)\n    // 将哈希值转换为十六进制字符串\n    return hex.EncodeToString(hash)\n&#125;\n\nfunc main() &#123;\n    str := &quot;hello, world!&quot;\n    fmt.Println(Sha256Encrypt(str)) // 输出：68e656b251e67e8358bef8483ab0d51c6619f3e7a1a9f0e75838d41ff368f728\n&#125;\n1.4 对称加密1.4.1 特点\n加密和解密使用相同的密钥。\n加密速度较快，适合处理大量数据。\n对称加密算法通常较简单。\n\n1.4.2 优点\n加密和解密的速度快，适用于大量数据的加密和传输。\n实现简单，计算成本低。\n\n1.4.3 缺点\n密钥的管理和分发可能存在安全风险。\n需要在通信双方之间事先共享密钥。\n\n1.4.4 场景分析\n对称加密适用于需要快速加密和解密大量数据的场景，如文件加密、数据库加密等。\n由于密钥的管理和分发存在风险，对称加密更适用于双方已经建立了安全信任关系的场景。\n\n1.4.5 常见的对称加密方式\nDES（Data Encryption Standard）：使用56位密钥，已经被更安全的算法所取代。\n3DES（Triple Data Encryption Algorithm）：对DES进行三次加密以增加安全性。\nAES（Advanced Encryption Standard）：目前最常用的对称加密算法，支持128、192和256位密钥。\n\n1.4.5.1 go实现des加密代码：\ngopackage main\n\nimport (\n    &quot;bytes&quot;\n    &quot;crypto/cipher&quot;\n    &quot;crypto/des&quot;\n    &quot;encoding/base64&quot;\n    &quot;fmt&quot;\n)\n\nfunc main() &#123;\n    key := []byte(&quot;01234567&quot;)          // DES密钥，8字节\n    plaintext := []byte(&quot;Hello, DES!&quot;) // 明文数据\n\n    // 调用encrypt()函数对明文进行加密，返回密文ciphertext和可能的错误err\n    ciphertext, err := encrypt(key, plaintext)\n    if err != nil &#123;\n        fmt.Println(&quot;加密失败:&quot;, err)\n        return\n    &#125;\n\n    // 使用Base64编码将密文转换为字符串并打印出来\n    fmt.Printf(&quot;加密结果: %s\\n&quot;, base64.StdEncoding.EncodeToString(ciphertext))\n\n    // 调用decrypt()函数对密文进行解密，返回解密后的明文decryptedText和可能的错误err\n    decryptedText, err := decrypt(key, ciphertext)\n    if err != nil &#123;\n        fmt.Println(&quot;解密失败:&quot;, err)\n        return\n    &#125;\n\n    // 打印解密后的明文\n    fmt.Printf(&quot;解密结果: %s\\n&quot;, decryptedText)\n&#125;\n\n// 定义pad()函数，用于对数据进行填充以适应加密算法的块大小\nfunc pad(data []byte, blockSize int) []byte &#123;\n    padding := blockSize - len(data)%blockSize              // 计算需要填充的字节数padding\n    padText := bytes.Repeat([]byte&#123;byte(padding)&#125;, padding) // 使用bytes.Repeat()函数创建一个字节数组，其中每个元素都是padding的值\n    return append(data, padText...)                         // 将填充的数据追加到原始数据后面并返回\n&#125;\n\n// 定义unpad()函数，用于去除填充数据\nfunc unpad(data []byte) []byte &#123;\n    padding := data[len(data)-1]         // 获取最后一个字节作为填充字节数padding\n    return data[:len(data)-int(padding)] // 返回去除填充后的数据\n&#125;\n\n// 定义encrypt()函数，用于进行DES加密\nfunc encrypt(key, plaintext []byte) ([]byte, error) &#123;\n    block, err := des.NewCipher(key) // 创建DES实例\n    if err != nil &#123;\n        return nil, err\n    &#125;\n\n    blockSize := block.BlockSize()        // 获取加密算法的块大小\n    plaintext = pad(plaintext, blockSize) // 调用pad()函数对明文进行填充\n\n    ciphertext := make([]byte, blockSize+len(plaintext)) // 创建一个字节数组ciphertext，长度为块大小加上填充后的明文长度\n    iv := ciphertext[:blockSize]                         // 将ciphertext的前块大小个字节作为初始向量iv\n    mode := cipher.NewCBCEncrypter(block, iv)            // 创建CBC模式的加密器mode\n    mode.CryptBlocks(ciphertext[blockSize:], plaintext)  // 使用加密器对填充后的明文进行加密，并将结果保存到ciphertext中\n\n    return ciphertext, nil // 返回加密后的密文\n&#125;\n\n// 定义decrypt()函数，用于进行DES解密\nfunc decrypt(key, ciphertext []byte) ([]byte, error) &#123;\n    block, err := des.NewCipher(key) // 创建DES实例\n    if err != nil &#123;\n        return nil, err\n    &#125;\n\n    blockSize := block.BlockSize() // 获取解密算法的块大小\n\n    if len(ciphertext) &lt; blockSize &#123; // 检查密文的长度是否小于块大小\n        return nil, fmt.Errorf(&quot;ciphertext too short&quot;)\n    &#125;\n\n    iv := ciphertext[:blockSize]        // 将密文的前块大小个字节作为初始向量iv\n    ciphertext = ciphertext[blockSize:] // 截取密文中除去初始向量部分的数据\n\n    if len(ciphertext)%blockSize != 0 &#123; // 检查密文长度是否是块大小的整数倍\n        return nil, fmt.Errorf(&quot;ciphertext is not a multiple of the block size&quot;)\n    &#125;\n\n    plaintext := make([]byte, len(ciphertext)) // 创建一个字节数组plaintext，用于保存解密后的明文\n\n    mode := cipher.NewCBCDecrypter(block, iv) // 创建CBC模式的解密器mode\n    mode.CryptBlocks(plaintext, ciphertext)   // 使用解密器对密文进行解密，并将结果保存到plaintext中\n\n    return unpad(plaintext), nil // 返回去除填充的明文\n&#125;\n1.4.5.2 go实现3des加密代码：\ngopackage main\n\nimport (\n    &quot;bytes&quot;\n    &quot;crypto/cipher&quot;\n    &quot;crypto/des&quot;\n    &quot;fmt&quot;\n)\n\nfunc main() &#123;\n    key := []byte(&quot;0123456789abcdef01234567&quot;) // 3DES密钥，24字节\n    plaintext := []byte(&quot;Hello, 3DES!&quot;)       // 明文数据\n\n    ciphertext, err := encrypt(key, plaintext)\n    if err != nil &#123;\n        fmt.Println(&quot;加密失败:&quot;, err)\n        return\n    &#125;\n\n    fmt.Printf(&quot;加密结果: %x\\n&quot;, ciphertext)\n\n    decryptedText, err := decrypt(key, ciphertext)\n    if err != nil &#123;\n        fmt.Println(&quot;解密失败:&quot;, err)\n        return\n    &#125;\n\n    fmt.Printf(&quot;解密结果: %s\\n&quot;, decryptedText)\n&#125;\n\nfunc pad(data []byte, blockSize int) []byte &#123;\n    padding := blockSize - len(data)%blockSize\n    padText := bytes.Repeat([]byte&#123;byte(padding)&#125;, padding)\n    return append(data, padText...)\n&#125;\n\nfunc unpad(data []byte) []byte &#123;\n    padding := data[len(data)-1]\n    return data[:len(data)-int(padding)]\n&#125;\n\nfunc encrypt(key, plaintext []byte) ([]byte, error) &#123;\n    block, err := des.NewTripleDESCipher(key)\n    if err != nil &#123;\n        return nil, err\n    &#125;\n\n    blockSize := block.BlockSize()\n    plaintext = pad(plaintext, blockSize)\n\n    ciphertext := make([]byte, blockSize+len(plaintext))\n    iv := make([]byte, blockSize)\n    mode := cipher.NewCBCEncrypter(block, iv)\n    mode.CryptBlocks(ciphertext[blockSize:], plaintext)\n\n    return ciphertext, nil\n&#125;\n\nfunc decrypt(key, ciphertext []byte) ([]byte, error) &#123;\n    block, err := des.NewTripleDESCipher(key)\n    if err != nil &#123;\n        return nil, err\n    &#125;\n\n    blockSize := block.BlockSize()\n\n    if len(ciphertext) &lt; blockSize &#123;\n        return nil, fmt.Errorf(&quot;ciphertext too short&quot;)\n    &#125;\n\n    iv := ciphertext[:blockSize]\n    ciphertext = ciphertext[blockSize:]\n\n    if len(ciphertext)%blockSize != 0 &#123;\n        return nil, fmt.Errorf(&quot;ciphertext is not a multiple of the block size&quot;)\n    &#125;\n\n    plaintext := make([]byte, len(ciphertext))\n\n    mode := cipher.NewCBCDecrypter(block, iv)\n    mode.CryptBlocks(plaintext, ciphertext)\n\n    return unpad(plaintext), nil\n&#125;\n1.4.5.3 go实现aes加密代码：\ngopackage main\n\nimport (\n    &quot;bytes&quot;\n    &quot;crypto/aes&quot;\n    &quot;crypto/cipher&quot;\n    &quot;crypto/rand&quot;\n    &quot;encoding/base64&quot;\n    &quot;fmt&quot;\n    &quot;io&quot;\n)\n\nfunc main() &#123;\n    key := []byte(&quot;0123456789abcdef&quot;)  // AES-128密钥，16字节\n    plaintext := []byte(&quot;Hello, AES!&quot;) // 明文数据\n\n    ciphertext, err := encrypt(key, plaintext)\n    if err != nil &#123;\n        fmt.Println(&quot;加密失败:&quot;, err)\n        return\n    &#125;\n\n    fmt.Printf(&quot;加密结果: %s\\n&quot;, base64.StdEncoding.EncodeToString(ciphertext))\n\n    decryptedText, err := decrypt(key, ciphertext)\n    if err != nil &#123;\n        fmt.Println(&quot;解密失败:&quot;, err)\n        return\n    &#125;\n\n    fmt.Printf(&quot;解密结果: %s\\n&quot;, decryptedText)\n&#125;\n\nfunc pad(data []byte, blockSize int) []byte &#123;\n    padding := blockSize - len(data)%blockSize\n    padText := bytes.Repeat([]byte&#123;byte(padding)&#125;, padding)\n    return append(data, padText...)\n&#125;\n\nfunc unpad(data []byte) []byte &#123;\n    padding := data[len(data)-1]\n    return data[:len(data)-int(padding)]\n&#125;\n\nfunc encrypt(key, plaintext []byte) ([]byte, error) &#123;\n    block, err := aes.NewCipher(key)\n    if err != nil &#123;\n        return nil, err\n    &#125;\n\n    blockSize := block.BlockSize()\n    plaintext = pad(plaintext, blockSize)\n\n    ciphertext := make([]byte, blockSize+len(plaintext))\n    iv := ciphertext[:blockSize]\n    if _, err := io.ReadFull(rand.Reader, iv); err != nil &#123;\n        return nil, err\n    &#125;\n\n    mode := cipher.NewCBCEncrypter(block, iv)\n    mode.CryptBlocks(ciphertext[blockSize:], plaintext)\n\n    return ciphertext, nil\n&#125;\n\nfunc decrypt(key, ciphertext []byte) ([]byte, error) &#123;\n    block, err := aes.NewCipher(key)\n    if err != nil &#123;\n        return nil, err\n    &#125;\n\n    blockSize := block.BlockSize()\n    if len(ciphertext) &lt; blockSize &#123;\n        return nil, fmt.Errorf(&quot;ciphertext too short&quot;)\n    &#125;\n\n    iv := ciphertext[:blockSize]\n    ciphertext = ciphertext[blockSize:]\n\n    if len(ciphertext)%blockSize != 0 &#123;\n        return nil, fmt.Errorf(&quot;ciphertext is not a multiple of the block size&quot;)\n    &#125;\n\n    plaintext := make([]byte, len(ciphertext))\n\n    mode := cipher.NewCBCDecrypter(block, iv)\n    mode.CryptBlocks(plaintext, ciphertext)\n\n    return unpad(plaintext), nil\n&#125;\n1.5 非对称加密1.5.1 特点\n加密和解密使用不同的密钥。\n加密速度较慢，适合处理少量数据。\n非对称加密算法通常较复杂。\n\n1.5.2 优点\n密钥的分发和管理相对较简单，无需事先共享密钥。\n提供了更高的安全性，可以用于身份验证和数字签名。\n\n1.5.3 缺点\n加密和解密的速度较慢，不适合处理大量数据。\n计算成本较高。\n\n1.5.4 场景分析\n非对称加密适用于需要较高安全性和身份验证的场景，如安全传输密钥、数字签名等。\n密钥的分发和管理相对较简单，适用于未建立安全信任关系的场景。\n\n1.5.5 常见的非对称加密方式\nRSA（Rivest, Shamir, Adleman）：基于大数因子分解的数学问题，广泛应用于安全通信和数字签名。\n\nECC（Elliptic Curve Cryptography）：基于椭圆曲线离散对数问题的加密算法，提供与RSA相当的安全性但计算成本较低。\nGo标准库中并没有提供直接的ECDSA加密和解密函数。ECDSA主要用于数字签名，而不是进行加密和解密操作。\n\n\n1.5.5.1 go实现rsa加密代码：\ngopackage main\n\nimport (\n    &quot;crypto/rand&quot;\n    &quot;crypto/rsa&quot;\n    &quot;crypto/x509&quot;\n    &quot;encoding/pem&quot;\n    &quot;fmt&quot;\n    &quot;os&quot;\n)\n\nfunc main() &#123;\n    // 生成RSA密钥对\n    privateKey, err := rsa.GenerateKey(rand.Reader, 2048)\n    if err != nil &#123;\n        fmt.Println(&quot;生成RSA密钥对失败:&quot;, err)\n        return\n    &#125;\n\n    // 保存私钥到文件\n    err = savePrivateKeyToFile(privateKey, &quot;private.pem&quot;)\n    if err != nil &#123;\n        fmt.Println(&quot;保存私钥到文件失败:&quot;, err)\n        return\n    &#125;\n    fmt.Println(&quot;私钥已保存到 private.pem 文件&quot;)\n\n    // 获取公钥\n    publicKey := &amp;privateKey.PublicKey\n\n    // 保存公钥到文件\n    err = savePublicKeyToFile(publicKey, &quot;public.pem&quot;)\n    if err != nil &#123;\n        fmt.Println(&quot;保存公钥到文件失败:&quot;, err)\n        return\n    &#125;\n    fmt.Println(&quot;公钥已保存到 public.pem 文件&quot;)\n\n    // 使用公钥加密\n    plaintext := []byte(&quot;Hello, RSA!&quot;)\n    ciphertext, err := encryptWithPublicKey(plaintext, publicKey)\n    if err != nil &#123;\n        fmt.Println(&quot;使用公钥加密失败:&quot;, err)\n        return\n    &#125;\n    fmt.Printf(&quot;加密结果: %x\\n&quot;, ciphertext)\n\n    // 使用私钥解密\n    decryptedText, err := decryptWithPrivateKey(ciphertext, privateKey)\n    if err != nil &#123;\n        fmt.Println(&quot;使用私钥解密失败:&quot;, err)\n        return\n    &#125;\n    fmt.Println(&quot;解密结果:&quot;, string(decryptedText))\n&#125;\n\n// 保存私钥到文件\nfunc savePrivateKeyToFile(privateKey *rsa.PrivateKey, filename string) error &#123;\n    privateKeyBytes := x509.MarshalPKCS1PrivateKey(privateKey)\n    privateKeyPEM := pem.EncodeToMemory(\n        &amp;pem.Block&#123;\n            Type:  &quot;RSA PRIVATE KEY&quot;,\n            Bytes: privateKeyBytes,\n        &#125;,\n    )\n\n    return os.WriteFile(filename, privateKeyPEM, 0600)\n&#125;\n\n// 保存公钥到文件\nfunc savePublicKeyToFile(publicKey *rsa.PublicKey, filename string) error &#123;\n    publicKeyBytes, err := x509.MarshalPKIXPublicKey(publicKey)\n    if err != nil &#123;\n        return err\n    &#125;\n\n    publicKeyPEM := pem.EncodeToMemory(\n        &amp;pem.Block&#123;\n            Type:  &quot;PUBLIC KEY&quot;,\n            Bytes: publicKeyBytes,\n        &#125;,\n    )\n\n    return os.WriteFile(filename, publicKeyPEM, 0644)\n&#125;\n\n// 使用公钥加密\nfunc encryptWithPublicKey(plaintext []byte, publicKey *rsa.PublicKey) ([]byte, error) &#123;\n    ciphertext, err := rsa.EncryptPKCS1v15(rand.Reader, publicKey, plaintext)\n    if err != nil &#123;\n        return nil, err\n    &#125;\n\n    return ciphertext, nil\n&#125;\n\n// 使用私钥解密\nfunc decryptWithPrivateKey(ciphertext []byte, privateKey *rsa.PrivateKey) ([]byte, error) &#123;\n    plaintext, err := rsa.DecryptPKCS1v15(rand.Reader, privateKey, ciphertext)\n    if err != nil &#123;\n        return nil, err\n    &#125;\n\n    return plaintext, nil\n&#125;\nprivateKeyBytes :&#x3D; x509.MarshalPKCS1PrivateKey(privateKey) 这行代码的作用是将给定的RSA私钥（*rsa.PrivateKey类型）转换为PKCS1格式的字节数组。\nPKCS1是一个公钥密码标准，定义了一种在RSA加密算法中使用的密钥格式。它定义了如何在二进制形式和ASCII码形式之间进行RSA密钥的编码和解码，以及如何对密钥进行加密和解密。\n在Go语言中，可以使用x509标准库的MarshalPKCS1PrivateKey函数将RSA私钥转换为PKCS1格式的字节数组。这个函数接受一个*rsa.PrivateKey类型的私钥作为参数，并返回一个字节数组表示该私钥的PKCS1编码。\npublicKeyBytes, err :&#x3D; x509.MarshalPKIXPublicKey(publicKey) 这行代码的作用是将给定的RSA公钥（*rsa.PublicKey类型）转换为PKIX格式的字节数组。\nPKIX是一种公钥基础设施标准，定义了证书和公钥的格式和交互方式。它提供了一种通用的公钥证书结构，适用于多种密码算法和应用场景。\n在Go语言中，可以使用x509标准库的MarshalPKIXPublicKey函数将RSA公钥转换为PKIX格式的字节数组。这个函数接受一个*rsa.PublicKey类型的公钥作为参数，并返回一个字节数组表示该公钥的PKIX编码。\n1.6 数字签名1.6.1 数字签名介绍数字签名是一种用于验证数据完整性和身份认证的技术。它基于公钥密码学的原理，通过使用私钥对数据进行加密生成签名，然后使用对应的公钥对签名进行解密验证。\n数字签名的过程如下：\n\n数据发送者使用私钥对要签名的数据进行加密运算，生成签名。\n数据发送者将签名与原始数据一起发送给接收者。\n数据接收者使用相同的公钥对签名进行解密运算，得到原始数据的哈希值。\n数据接收者计算接收到的原始数据的哈希值。\n数据接收者将两个哈希值进行比较，如果相等，则说明数据完整且未被篡改，并可以确认发送者的身份。\n\n通过数字签名，接收者可以验证数据的完整性，并确保数据来自于拥有私钥的发送者。即使在数据传输过程中被篡改，也能通过验证步骤的不匹配来检测到数据的篡改。\n数字签名在信息安全领域有广泛的应用，例如：\n\n网络通信：用于保护数据传输的完整性和身份认证。\n数字证书：用于认证网站和服务的可信度。\n文件完整性验证：用于验证文件在传输或存储过程中是否被修改。\n数字版权保护：用于验证数字内容的真实性和完整性。\n\n1.6.2 go实现数字签名\n私钥签名\n公钥验证签名\n\n1.6.2.1 go实现rsa数字签名代码：\ngopackage main\n\nimport (\n    &quot;crypto&quot;\n    &quot;crypto/rand&quot;\n    &quot;crypto/rsa&quot;\n    &quot;crypto/sha256&quot;\n    &quot;crypto/x509&quot;\n    &quot;encoding/hex&quot;\n    &quot;encoding/pem&quot;\n    &quot;errors&quot;\n    &quot;fmt&quot;\n    &quot;os&quot;\n)\n\nfunc main() &#123;\n    // 生成RSA密钥对\n    privateKey, publicKey, err := generateRSAKeyPair(2048)\n    if err != nil &#123;\n        fmt.Println(&quot;生成RSA密钥对失败:&quot;, err)\n        return\n    &#125;\n\n    // 保存私钥到文件\n    err = savePrivateKeyToFile(privateKey, &quot;private.pem&quot;)\n    if err != nil &#123;\n        fmt.Println(&quot;保存私钥到文件失败:&quot;, err)\n        return\n    &#125;\n    fmt.Println(&quot;私钥已保存到 private.pem 文件&quot;)\n\n    // 保存公钥到文件\n    err = savePublicKeyToFile(publicKey, &quot;public.pem&quot;)\n    if err != nil &#123;\n        fmt.Println(&quot;保存公钥到文件失败:&quot;, err)\n        return\n    &#125;\n    fmt.Println(&quot;公钥已保存到 public.pem 文件&quot;)\n\n    // 要签名的数据\n    message := &quot;Hello, RSA!&quot;\n\n    // 计算消息的散列值\n    hashed := sha256.Sum256([]byte(message))\n\n    // 使用私钥进行签名\n    signature, err := signWithPrivateKey(hashed[:], privateKey)\n    if err != nil &#123;\n        fmt.Println(&quot;签名失败:&quot;, err)\n        return\n    &#125;\n\n    // 将签名的结果转换为16进制字符串\n    signatureHex := hex.EncodeToString(signature)\n    fmt.Printf(&quot;签名结果: %s\\n&quot;, signatureHex)\n\n    // 使用公钥进行验证\n    err = verifyWithPublicKey(hashed[:], signature, publicKey)\n    if err != nil &#123;\n        fmt.Println(&quot;验证签名失败:&quot;, err)\n        return\n    &#125;\n    fmt.Println(&quot;签名验证通过&quot;)\n&#125;\n\n// 生成RSA密钥对\nfunc generateRSAKeyPair(keySize int) (*rsa.PrivateKey, *rsa.PublicKey, error) &#123;\n    privateKey, err := rsa.GenerateKey(rand.Reader, keySize)\n    if err != nil &#123;\n        return nil, nil, err\n    &#125;\n\n    publicKey := &amp;privateKey.PublicKey\n\n    return privateKey, publicKey, nil\n&#125;\n\n// 保存私钥到文件\nfunc savePrivateKeyToFile(privateKey *rsa.PrivateKey, filename string) error &#123;\n    privateKeyBytes := x509.MarshalPKCS1PrivateKey(privateKey)\n    privateKeyPEM := pem.EncodeToMemory(\n        &amp;pem.Block&#123;\n            Type:  &quot;RSA PRIVATE KEY&quot;,\n            Bytes: privateKeyBytes,\n        &#125;,\n    )\n\n    return os.WriteFile(filename, privateKeyPEM, 0600)\n&#125;\n\n// 保存公钥到文件\nfunc savePublicKeyToFile(publicKey *rsa.PublicKey, filename string) error &#123;\n    publicKeyBytes, err := x509.MarshalPKIXPublicKey(publicKey)\n    if err != nil &#123;\n        return err\n    &#125;\n\n    publicKeyPEM := pem.EncodeToMemory(\n        &amp;pem.Block&#123;\n            Type:  &quot;PUBLIC KEY&quot;,\n            Bytes: publicKeyBytes,\n        &#125;,\n    )\n\n    return os.WriteFile(filename, publicKeyPEM, 0644)\n&#125;\n\n// 使用私钥进行签名\nfunc signWithPrivateKey(message []byte, privateKey *rsa.PrivateKey) ([]byte, error) &#123;\n    signature, err := rsa.SignPKCS1v15(rand.Reader, privateKey, crypto.SHA256, message)\n    if err != nil &#123;\n        return nil, err\n    &#125;\n\n    return signature, nil\n&#125;\n\n// 使用公钥进行验证\nfunc verifyWithPublicKey(message, signature []byte, publicKey *rsa.PublicKey) error &#123;\n    err := rsa.VerifyPKCS1v15(publicKey, crypto.SHA256, message, signature)\n    if err != nil &#123;\n        return errors.New(&quot;签名验证失败&quot;)\n    &#125;\n\n    return nil\n&#125;\n1.6.2.2 go实现ecc数字签名代码：\ngopackage main\n\nimport (\n    &quot;crypto/ecdsa&quot;\n    &quot;crypto/elliptic&quot;\n    &quot;crypto/rand&quot;\n    &quot;crypto/sha256&quot;\n    &quot;crypto/x509&quot;\n    &quot;encoding/asn1&quot;\n    &quot;encoding/hex&quot;\n    &quot;encoding/pem&quot;\n    &quot;errors&quot;\n    &quot;fmt&quot;\n    &quot;math/big&quot;\n    &quot;os&quot;\n)\n\nfunc main() &#123;\n    // 生成公钥和私钥\n    privateKey, err := ecdsa.GenerateKey(elliptic.P256(), rand.Reader)\n    if err != nil &#123;\n        fmt.Println(&quot;生成私钥失败：&quot;, err)\n        return\n    &#125;\n\n    // 保存私钥到文件\n    err = savePrivateKeyToFile(privateKey, &quot;private.pem&quot;)\n    if err != nil &#123;\n        fmt.Println(&quot;保存私钥到文件失败：&quot;, err)\n        return\n    &#125;\n    fmt.Println(&quot;私钥已保存到 private.pem 文件&quot;)\n\n    // 保存公钥到文件\n    publicKey := &amp;privateKey.PublicKey\n    err = savePublicKeyToFile(publicKey, &quot;public.pem&quot;)\n    if err != nil &#123;\n        fmt.Println(&quot;保存公钥到文件失败：&quot;, err)\n        return\n    &#125;\n    fmt.Println(&quot;公钥已保存到 public.pem 文件&quot;)\n\n    // 要签名的数据\n    message := &quot;Hello, ECC!&quot;\n\n    // 计算消息的散列值\n    hashed := sha256.Sum256([]byte(message))\n\n    // 使用私钥进行签名\n    signature, err := signWithPrivateKey(hashed[:], privateKey)\n    if err != nil &#123;\n        fmt.Println(&quot;签名失败：&quot;, err)\n        return\n    &#125;\n\n    // 将签名的结果转换为16进制字符串\n    signatureHex := hex.EncodeToString(signature)\n    fmt.Printf(&quot;签名结果： %s\\n&quot;, signatureHex)\n\n    // 使用公钥进行验证\n    err = verifyWithPublicKey(hashed[:], signature, publicKey)\n    if err != nil &#123;\n        fmt.Println(&quot;验证签名失败：&quot;, err)\n        return\n    &#125;\n    fmt.Println(&quot;签名验证通过&quot;)\n&#125;\n\n// 保存私钥到文件\nfunc savePrivateKeyToFile(privateKey *ecdsa.PrivateKey, filename string) error &#123;\n    privateKeyBytes, err := x509.MarshalECPrivateKey(privateKey)\n    if err != nil &#123;\n        return err\n    &#125;\n\n    privateKeyPEM := pem.EncodeToMemory(&amp;pem.Block&#123;\n        Type:  &quot;EC PRIVATE KEY&quot;,\n        Bytes: privateKeyBytes,\n    &#125;)\n\n    return os.WriteFile(filename, privateKeyPEM, 0600)\n&#125;\n\n// 保存公钥到文件\nfunc savePublicKeyToFile(publicKey *ecdsa.PublicKey, filename string) error &#123;\n    publicKeyBytes, err := x509.MarshalPKIXPublicKey(publicKey)\n    if err != nil &#123;\n        return err\n    &#125;\n\n    publicKeyPEM := pem.EncodeToMemory(&amp;pem.Block&#123;\n        Type:  &quot;PUBLIC KEY&quot;,\n        Bytes: publicKeyBytes,\n    &#125;)\n\n    return os.WriteFile(filename, publicKeyPEM, 0644)\n&#125;\n\n// 使用私钥进行签名\nfunc signWithPrivateKey(message []byte, privateKey *ecdsa.PrivateKey) ([]byte, error) &#123;\n    r, s, err := ecdsa.Sign(rand.Reader, privateKey, message)\n    if err != nil &#123;\n        return nil, err\n    &#125;\n\n    signature, err := asn1.Marshal(ecdsaSignature&#123;r, s&#125;)\n    if err != nil &#123;\n        return nil, err\n    &#125;\n\n    return signature, nil\n&#125;\n\n// 使用公钥进行验证\nfunc verifyWithPublicKey(message, signature []byte, publicKey *ecdsa.PublicKey) error &#123;\n    var sig ecdsaSignature\n    _, err := asn1.Unmarshal(signature, &amp;sig)\n    if err != nil &#123;\n        return err\n    &#125;\n\n    if !ecdsa.Verify(publicKey, message, sig.R, sig.S) &#123;\n        return errors.New(&quot;签名验证失败&quot;)\n    &#125;\n\n    return nil\n&#125;\n\ntype ecdsaSignature struct &#123;\n    R, S *big.Int\n&#125;\nArticle link： https://tqgoblin.site/post/csdn/go语言--区块链学习（一）/  Author： Stephen  \n","slug":"csdn/go语言--区块链学习（一）","date":"2022-01-19T12:21:10.000Z","categories_index":"区块链学习","tags_index":"golang 区块链 密码学","author_index":"Stephen"},{"id":"df4a2b3e83ff0d1bcb88dc96e346c071","title":"Tomcat组成与工作原理","content":"什么是tomcat开源的 Java Web 应用服务器，实现了 Java EE(Java Platform Enterprise Edition)的部 分技术规范，比如 Java Servlet、Java Server Page、JSTL、Java WebSocket。Java EE 是 Sun 公 司为企业级应用推出的标准平台，定义了一系列用于企业级开发的技术规范，除了上述的之外，还有 EJB、Java Mail、JPA、JTA、JMS 等，而这些都依赖具体容器的实现。\n\n上图对比了 Java EE 容器的实现情况，Tomcat 和 Jetty 都只提供了 Java Web 容器必需的 Servlet 和 JSP 规范，开发者要想实现其他的功能，需要自己依赖其他开源实现。\nGlassfish 是由 sun 公司推出，Java EE 最新规范出来之后，首先会在 Glassfish 上进行实 现，所以是研究 Java EE 最新技术的首选。\n最常见的情况是使用 Tomcat 作为 Java Web 服务器，使用 Spring 提供的开箱即用的强大的功能，并依赖其他开源库来完成负责的业务功能实现。\nServlet容器Tomcat 组成如下图：主要有 Container 和 Connector 以及相关组件构成。\n\nServer：指的就是整个 Tomcat 服 务器，包含多组服务，负责管理和 启动各个 Service，同时监听 8005 端口发过来的 shutdown 命令，用 于关闭整个容器 ；\nService：Tomcat 封装的、对外提 供完整的、基于组件的 web 服务， 包含 Connectors、Container 两个 核心组件，以及多个功能组件，各 个 Service 之间是独立的，但是共享 同一 JVM 的资源 ；\nConnector：Tomcat 与外部世界的连接器，监听固定端口接收外部请求，传递给 Container，并 将 Container 处理的结果返回给外部；\nContainer：Catalina，Servlet 容器，内部有多层容器组成，用于管理 Servlet 生命周期，调用 servlet 相关方法。\nLoader：封装了 Java ClassLoader，用于 Container 加载类文件；Realm：\nTomcat 中为 web 应用程序提供访问认证和角色管理的机制；\nJMX：Java SE 中定义技术规范，是一个为应用程序、设备、系统等植入管理功能的框架，通过 JMX 可以远程监控 Tomcat 的运行状态；\nJasper：Tomcat 的 Jsp 解析引擎，用于将 Jsp 转换成 Java 文件，并编译成 class 文件。Session：负责管理和创建 session，以及 Session 的持久化(可自定义)，支持 session 的集群。\nPipeline：在容器中充当管道的作用，管道中可以设置各种 valve(阀门)，请求和响应在经由管 道中各个阀门处理，提供了一种灵活可配置的处理请求和响应的机制\nNaming：命名服务，JNDI， Java 命名和目录接口，是一组在 Java 应用中访问命名和目录服务的 API。命名服务将名称和对象联系起来，使得我们可以用名称访问对象，目录服务也是一种命名 服务，对象不但有名称，还有属性。Tomcat 中可以使用 JNDI 定义数据源、配置信息，用于开发 与部署的分离。\nContainer组成\n\nEngine：Servlet 的顶层容器，包含一 个或多个 Host 子容器；\nHost：虚拟主机，负责 web 应用的部 署和 Context 的创建；\nContext：Web 应用上下文，包含多个 Wrapper，负责 web 配置的解析、管 理所有的 Web 资源；\nWrapper：最底层的容器，是对 Servlet 的封装，负责 Servlet 实例的创 建、执行和销毁。\n生命周期管理\nTomcat 为了方便管理组件和容器的生命周期，定义了从创建、启动、到停止、销毁共 12 中状态，tomcat 生命周期管理了内部状态变化的规则控制，组件和容器只需实现相应的生命周期 方法即可完成各生命周期内的操作(initInternal、startInternal、stopInternal、 destroyInternal)；\n比如执行初始化操作时，会判断当前状态是否 New，如果不是则抛出生命周期异常；是的 话则设置当前状态为 Initializing，并执行 initInternal 方法，由子类实现，方法执行成功则设置当 前状态为 Initialized，执行失败则设置为 Failed 状态；\n\nTomcat 的生命周期管理引入了事件机制，在组件或容器的生命周期状态发生变化时会通 知事件监听器，监听器通过判断事件的类型来进行相应的操作。\n事件监听器的添加可以在 server.xml 文件中进行配置;上班摸鱼神器：https://www.yoodb.com/slack-off/home.html\nTomcat 各类容器的配置过程就是通过添加 listener 的方式来进行的，从而达到配置逻辑与 容器的解耦。如 EngineConfig、HostConfig、ContextConfig。\nEngineConfig：主要打印启动和停止日志\nHostConfig：主要处理部署应用，解析应用 META-INF&#x2F;context.xml 并创建应用的 Context ContextConfig：主要解析并合并 web.xml，扫描应用的各类 web 资源 (filter、servlet、listener)\n\nTomcat 的启动过程\n\n启动从 Tomcat 提供的 start.sh 脚本开始，shell 脚本会调用 Bootstrap 的 main 方法，实际 调用了 Catalina 相应的 load、start 方法。\nload 方法会通过 Digester 进行 config&#x2F;server.xml 的解析，在解析的过程中会根据 xml 中的关系 和配置信息来创建容器，并设置相关的属性。接着 Catalina 会调用 StandardServer 的 init 和 start 方法进行容器的初始化和启动。\n按照 xml 的配置关系，server 的子元素是 service，service 的子元素是顶层容器 Engine，每层容器有持有自己的子容器，而这些元素都实现了生命周期管理 的各个方法，因此就很容易的完成整个容器的启动、关闭等生命周期的管理。\nStandardServer 完成 init 和 start 方法调用后，会一直监听来自 8005 端口(可配置)，如果接收 到 shutdown 命令，则会退出循环监听，执行后续的 stop 和 destroy 方法，完成 Tomcat 容器的 关闭。同时也会调用 JVM 的 Runtime.getRuntime()﴿.addShutdownHook 方法，在虚拟机意外退 出的时候来关闭容器。\n所有容器都是继承自 ContainerBase，基类中封装了容器中的重复工作，负责启动容器相关的组 件 Loader、Logger、Manager、Cluster、Pipeline，启动子容器(线程池并发启动子容器，通过 线程池 submit 多个线程，调用后返回 Future 对象，线程内部启动子容器，接着调用 Future 对象 的 get 方法来等待执行结果)。\njavaList&lt;Future&lt;Void&gt;&gt; results = new ArrayList&lt;Future&lt;Void&gt;&gt;();\nfor (int i = 0; i &lt; children.length; i++) &#123;\n    results.add(startStopExecutor.submit(new StartChild(children[i])));\n&#125;\nboolean fail = false;\nfor (Future&lt;Void&gt; result ：results) &#123;\n    try &#123;\n        result.get();\n    &#125; catch (Exception e) &#123;\n        log.error(sm.getString(&quot;containerBase.threadedStartFailed&quot;)， e);\n        fail = true;\n    &#125;\n&#125;Web 应用的部署方式\n注：catalina.home：安装目录;catalina.base：工作目录;默认值 user.dir\n\nServer.xml 配置 Host 元素，指定 appBase 属性，默认$catalina.base&#x2F;webapps&#x2F;\n\nServer.xml 配置 Context 元素，指定 docBase，元素，指定 web 应用的路径\n\n自定义配置：在$catalina.base&#x2F;EngineName&#x2F;HostName&#x2F;XXX.xml 配置 Context 元素\n\n\nHostConfig 监听了 StandardHost 容器的事件，在 start 方法中解析上述配置文件：\n\n扫描 appbase 路径下的所有文件夹和 war 包，解析各个应用的 META-INF&#x2F;context.xml，并 创建 StandardContext，并将 Context 加入到 Host 的子容器中。\n\n解析$catalina.base&#x2F;EngineName&#x2F;HostName&#x2F;下的所有 Context 配置，找到相应 web 应 用的位置，解析各个应用的 META-INF&#x2F;context.xml，并创建 StandardContext，并将 Context 加入到 Host 的子容器中。\n\n\n注：\n\nHostConfig 并没有实际解析 Context.xml，而是在 ContextConfig 中进行的。\n\nHostConfig 中会定期检查 watched 资源文件(context.xml 配置文件)\n\n\nContextConfig 解析 context.xml 顺序：\n\n先解析全局的配置 config&#x2F;context.xml\n\n然后解析 Host 的默认配置 EngineName&#x2F;HostName&#x2F;context.xml.default\n\n最后解析应用的 META-INF&#x2F;context.xml\n\n\nContextConfig 解析 web.xml 顺序：\n\n先解析全局的配置 config&#x2F;web.xml\n\n然后解析 Host 的默认配置 EngineName&#x2F;HostName&#x2F;web.xml.default 接着解析应用的 MEB-INF&#x2F;web.xml\n\n扫描应用 WEB-INF&#x2F;lib&#x2F;下的 jar 文件，解析其中的 META-INF&#x2F;web-fragment.xml 最后合并 xml 封装成 WebXml，并设置 Context\n\n\n注：\n\n扫描 web 应用和 jar 中的注解(Filter、Listener、Servlet)就是上述步骤中进行的。\n\n容器的定期执行：backgroundProcess，由 ContainerBase 来实现的，并且只有在顶层容器 中才会开启线程。(backgroundProcessorDelay&#x3D;10 标志位来控制)\n\n\nServlet生命周期\nServlet 是用 Java 编写的服务器端程序。其主要功能在于交互式地浏览和修改数据，生成动态 Web 内容。\n\n请求到达 server 端，server 根据 url 映射到相应的 Servlet\n\n判断 Servlet 实例是否存在，不存在则加载和实例化 Servlet 并调用 init 方法\n\nServer 分别创建 Request 和 Response 对象，调用 Servlet 实例的 service 方法(service 方法 内部会根据 http 请求方法类型调用相应的 doXXX 方法)\n\ndoXXX 方法内为业务逻辑实现，从 Request 对象获取请求参数，处理完毕之后将结果通过 response 对象返回给调用方\n\n当 Server 不再需要 Servlet 时(一般当 Server 关闭时)，Server 调用 Servlet 的 destroy() 方 法。\n\n\nload on startup\n当值为 0 或者大于 0 时，表示容器在应用启动时就加载这个 servlet; 当是一个负数时或者没有指定时，则指示容器在该 servlet 被选择时才加载; 正数的值越小，启动该 servlet 的优先级越高;\nsingle thread model\n每次访问 servlet，新建 servlet 实体对象，但并不能保证线程安全，同时 tomcat 会限制 servlet 的实例数目最佳实践：不要使用该模型，servlet 中不要有全局变量\n请求处理过程  \n\n\n根据 server.xml 配置的指定的 connector 以及端口监听 http、或者 ajp 请求\n\n请求到来时建立连接,解析请求参数,创建 Request 和 Response 对象,调用顶层容器 pipeline 的 invoke 方法\n\n容器之间层层调用,最终调用业务 servlet 的 service 方法\n\nConnector 将 response 流中的数据写到 socket 中\n\n\nPipeline 与 Valve\n\nPipeline 可以理解为现实中的管道,Valve 为管道中的阀门,Request 和 Response 对象在管道中 经过各个阀门的处理和控制。\n每个容器的管道中都有一个必不可少的 basic valve,其他的都是可选的,basic valve 在管道中最 后调用,同时负责调用子容器的第一个 valve。\nValve 中主要的三个方法:setNext、getNext、invoke;valve 之间的关系是单向链式结构,本身 invoke 方法中会调用下一个 valve 的 invoke 方法。\n各层容器对应的 basic valve 分别是 StandardEngineValve、StandardHostValve、 StandardContextValve、StandardWrapperValve。\nJSP引擎\n\nJSP 生命周期\n\n编译阶段:servlet 容器编译 servlet 源文。上班摸鱼神器：https://www.yoodb.com/slack-off/home.html\n\n件,生成 servlet 类\n\n初始化阶段:加载与 JSP 对应的 servlet 类, 创建其实例,并调用它的初始化方法\n\n执行阶段:调用与 JSP 对应的 servlet 实例的 服务方法\n\n销毁阶段:调用与 JSP 对应的 servlet 实例的 销毁方法,然后销毁 servlet 实例\n\n\nJSP元素\n\n代码片段：&lt;% 代码片段 %&gt;\n\nJSP声明：&lt;%! declaration; [ declaration; ]+ … %&gt;\n\nJSP表达式：&lt;%&#x3D; 表达式 %&gt;\n\nJSP注释：&lt;%– 注释 --%&gt;\n\nJSP指令：    &lt;%@ directive attribute&#x3D;“value” %&gt;\n\nJSP行为：    &lt;jsp:action_name attribute&#x3D;“value” &#x2F;&gt;\n\nHTML元素：html&#x2F;head&#x2F;body&#x2F;div&#x2F;p&#x2F;…\n\nJSP隐式对象：request、response、out、session、application、config、\n\npageContext、page、Exception\n\n\nJSP 元素说明\n\n代码片段:包含任意量的 Java 语句、变量、方法或表达式;\n\nJSP 声明:一个声明语句可以声明一个或多个变量、方法,供后面的 Java 代码使用;\n\nJSP 表达式:输出 Java 表达式的值,String 形式;\n\nJSP 注释:为代码作注释以及将某段代码注释掉\n\nJSP 指令:用来设置与整个 JSP 页面相关的属性,\n\n&lt;%@ page … %&gt;定义页面的依赖属性,比如 language、contentType、errorPage、 isErrorPage、import、isThreadSafe、session 等等\n\n&lt;%@ include … %&gt;包含其他的 JSP 文件、HTML 文件或文本文件,是该 JSP 文件的一部分,会 被同时编译执行\n\n&lt;%@ taglib … %&gt;引入标签库的定义,可以是自定义标签\n\nJSP 行为:jsp:include、jsp:useBean、jsp:setProperty、jsp:getProperty、jsp:forward\n\n\nJsp 解析过程\n\n\n代码片段:在_jspService()方法内直接输出\n\nJSP 声明: 在 servlet 类中进行输出\n\nJSP 表达式:在_jspService()方法内直接输出\n\nJSP 注释:直接忽略,不输出\n\nJSP 指令:根据不同指令进行区分,include:对引入的文件进行解析;page 相关的属性会做为 JSP 的属性,影响的是解析和请求处理时的行为\n\nJSP 行为:不同的行为有不同的处理方式,jsp:useBean 为例,会从 pageContext 根据 scope 的 类别获取 bean 对象,如果没有会创建 bean,同时存到相应 scope 的 pageContext 中\n\nHTML:在_jspService()方法内直接输出\n\nJSP 隐式对象:在_jspService()方法会进行声明,只能在方法中使用;\n\n\nConnector\n\nHttp:HTTP 是超文本传输协议,是客户端浏览器或其他程序与 Web 服务器之间的应用层通信协议\nAJP:Apache JServ 协议(AJP)是一种二进制协议,专门代理从 Web 服务器到位于后端的应用 程序服务器的入站请求\n阻塞 IO\n\n非阻塞 IO\n\nIO多路复用\n\n阻塞与非阻塞的区别在于进行读操作和写操作的系统调用时，如果此时内核态没有数据可读或者没有缓冲空间可写时，是否阻塞。\nIO多路复用的好处在于可同时监听多个socket的可读和可写事件，这样就能使得应用可以同时监听多个socket，释放了应用线程资源。\nTomcat各类Connector对比\nConnector的实现模式有三种，分别是BIO、NIO、APR，可以在server.xml中指定。\n\nJIO：用java.io编写的TCP模块，阻塞IO\n\nNIO：用java.nio编写的TCP模块，非阻塞IO，（IO多路复用）\n\nAPR：全称Apache Portable Runtime，使用JNI的方式来进行读取文件以及进行网络传输\n\n\nApache Portable Runtime是一个高度可移植的库，它是Apache HTTP Server 2.x的核心。APR具有许多用途，包括访问高级IO功能（如sendfile，epoll和OpenSSL），操作系统级功能（随机数生成，系统状态等）和本地进程处理（共享内存，NT管道和Unix套接字）。\n表格中字段含义说明：\n\nSupport Polling：是否支持基于IO多路复用的socket事件轮询\n\nPolling Size：轮询的最大连接数\n\nWait for next Request：在等待下一个请求时，处理线程是否释放，BIO是没有释放的，所以在keep-alive&#x3D;true的情况下处理的并发连接数有限\n\nRead Request Headers：由于request header数据较少，可以由容器提前解析完毕，不需要阻塞\n\nRead Request Body：读取request body的数据是应用业务逻辑的事情，同时Servlet的限制，是需要阻塞读取的\n\nWrite Response：跟读取request body的逻辑类似，同样需要阻塞写\n\n\nNIO处理相关类\n\nAcceptor线程负责接收连接，调用accept方法阻塞接收建立的连接，并对socket进行封装成PollerEvent，指定注册的事件为op_read，并放入到EventQueue队列中，PollerEvent的run方法逻辑的是将Selector注册到socket的指定事件；\nPoller线程从EventQueue获取PollerEvent，并执行PollerEvent的run方法，调用Selector的select方法，如果有可读的Socket则创建Http11NioProcessor，放入到线程池中执行；\nCoyoteAdapter是Connector到Container的适配器，Http11NioProcessor调用其提供的service方法，内部创建Request和Response对象，并调用最顶层容器的Pipeline中的第一个Valve的invoke方法\nMapper主要处理http url 到servlet的映射规则的解析，对外提供map方法\nNIO Connector主要参数\n\nComet\nComet是一种用于web的推送技术，能使服务器实时地将更新的信息传送到客户端，而无须客户端发出请求\n在WebSocket出来之前，如果不使用comet，只能通过浏览器端轮询Server来模拟实现服务器端推送。\nComet支持servlet异步处理IO，当连接上数据可读时触发事件，并异步写数据(阻塞)\n\nTomcat要实现Comet，只需继承HttpServlet同时，实现CometProcessor接口\n\nBegin：新的请求连接接入调用，可进行与Request和Response相关的对象初始化操作，并保存response对象，用于后续写入数据\n\nRead：请求连接有数据可读时调用\n\nEnd：当数据可用时，如果读取到文件结束或者response被关闭时则被调用\n\nError：在连接上发生异常时调用，数据读取异常、连接断开、处理异常、socket超时\n\n\nNote：\n\nRead：在post请求有数据，但在begin事件中没有处理，则会调用read，如果read没有读取数据，在会触发Error回调，关闭socket\n\nEnd：当socket超时，并且response被关闭时也会调用；server被关闭时调用\n\nError：除了socket超时不会关闭socket，其他都会关闭socket\n\nEnd和Error时间触发时应关闭当前comet会话，即调用CometEvent的close方法\n\nNote：在事件触发时要做好线程安全的操作\n\n\n异步Servlet\n\n传统流程：\n\n首先，Servlet 接收到请求之后，request数据解析；\n\n接着，调用业务接口的某些方法，以完成业务处理；\n\n最后，根据处理的结果提交响应，Servlet 线程结束\n\n\n异步处理流程：\n\n客户端发送一个请求\n\nServlet容器分配一个线程来处理容器中的一个servlet\n\nservlet调用request.startAsync()，保存AsyncContext, 然后返回\n\n任何方式存在的容器线程都将退出，但是response仍然保持开放\n\n业务线程使用保存的AsyncContext来完成响应（线程池）\n\n客户端收到响应\n\n\nServlet 线程将请求转交给一个异步线程来执行业务处理，线程本身返回至容器，此时 Servlet 还没有生成响应数据，异步线程处理完业务以后，可以直接生成响应数据（异步线程拥有 ServletRequest 和 ServletResponse 对象的引用）\n为什么web应用中支持异步？\n推出异步，主要是针对那些比较耗时的请求：比如一次缓慢的数据库查询，一次外部REST API调用, 或者是其他一些I&#x2F;O密集型操作。这种耗时的请求会很快的耗光Servlet容器的线程池，继而影响可扩展性。\nNote：从客户端的角度来看，request仍然像任何其他的HTTP的request-response交互一样，只是耗费了更长的时间而已\n异步事件监听\n\nonStartAsync：Request调用startAsync方法时触发\n\nonComplete：syncContext调用complete方法时触发\n\nonError：处理请求的过程出现异常时触发\n\nonTimeout：socket超时触发\n\n\nNote :\nonError&#x2F; onTimeout触发后，会紧接着回调onComplete\nonComplete 执行后，就不可再操作request和response\nArticle link： https://tqgoblin.site/post/csdn/Tomcat组成与工作原理/  Author： Stephen  \n","slug":"csdn/Tomcat组成与工作原理","date":"2021-10-10T14:43:18.000Z","categories_index":"面试","tags_index":"tomcat java","author_index":"Stephen"},{"id":"401056d31ba23a246721cb99d93af09c","title":"Golang基础入门","content":"Golang基础入门0.1 简介本篇教程适用于对Golang感兴趣的初学者，也适用于对Golang基础知识点不太熟练的初学者，也适用于对于Golang基础知识点查缺补漏的初学者，此文档知识内容丰富，收录了较为详细的Golang常用知识点，通过本篇文档，希望对大家Golang的基础学习有所帮助（如果有重要的知识点，博主未列出，请在下方评论处指出，谢谢！）。\n（博主只是一个Golang的初学者之一，在此，仅希望各位初学者们在一开始学习Golang的时候，少走一些弯路，如果大家在阅读的过程中发现有很明显的错误或者说还有很多不足之处，欢迎大家指出，博主很乐意接受大家的建议，因为这样也可以帮助到博主自己，在此先提前谢谢大家。）\n0.2 先决条件\n一些编程经验。Golang的代码十分简洁、美观，这使得上手具有一定的难度，需要您有一定的编程经验，如：C&#x2F;C++，Java，Python等，如果没有的话，也没关系，博主也会尽量用通俗易懂的语言来介绍。\n用于编辑代码的工具。大多数文本编辑器都对 Go 有很好的支持，都可以用于敲golang的代码，包括记事本。其中最受欢迎的是 VSCode（免费）、GoLand（付费）和Vim（免费）。（GoLand、IDEA等JetBrains全家桶系列，学生党可以白嫖）\n会使用命令终端。如Windows 中的 PowerShell 或 cmd。\n\n0.3 参考文献Documentation - The Go Programming Language (google.cn)\nRelease History - The Go Programming Language (google.cn)\nGo语言中文文档-地鼠文档 (topgoer.cn)\nGo专家编程-地鼠文档 (topgoer.cn)\nGin教程_Golang+Gin框架入门实战教程-百度网盘下载地址 (itying.com)\nGo语言、golang权威学习教程 (magedu.com)\nGo语言：环境变量GOPROXY和GO111MODULE设置_猪哥-嵌入式的博客-CSDN博客\n\n模块一：初识Go1.1 什么是 Golang？1.1.1 Go发展历史Go的核心开发团队\nKen Thompson\n\n1966年：加入了贝尔实验室，在参与 Multics （多路信息计算）开发期间，创造出了B语言，并用一个月的时间用B语言开发了全新的操作系统UNICS，后来改名为我们所熟悉的UNIX 操作系统。\n1971年：和丹尼斯·利奇（Dennis Ritchie）一起共同发明了C语言。\n1973年：和丹尼斯·利奇（Dennis Ritchie）使用C语言重写了UNIX，并安装于PDP-11的机器之上。\n1983年：美国计算机协会将图灵奖授予汤普森。\n2000年：离开贝尔实验室，已退休的汤普森成为了一名飞行员。\n2006年：加入Google工作。\n2007年：64岁的高龄，与Rob Pike和Robert Griesemer主导了Go的开发。\n\nRob Pike\n\nGo项目总负责人。\n贝尔实验室Unix团队成员，参与的项目包括Plan 9，Inferno操作系统和Limbo编程语言。\nUTF-8字符集规范唯二的发明人之一（另一位是Ken Thompson）。\n《UNIX环境编程》和《程序设计实践》这两本书的作者之一。\n第22届莫斯科夏季奥运会射箭项目的银牌得主。\n业余天文学家，设计的珈玛射线望远镜差点被 NASA 用在航天飞机上。\n他的媳妇Renee French 就是 Go 语言吉祥物的设计人。\n\nRobert Griesemer\n\n参与V8 JavaScript引擎的开发。\n参与Java HotSpot虚拟机的研发。\n\n起源\n2007年，Google的几位大牛正在用C++开发一些比较繁琐但是核心的工作，主要是分布式集群，大牛觉得很闹心。此时C++委员会来他们公司做技术演讲，说C++将要添加35个新特性，大牛心里飘过一万个CNM，“C++特性还不够多吗”。于是Rob Pike说要不自己搞个简单一点的语言吧，首先名字得简单好记，大腿一拍就叫“go”。把事情搞复杂很容易，把事情搞简单才更深刻。\n发展\n\n2007年9月21日，开始雏形设计。\n2009年10月30日，Rob Pike宣布了Go的存在。\n2009年11月10日，以完全开源的方式公布了Linux和Mac OSX上的版本，11月22日公布了Windows版本。\n2010年1月8日，当选2009年年度语言。\n2010年5月，谷歌投入使用。\n2011年4月，谷歌开始抽调员工全职开发Go，并于5月宣布Google APP Engine支持Go。\n2015年8月19日 ，Go1.5版本发布，本次更新中移除了“最后残余的C代码”，请内存管理方面权威专家Rick Hudson对GC进行重新设计\n2018年8月24日 ， Go1.11版本发布，开始不支持WinXP系统\n2022年3月15日， Go1.18版本发布，开始全面支持泛型，以及新增Workspaces的概念\n\n现状\n从世界范围看，Go在中国的发展势头最猛，且远超第二名。在很多互联网大厂Go已成为主要开发语言。\n下图显示了在 开发人者生态系统调查 2020 中调查的每个国家使用 Go 作为主要语言的开发者的分布情况（受访者最多可以选择 3 种主要语言）。 中国的开发者集中度最高，有 16% 的中国开发者使用 Go 语言。\n\n资料来源于Go 语言现状调查报告 | The GoLand Blog (jetbrains.com)\n1.1.2 Go的优劣优势\n\n语法简单，易于学习。类C的语法，同时比C&#x2F;C++简洁和干净。\n\n自带GC，方便使用。\n\n快速编译，高效执行。\n\n简单的依赖管理。\n\n并发编程，轻松驾驭。\n\n静态类型，同时有一些动态语言的特征(var声明)。\n\n标准类库，规范统一\n\n\n劣势\n\n不支持动态加载代码。\n\n发展时间短，生态不及Java、C++庞大，但是够用。\n\n\n1.1.3 Go的应用场景应用场景总览\n\n巨型中央服务器领域。\n高性能分布式领域。\n游戏服务端开发。\n复杂事件处理。\n对实时性要求很高的软件开发。\n可以在Intel和ARM处理器上运行，因此也可以在安卓上运行。\n\ngo微服务开发\n\n零依赖，让我们可以最小化我们的镜像,节省存储与拉取镜像带宽。\nRuntime使用更小的内存，对比Java的JVM。\n更好的并行能力，当你真的需求更多CPU的时候。\n更高的性能，对比解释性语言，在处理数据已经并发方面优势明显。\n简单，学习成本低，内部人员可以转入Go阵营。\n使用Go能更接近云原生生态，比如docker，k8s, habor都是用Go开发的。\n\n\n模块二：Go环境配置2.1 学习目标\n安装Go的安装包\n\n下载VSCode\n\n写出第一个Golang程序\n\n了解GOROOT，GOPATH，GOPROXY，GO111MODULE\n\n\n2.2 安装Golang下载地址：\nThe Go Programming Language (golang.org)（官方链接，需要挂梯子）\nAll releases - The Go Programming Language (google.cn)（Golang官网的官方镜像，强烈推荐）\nGo下载 - Go中文网 - Golang中文社区 (studygolang.com)（国内Golang的交流社区）\n然后下载对应平台的安装包即可。请记住您的Golang的安装位置，这一点很重要\ngo1.21.0.windows-amd64.msi（windows推荐下载这个，直接下一步，下一步安装即可，请记住安装位置）\ngo1.21.0.windows-amd64.zip（下载这个虽然只需要解压就好了，但是不太推荐，需要在Path中手动配置Go的路径）\n\n好吧，如果你不记的话可能也没啥问题，如果是用.msi安装的话，Path会自己帮你配好，此外一般的编译器也会自动找到它的位置，如果是直接解压.zip的仁兄，请移步到GOROOT的配置\nPath已经配好的（.msi安装请忽略），按win+R输入cmd回车，输入go version，即可看到相应版本信息，没配置Path的话，只能看到，’xxx’不是内部或外部命令，也不是可运行的程序或批处理文件\n2.3 配置GOPATH？不，咱先不配，后面将详细介绍什么是GOROOT，GOPATH，GOPROXY，GO111MODULE，然后去了解，为什么要配它们\n2.4 VSCode的安装及配置微软商店：（直接搜索就好）\n\n\n官网下载：\nVisual Studio Code - Code Editing. Redefined\n\n直接下一步，下一步安装即可\n打开后点击左边的拓展\n\n输入Chinese\n\n安装之后，右下角有个弹窗\n继续输入Go，安装第一个\n\n此时VSCode即可用于写Golang的代码\n但是，刚装完Go插件的小伙伴应该会发现，右下角会出现一堆弹窗，不用管它们，全X掉就好，因为点了也没用。\n2.5 第一个Golang程序先了解一下常用的几个go命令：（也许学的过程中只会用到go run）\ntxtgo env\t\t\t用于打印Go的环境信息。\ngo run\t\t\t命令可以编译并运行命令源码文件。\ngo get\t\t\t可以根据要求和实际情况从互联网上下载或更新指定的代码包及其依赖包，并对它们进行编译和安装。\ngo build\t\t命令用于编译我们指定的源码文件或代码包以及它们的依赖包。\ngo install\t\t用于编译并安装指定的代码包及它们的依赖包。\ngo clean\t\t命令会删除掉执行其它命令时产生的一些文件和目录。\ngo doc\t\t\t命令可以打印附于Go程序实体上的文档。我们可以通过把程序实体的标识符作为该命令的参数来达到查看其文档的目的。\ngo test\t\t\t命令用于对Go编写的程序进行测试。\ngo list\t\t\t命令的作用是列出指定的代码包的信息。\ngo fix\t\t\t会把指定代码包的所有Go源码文件中的旧版本代码修正为新版本的代码。\ngo vet\t\t\t是一个用于检查Go源码中静态错误的简单工具。\ngo tool pprof\t命令来交互式的访问概要文件的内容。打开此电脑，选择一个路径用于保存你的Golang项目代码\n这里我用的是D:\\，在此目录下新建一个go_project文件夹，当然叫其他名字也可以。\n\n双击打开，并新建src，pkg，bin三个文件夹。\n\n不用太多疑惑，这是一个完整的Golang项目所需要的，实际上初学者所用到的只有src一个文件夹，或者说也不需要这么新建文件夹，随便在桌面空白处新建一个文本文档，然后把.txt改成.go，然后用VSCode打开，上手敲代码就好了，这里只是养成一个好习惯而已。\n下面简单介绍这三个文件夹的作用：\nbin：用来存放编译后生成的可执行文件\npkg：用来存放编译后生成的归档文件\nsrc：用来存放源码文件\n接下来，我们将尝试用VSCode写你的第一个Golang程序\n\n选择刚才创建的项目文件夹\n\n右键src，选择新建文件夹\n前文提到src即是我们存放源码的地方，所以我们写的代码全放在该文件夹下。\n\n文件夹名字随便起就行，这里就不得不提一嘴，为什么又要新建文件夹？直接新建文件不好吗？\n新建文件夹是为了更好地去规范我们的代码的存放位置，说白了，要建立一种框架意识，共同起到一种功能的所有代码放一块，打包起来，你说包也行，模块也勉强可以（学过C++或者Java的会很好理解这一点）。\n项目 &gt; 模块（可以没有） &gt; 包 &gt; 单元（说法不准确，理解就行）\n包就是文件夹，一个单元就是一个.go文件，从前往后的包含关系。\n同时这也是为了去符合Golang的一个硬性规范，Golang一个包下只能有一个main()函数（与Java不同）（不信的可以去试试，看看会不会报错）。\n\n然后新建一个叫hello.go的文件\n\n然后右下角会出现一堆弹窗，不用管它们，全X掉就好，因为点了也没用。\n来到右边，我们开始写我们的第一个Golang程序（报红很正常，咱们硬写，后面会解释为什么会报红）\n温馨小提示：正常来说package包名默认与文件夹保持一致，这里第一行默认应为package day1，但是Go硬性规定，main()函数所在的.go文件，包名必须为main，否则将无法运行，这一点很重要！\n\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n\n  fmt.Println(&quot;Hello World!&quot;)\n\n&#125;\n写完后，记得按ctrl+s保存一下\n然后我们选择在终端中打开\n\n这里默认就是该文件所在的文件夹路径，注意右上角是powershell，可以理解为cmd的超集，把它当作cmd用就好了\n\n**然后输入go run hello.go**（也可以先go build hello.go生成可执行二进制文件.exe，然后输入hello运行，即先编译，再运行，有些类似Java的-javac、-java命令）\n\n回车后，就可以看到执行结果啦\n\n后面解决，一堆报红及GOROOT，GOPATH，GOPROXY，GO111MODULE是什么。\n2.6 VSCode的Go工具插件（一）现在来介绍VSCode的Go工具插件：\n有趣的是这些Go插件都是Go的第三方库的开源模块(所以需要配置GOPROXY环境变量)。\n打开VSCode（已打开当我没说），按ctrl+shift+p\n输入 Go:Install/Update Tools\n\n选择全选，并点击确定\n\n然后耐心等待一会儿，你就能在下方的窗口中看到全部下载失败了\n\n不过不要心急，这是有办法解决的，配置一下GOPROXY代理就好了，此外请注意这里，是重点内容\n\n让我们先进入下一章节。\n2.7 什么是GOROOT，GOPATH，GOPROXY，GO111MODULE？GOROOT：GOROOT就是Go的安装目录，前面安装Go的时候让大家记住的（类似于java的JDK）。\nGOPATH：GOPATH就是我们的工作空间，保存go项目代码和第三方依赖包（如果go项目没有配置GOPATH，则需要go.mod，go.work管理）。\nGOPROXY：GOPROXY表示的是go的代理设置，之所以有这个环境变量，是因为go这种语言不像C语言，在C语言中，如果我们想要使用别人的第三方代码，一般有两种途径：\n\n将第三方代码源码合并到自己的工程文件中，再合并编译。\n将第三方代码编译生成的共享库**.so或.dll** 文件放到工程目录下，然后通过条件编译来使用。\n\nGo中如何使用别人的第三方代码，即模块（MODULE），类似于java，我们可以在编程时，通过第三方代码的库地址，如git仓库，然后在编译的时候，IDE会帮我们自动的拉取第三方库文件到当前工程。\n这样做虽然很方便，但是很多第三方代码库是在国外服务器上的，因为一些限制，我们不能很顺利的使用和下载这些仓库，这样就会导致下载缓慢或者失败，所以这个时候就需要一个代理来实现下载，这个代理就是中间商，可以跨过限制来访问。（说白点就是梯子）\nGolang官方默认的GOPROXY是https://goproxy.io，但是效果不太理想，我们可以使用国内的七牛云代理https://goproxy.cn。\nGO111MODULE：GO111MODULE是go modules功能的开关，关于go modules这里不详细说明，感兴趣的可以去搜一下。\n在没有go modules机制时，go工程中对于第三方功能包的管理非常复杂，也非常专业，这就导致程序员在进行开发的时候，对于第三方功能包的管理很不方便，所以有了go modules机制。\n这个机制的开关是通过GO111MODULE环境变量来配置的。\n接下来我们开始配置相关的环境变量\n为了使所有的计算机用户都可以使用 Go 环境，我们就在系统变量之中配置。\n当然，在用户变量之中配置也是可以的，毕竟我们的电脑一般就一个用户用。\n先打开我们的环境变量\n\n\n现在我们看到我们的环境变量是这个样子，先进入到我们的系统变量中的Path里面看一下\n\n仔细看最后一个路径，是不是很眼熟，没错这就是我们安装.msi，go的安装包的时候，它帮我们配置好的Path路径。\n如果是用.zip解压的，是没有这个路径的，需要自己手动配置，如果不配置这个Path，我们在使用终端中go命令的时候，就会显示’xxx’不是内部或外部命令，也不是可运行的程序或批处理文件，原因是终端中当前路径下，是没有go的命令文件的，添加到Path之后，终端中输入相应的命令，系统会优先在Path中搜索执行该指令的文件，然后执行，大家可以试着将任何软件的启动项路径写入Path，然后在任意位置打开cmd，直接调用启动文件\n\n然后我们退出，返回环境变量页面\n2.7.1 配置GOROOT接下来我们要修改一下GOROOT的写法，其实不改也没什么大问题\n\n\n这样我们的GOROOT就配好了\n2.7.2 配置GOPROXY和GO111MODULE\n\n最后GOPATH的配置，我们还是先不配，我们在 2.9.1 GOPATH配置中讲解\n这里只需要留意一下用户变量中的这个GOPATH的路径，是我们在VSCode尝试下载Go工具插件的时候，帮我们配置的路径，也就是Go工具插件的安装路径\n\n最后不要忘了点确定！\n\n2.8 VSCode的Go工具插件（二）现在我们重新下载Go工具插件（可能需要重启一下VSCode）\n\n现在就成功了，然后你会发现新增了很多实用的功能，自动导包，代码自动提示，保存后自动格式化代码，显示函数信息等等。\n2.9 go mod，go work命令介绍golang是从v1.11引入了go mod机制，为了完善go mod机制，又从v1.18引入了多模块工作区机制（要把哪些mod纳入到你的工作区内），即go work机制，从之前的版本升级到v1.18及之后，vscode工作环境可能会报错。\nvscode打开的文件夹的路径默认作为了其工作区（workspace），而这个工作区的根目录要有go.mod文件，如果根目录，或src下没有这个go.mod就要指定哪些子目录的go.mod纳入工作区\n一般是项目的根目录作为工作区，如这里的D:\\go_project，工作区的根目录，或src下要有go.mod，我们需要用指令生成一个（自己手写也可以），如果不想在工作区目录下生成，就需要用go work命令指明工作区的子目录（这里就是src）下的哪些mod需要纳入工作区。\n接下来，我们来解决这个报错信息。\n当我们没有给我们的项目路径设置GOPATH的时候，我们就需要依赖于go.mod和go.work来管理我们的项目。\n\n首先，在集成终端中打开day1的路径\n\n输入go mod init day1，初始化该模块\n\n报错就消失了，但是不要着急，还没结束\n\n之前提过，Golang一个包下只能有一个main()函数，所以下面这种就会报错\n\n此时就需要像这样，新建一个包，这样就不会报错了，但是还没结束，我们并没有给day2这个包配置go.mod，尽管它没有报错\n（这里是一个文件夹，表示一个模块，即多模块项目）\n\n给day2配置完go.mod后，会发现，它又报错了\n\n原因是，当一个go项目存在多个模块时，要使go.mod之间产生依赖，我们需要用go.work去管理go.mod（1.18之后的配置）\n\n然后我们打开任意工作区子目录的集成终端（emmm，我没有找到直接打开工作区的终端的选项）\n\n然后输入cd ..回退到工作区路径（go.work需要在工作区目录下，如果一开始打开的工作区（从VSCode打开的文件夹）为src，go.work就放在src目录下）\n\n然后输入go work init src\\day1初始化一下，再输入go work use src\\day2，将day2纳入工作区\n（也可以输入go work init初始化一个空的工作区，然后再用go work use纳入day1，day2）\n\n最后就不会报错了\n\n补充：\ngo.mod放在src目录下（不是子文件夹），可以不需要go.work（一开始的介绍里面讲的比较清晰）\n也可以不使用go.mod和go.work，直接在GOPATH中加上项目路径即可\n2.9.1 GOPATH配置（可以不配，使用go mod管理）可以借用一下之前用户变量里面的GOPATH，直接在末尾添加;D:\\go_project（你的项目路径，不要忘了加’;’）\n\n\n或者添加在系统变量里面\n\n最后不要忘了点确定！\n\n这样也是可以的，最后重启一下VSCode就好了\n\n虽然上面很多东西不讲，后面也可以继续学，但是博主觉得，把这些东西弄明白挺重要的，虽然讲的都很粗浅，但是可以增加对Golang的一些理解，对于以后大家深入学习Golang有一定的帮助的。\n拓展1：VSCode文件夹分层显示如果你的VSCode的子文件夹为一个时，会自动收缩成一行，感觉十分不方便，可以打开设置，点击工作区，点击功能，点击资源管理器，找到Compact Folders，取消勾选即可。\n\n拓展2：Goland的方便之处（小特性）前面讲过：Golang一个包下只能有一个main()函数，所以正常而言，每写一个main()函数就要新建一个文件夹，但是Goland一个包下可以写好几个包含.go的文件，注意，这样写肯定不会给你运行，但是你可以按ctrl+shift+F10强行运行当前的.go文件（一个文件），就不需要每次都要创建文件夹了\n初始配置已经全部讲完，正式开始你的Golang学习之旅吧！\n\n模块三：Go基础语法3.1 学习目标\n认识基本的标识符与关键字\n了解基本的操作符与表达式\n熟练掌握变量、常量、字面量的相关操作\n\n3.2 标识符与关键字Go变量、常量、自定义类型、包、函数的命名方式必须遵循以下规则：\n\n首字符可以是任意Unicode字符或下划线。\n首字符之外的部分可以是Unicode字符、下划线或数字。\n名字的长度无限制。\n\n理论上名字里可以有汉字，甚至可以全是汉字，但实际中不要这么做。\n3.2.1 Go关键字gobreak  default  func  interface  select  case  defer  go  map  struct  chan  else  goto  package  switch  const  if  range  type  continue  for  import  return  fallthrough  var3.2.2 常量gotrue  false  iota  nil   3.2.3 数据类型goint  int8  int16  int32  int64  uint  uint8  uint16  uint32  uint64  uintptr  float32  float64  complex128  complex64  bool  byte  rune  string  error3.2.4 函数gomake  len  cap  new  append  copy  close  delete  complex  real  imag  panic  recover3.3 操作符与表达式Go 语言内置的运算符有：\n\n算术运算符\n关系运算符\n逻辑运算符\n位运算符\n赋值运算符\n\n3.3.1 算法术运算符\n\n\n运算符\n描述\n\n\n\n+\n相加\n\n\n-\n相减\n\n\n*\n相乘\n\n\n&#x2F;\n相除\n\n\n%\n求余\n\n\ngo// arithmetic 算术运算\nfunc arithmetic() &#123;\n    var a float32 = 8\n    var b float32 = 3\n    var c float32 = a + b\n    var d float32 = a - b\n    var e float32 = a * b\n    var f float32 = a / b\n    fmt.Printf(&quot;a=%.3f, b=%.3f, c=%.3f, d=%.3f, e=%.3f, f=%.3f\\n&quot;, a, b, c, d, e, f)\n&#125;3.3.2 关系运算符\n\n\n运算符\n描述\n\n\n\n=&#x3D;\n检查两个值是否相等，如果相等返回 True 否则返回 False\n\n\n!&#x3D;\n检查两个值是否不相等，如果不相等返回 True 否则返回 False\n\n\n&gt;\n检查左边值是否大于右边值，如果是返回 True 否则返回 False\n\n\n&gt;&#x3D;\n检查左边值是否大于等于右边值，如果是返回 True 否则返回 False\n\n\n&lt;\n检查左边值是否小于右边值，如果是返回 True 否则返回 False\n\n\n&lt;&#x3D;\n检查左边值是否小于等于右边值，如果是返回 True 否则返回 False\n\n\ngo// relational 关系运算符\nfunc relational() &#123;\n    var a float32 = 8\n    var b float32 = 3\n    var c float32 = 8\n    fmt.Printf(&quot;a==b吗 %t\\n&quot;, a == b)\n    fmt.Printf(&quot;a!=b吗 %t\\n&quot;, a != b)\n    fmt.Printf(&quot;a&gt;b吗 %t\\n&quot;, a &gt; b)\n    fmt.Printf(&quot;a&gt;=b吗 %t\\n&quot;, a &gt;= b)\n    fmt.Printf(&quot;a&lt;c吗 %t\\n&quot;, a &lt; b)\n    fmt.Printf(&quot;a&lt;=c吗 %t\\n&quot;, a &lt;= c)\n&#125;3.3.3 逻辑运算符\n\n\n运算符\n描述\n\n\n\n&amp;&amp;\n逻辑 AND 运算符。 如果两边的操作数都是 True，则为 True，否则为 False\n\n\n\n|\n\n\n!\n逻辑 NOT 运算符。 如果条件为 True，则为 False，否则为 True\n\n\ngo// logistic 逻辑运算符\nfunc logistic() &#123;\n    var a float32 = 8\n    var b float32 = 3\n    var c float32 = 8\n    fmt.Printf(&quot;a&gt;b &amp;&amp; b&gt;c吗 %t\\n&quot;, a &gt; b &amp;&amp; b &gt; c)\n    fmt.Printf(&quot;a&gt;b || b&gt;c吗 %t\\n&quot;, a &gt; b || b &gt; c)\n    fmt.Printf(&quot;a&gt;b不成立，对吗 %t\\n&quot;, !(a &gt; b))\n    fmt.Printf(&quot;b&gt;c不成立，对吗 %t\\n&quot;, !(b &gt; c))\n&#125;3.3.4 位运算符\n\n\n运算符\n描述\n\n\n\n&amp;\n参与运算的两数各对应的二进位相与（两位均为1才为1）\n\n\n|\n参与运算的两数各对应的二进位相或（两位有一个为1就为1）\n\n\n^\n参与运算的两数各对应的二进位相异或，当两对应的二进位相同时为0，不同时为1。作为一元运算符时表示按位取反，，符号位也跟着变\n\n\n&lt;&lt;\n左移n位就是乘以2的n次方。a&lt;&lt;b是把a的各二进位全部左移b位，高位丢弃，低位补0。通过左移，符号位可能会变\n\n\n&gt;&gt;\n右移n位就是除以2的n次方。a&gt;&gt;b是把a的各二进位全部右移b位，正数高位补0，负数高位补1\n\n\ngo// bit_op 位运算\nfunc bit_op() &#123;\n    fmt.Printf(&quot;os arch %s, int size %d\\n&quot;, runtime.GOARCH, strconv.IntSize) // int是4字节还是8字节，取决于操作系统是32位还是64位\n    var a int32 = 260\n    fmt.Printf(&quot;260     %s\\n&quot;, BinaryFormat(a))\n    fmt.Printf(&quot;-260    %s\\n&quot;, BinaryFormat(-a)) // 负数用补码表示。在对应正数二进制表示的基础上，按拉取反，再末位加1\n    fmt.Printf(&quot;260&amp;4   %s\\n&quot;, BinaryFormat(a&amp;4))\n    fmt.Printf(&quot;260|3   %s\\n&quot;, BinaryFormat(a|3))\n    fmt.Printf(&quot;260^7   %s\\n&quot;, BinaryFormat(a^7))     // ^作为二元运算符时表示异或\n    fmt.Printf(&quot;^-260   %s\\n&quot;, BinaryFormat(^-a))     // ^作为一元运算符时表示按位取反，符号位也跟着变\n    fmt.Printf(&quot;-260&gt;&gt;10 %s\\n&quot;, BinaryFormat(-a&gt;&gt;10)) // 正数高位补0，负数高位补1\n    fmt.Printf(&quot;-260&lt;&lt;3 %s\\n&quot;, BinaryFormat(-a&lt;&lt;3))   // 负数左移，可能变成正数\n    // Go没有循环（无符号）左/右移符号   &gt;&gt;&gt;  &lt;&lt;&lt;\n&#125;\n\n// 输出一个int32对应的二进制表示\nfunc BinaryFormat(n int32) string &#123;\n    a := uint32(n)               // 将传进来的int32类型数值转换为无符号的uint32并赋值给a\n    sb := strings.Builder&#123;&#125;      // 拼接字符串\n    c := uint32(math.Pow(2, 31)) // 求2的31次方的幂值 二进制为：10000000000000000000000000000000\n    for i := 0; i &lt; 32; i++ &#123;\n        if a&amp;c &gt; 0 &#123;\n            sb.WriteString(&quot;1&quot;)\n        &#125; else &#123;\n            sb.WriteString(&quot;0&quot;)\n        &#125;\n        c &gt;&gt;= 1\n    &#125;\n    return sb.String()\n&#125;3.3.5 赋值运算符\n\n\n运算符\n描述\n\n\n\n=\n简单的赋值运算符，将一个表达式的值赋给一个左值\n\n\n+&#x3D;\n相加后再赋值\n\n\n-&#x3D;\n相减后再赋值\n\n\n*&#x3D;\n相乘后再赋值\n\n\n&#x2F;&#x3D;\n相除后再赋值\n\n\n%&#x3D;\n求余后再赋值\n\n\n&lt;&lt;&#x3D;\n左移后赋值\n\n\n&gt;&gt;&#x3D;\n右移后赋值\n\n\n&amp;&#x3D;\n按位与后赋值\n\n\n|&#x3D;\n按位或后赋值\n\n\n^&#x3D;\n按位异或后赋值\n\n\ngo// assignment 赋值运算\nfunc assignment() &#123;\n    var a, b int = 8, 3\n    a += b\n    fmt.Printf(&quot;a+=b %d\\n&quot;, a)\n    a, b = 8, 3\n    a -= b\n    fmt.Printf(&quot;a-=b %d\\n&quot;, a)\n    a, b = 8, 3\n    a *= b\n    fmt.Printf(&quot;a*=b %d\\n&quot;, a)\n    a, b = 8, 3\n    a /= b\n    fmt.Printf(&quot;a/=b %d\\n&quot;, a)\n    a, b = 8, 3\n    a %= b\n    fmt.Printf(&quot;a%%=b %d\\n&quot;, a) // %在fmt里有特殊含意，所以需要前面再加个%转义一下\n    a, b = 8, 3\n    a &lt;&lt;= b\n    fmt.Printf(&quot;a&lt;&lt;=b %d\\n&quot;, a)\n    a, b = 8, 3\n    a &gt;&gt;= b\n    fmt.Printf(&quot;a&gt;&gt;=b %d\\n&quot;, a)\n    a, b = 8, 3\n    a &amp;= b\n    fmt.Printf(&quot;a&amp;=b %d\\n&quot;, a)\n    a, b = 8, 3\n    a |= b\n    fmt.Printf(&quot;a|=b %d\\n&quot;, a)\n    a, b = 8, 3\n    a ^= b\n    fmt.Printf(&quot;a^=b %d\\n&quot;, a)\n&#125;3.4 变量、常量、字面量3.4.1 变量3.4.1.1 变量类型\n\n\n类型\nGo变量类型\nfmt输出\n\n\n\n整型\nint int8 int16 int32 int64 uint uint8 uint16 uint32 uint64\n%d\n\n\n浮点型\nfloat32 float64\n%f %e %g\n\n\n布尔型\nbool\n%t\n\n\n指针\nuintptr\n%p\n\n\n引用\nmap slice channel\n%v\n\n\n字节\nbyte\n%c\n\n\n任意字符\nrune\n%c\n\n\n字符串\nstring\n%s\n\n\n错误\nerror\n%v\n\n\n3.4.1.2 变量声明Go中的变量需要声明后才能使用，同一作用域内不支持重复声明。并且Go的变量声明后必须使用。\n标准声明\nGo的变量声明格式为：\ngo    var 变量名 变量类型变量声明以关键字var开头，变量类型放在变量的后面，行尾无需分号。 举个例子：\ngo    var name string \n    var age int \n    var isOk bool批量声明\n每声明一个变量就需要写var关键字会比较繁琐，Go中还支持批量变量声明：\ngo    var ( \n        name string \n        age int \n        isOk bool \n    )3.4.1.3 变量的初始化Go在声明变量的时候，会自动对变量对应的内存区域进行初始化操作。每个变量会被初始化成其类型的默认值，例如： 整型和浮点型变量的默认值为0。 字符串变量的默认值为空字符串。 布尔型变量默认为false。 切片、函数、指针变量的默认为nil。\n当然我们也可在声明变量的时候为其指定初始值。变量初始化的标准格式如下：\n标准格式\ngo    var 变量名 类型 = 表达式举个例子：\ngo    var name string = &quot;pprof.cn&quot;\n    var sex int = 1或者一次初始化多个变量\ngo    var name, sex = &quot;pprof.cn&quot;, 1类型推导\n有时候我们会将变量的类型省略，这个时候编译器会根据等号右边的值来推导变量的类型完成初始化。\ngo    var name = &quot;pprof.cn&quot;\n    var sex = 1短变量声明\n在函数内部，可以使用更简略的:=方式声明并初始化变量。\n注意：短变量只能用于声明局部变量，不能用于全局变量的声明\ngopackage main\n\nimport &quot;fmt&quot;\n\n// 全局变量m\nvar m = 100\n\nfunc main() &#123;\n    n := 10\n    m := 200 // 此处声明局部变量m\n    fmt.Println(m, n)\n&#125;\n匿名变量\n在使用多重赋值时，如果想要忽略某个值，可以使用匿名变量（anonymous variable）。 匿名变量用一个下划线_表示，例如：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc foo() (int, string) &#123;\n    return 10, &quot;Q1mi&quot;\n&#125;\n\nfunc main() &#123;\n    x, _ := foo()\n    _, y := foo()\n    fmt.Println(&quot;x=&quot;, x)\n    fmt.Println(&quot;y=&quot;, y)\n&#125;\n匿名变量不占用命名空间，不会分配内存，所以匿名变量之间不存在重复声明。\n注意事项：\n\n函数外的每个语句都必须以关键字开始（var、const、func 等）\n:=不能使用在函数外。\n_多用于占位，表示忽略值\n\n3.4.2 常量相对于变量，常量是恒定不变的值，多用于定义程序运行期间不会改变的那些值。 常量的声明和变量声明非常类似，只是把var换成了const，常量在定义的时候必须赋值。\ngo    const pi = 3.1415\n    const e = 2.7182声明了pi和e这两个常量之后，在整个程序运行期间它们的值都不能再发生变化了。\n多个常量也可以一起声明：\ngo    const (\n        pi = 3.1415\n        e  = 2.7182\n    )const同时声明多个常量时，如果省略了值则表示和上面一行的值相同。 例如：\ngo    const (\n        n1 = 100\n        n2\n        n3\n    )上面示例中，常量n1、n2、n3的值都是100。\n3.4.2.1 iotaiota是go语言的常量计数器，只能在常量的表达式中使用。iota在const关键字出现时将被重置为0。const中每新增一行常量声明将使iota计数一次(iota可理解为const语句块中的行索引)。 使用iota能简化定义，在定义枚举时很有用。\n举个例子：\ngo    const (\n        n1 = iota //0\n        n2        //1\n        n3        //2\n        n4        //3\n    )几个常见的iota示例:\n使用_跳过某些值\ngo    const (\n        n1 = iota //0\n        n2        //1\n        _\n        n4 //3\n    )iota声明中间插队\ngo    const (\n        n1 = iota //0\n        n2 = 100  //100\n        n3 = iota //2\n        n4        //3\n    )\n    const n5 = iota //0定义数量级 （这里的&lt;&lt;表示左移操作，1 &lt;&lt; 10表示将1的二进制表示向左移10位，也就是由1变成了10000000000，也就是十进制的1024。同理2 &lt;&lt; 2表示将2的二进制表示向左移2位，也就是由10变成了1000，也就是十进制的8。）\ngo    const (\n        _  = iota\n        KB = 1 &lt;&lt; (10 * iota)\n        MB = 1 &lt;&lt; (10 * iota)\n        GB = 1 &lt;&lt; (10 * iota)\n        TB = 1 &lt;&lt; (10 * iota)\n        PB = 1 &lt;&lt; (10 * iota)\n    )多个iota定义在一行\ngo    const (\n        a, b = iota + 1, iota + 2 //1,2\n        c, d                      //2,3\n        e, f                      //3,4\n    )3.4.3 字面量字面量：没有出现变量名，直接出现了值。\n基础类型的字面量相当于是常量。\ngo    fmt.Printf(&quot;%t\\n&quot;, 04 == 4.00)      // 用到了整型字面量和浮点型字面量\n    fmt.Printf(&quot;%v\\n&quot;, .4i)             // 虚数字面量 0.4i\n    fmt.Printf(&quot;%t\\n&quot;, &#39;\\u4f17&#39; == &#39;众&#39;) // Unicode和rune字面量\n    fmt.Printf(&quot;Hello\\nWorld\\n!\\n&quot;)     // 字符串字面量3.4.4 变量作用域对于全局变量，如果以大写字母开头，所有地方都可以访问，跨package访问时需要带上package名称；如果以小写字母开头，则本package内都可以访问。函数内部的局部变量，仅本函数内可以访问。&#123;&#125;可以固定一个作用域。内部声明的变量可以跟外部声明的变量有冲突，以内部的为准（就近原则）。\ngovar (\n    A = 3 // 所有地方都可以访问\n    b = 4 // 本package内可以访问\n)\n\nfunc foo() &#123;\n    b := 5 // 本函数内可以访问\n    &#123;\n        b := 6 // 本作用域内可以访问\n    &#125;\n&#125;3.5 注释3.5.1 注释的形式\n单行注释，以//打头。\n多行注释有2种形式：\n\ntxt1.  连续多行以`//`打头，注意多行注释之间不能出现空行。\n2.  在段前使用`/*`，段尾使用`*/`。\n注释行前加缩进即可写go代码。\n注释中给定的关键词。NOTE:引人注意，TODO:将来需要优化，Deprecated:变量或函数强烈建议不要再使用。\n\ngo// Add 2个整数相加\n// 返回和。\n//\n// NOTE: 注释可以有多行，但中间不能出现空行（仅有//不算空行）。\nfunc Add(a, b int) int &#123;\n    return a + b\n&#125;\n\n/*\nSub 函数使用示例：\n  for i:=0;i&lt;3;i++&#123;\n      Sub(i+1, i)\n  &#125;\n看到了吗？只需要行前缩进，注释里就可以写go代码，是不是很简单。\n*/\nfunc Sub(a, b int) int &#123;\n    return a - b\n&#125;\n\n// TODO: Prod 该函数不能并发调用，需要优化\nfunc Prod(a, b int) int &#123;\n    return a * b\n&#125;\n\n// Deprecated: Div 不要再调用了\nfunc Div(a, b int) int &#123;\n    return a / b\n&#125;3.5.2 注释的位置针对行的注释在行上方或右侧。函数的上方在func xxx()上方。结构体的注释在type xxx struct上方。包注释在package xxx的上方。一个包只需要在一个地方写包注释，通常会专门写一个doc.go，里面只有一行package xxx和关于包的注释。\n（关于doc这里不详细说明，感兴趣的可以去搜一下）\ngo// FormatBool, FormatFloat, FormatInt, and FormatUint convert values to strings:\n//\n//\ts := strconv.FormatBool(true)\n//\ts := strconv.FormatFloat(3.1415, &#39;E&#39;, -1, 64)\n//\ts := strconv.FormatInt(-42, 16)\n//\ts := strconv.FormatUint(42, 16)\npackage fmt3.6 Go 语言变量、常量命名规则\n变量名称必须由数字、字母、下划线组成。\n标识符开头不能是数字。\n标识符不能是保留字和关键字。\n变量的名字是区分大小写的如：age和Age是不同的变量。在实际的运用中,也建议,不要 用一个单词大小写区分两个变量。\n标识符（变量名称）一定要见名思意：变量名称建议用名词，方法名称建议用动词。\n变量命名一般采用驼峰式，当遇到特有名词（缩写或简称，如DNS）的时候，特有名词 根据是否私有全部大写或小写。\n\n3.7 Go 语言代码风格\n代码每一行结束后不用写分号（;）\n\n运算符左右建议各加一个空格。\n\n\ngo   var username string = &quot;scc749&quot;\nGo 语言程序员推荐使用驼峰式命名当名字有几个单词组成的时优先使用大小写分隔。\n\n强制的代码风格左括号必须紧接着语句不换行，这个特性刚开始会使开发者不习惯，但随着对 Go 语言的不断熟悉，就会发现风格统一让大家在阅读代码时把注意力集中到了解决问题上，而不是代码风格上。\n\ngo fmt主要用于格式化文档，让所有人的代码风格保持一致（命令行命令，已经安装VSCode的Go工具插件可以忽略，因为保存后可以自动格式化）\nD:\\go_project\\src\\day1&gt; go fmt hello.gohello.go\n\n\n\n模块四：Go数据类型4.1 学习目标\n了解Go的数据类型\n熟练掌握数组、切片、字符串、map的相关知识点，并可以在复杂的环境下熟练使用\n\n4.2 数据类型概览4.2.1 基础数据类型\n\n\n类型\n长度(字节)\n默认值\n说明\n\n\n\nbool\n1\nfalse\n\n\n\nbyte\n1\n0\nuint8，取值范围[0,255]\n\n\nrune\n4\n0\nUnicode Code Point, int32\n\n\nint, uint\n4或8\n0\n32 或 64 位，取决于操作系统\n\n\nint8, uint8\n1\n0\n-128 ~ 127, 0 ~ 255\n\n\nint16, uint16\n2\n0\n-32768 ~ 32767, 0 ~ 65535\n\n\nint32, uint32\n4\n0\n-21亿~ 21亿, 0 ~ 42亿，rune是int32 的别名\n\n\nint64, uint64\n8\n0\n\n\n\nfloat32\n4\n0.0\n\n\n\nfloat64\n8\n0.0\n\n\n\ncomplex64\n8\n\n\n\n\ncomplex128\n16\n\n\n\n\nuintptr\n4或8\n\n以存储指针的 uint32 或 uint64 整数\n\n\ngopackage main\n\nimport (\n    &quot;errors&quot;\n    &quot;fmt&quot;\n    &quot;runtime&quot;\n    &quot;strconv&quot;\n    &quot;unsafe&quot;\n)\n\nfunc main() &#123;\n    fmt.Printf(&quot;os arch %s, int size %d\\n&quot;, runtime.GOARCH, strconv.IntSize) // int是4字节还是8字节，取决于操作系统是32位还是64位\n    var a int = 5\n    var b int8 = 5\n    var c int16 = 5\n    var d int32 = 5\n    var e int64 = 5\n    var f uint = 5\n    var g uint8 = 5\n    var h uint16 = 5\n    var i uint32 = 5\n    var j uint64 = 5\n    fmt.Printf(&quot;a=%d, b=%d, c=%d, d=%d, e=%d, f=%d, g=%d, h=%d, i=%d, j=%d\\n&quot;, a, b, c, d, e, f, g, h, i, j)\n    var k float32 = 5\n    var l float64 = 5\n    fmt.Printf(&quot;k=%f, l=%.2f\\n&quot;, k, l) //%.2f保留2位小数\n    var m complex128 = complex(4, 7)\n    var n complex64 = complex(4, 7)\n    fmt.Printf(&quot;type of m is %T, type of n is %T\\n&quot;, m, n) // %T输出变量类型\n    fmt.Printf(&quot;m=%v, n=%v\\n&quot;, m, n)                       // 按值的本来值输出\n    fmt.Printf(&quot;m=%+v, n=%+v\\n&quot;, m, n)                     // 在 %v 基础上，对结构体字段名和值进行展开\n    fmt.Printf(&quot;m=%#v, n=%#v\\n&quot;, m, n)                     // 输出 Go 语言语法格式的值\n    fmt.Printf(&quot;m的实部%f, m的虚部%f\\n&quot;, real(m), imag(m))\n    fmt.Printf(&quot;m的实部%e, m的虚部%g\\n&quot;, real(m), imag(m)) // %e科学计数法，%g根据实际情况采用%e或%f格式（以获得更简洁、准确的输出）\n    o := true                                        // 等价于var o bool = true\n    fmt.Printf(&quot;o=%t\\n&quot;, o)                          // %t布尔变量\n    var pointer unsafe.Pointer = unsafe.Pointer(&amp;a)\n    var p uintptr = uintptr(pointer)\n    var ptr *int = &amp;a\n    fmt.Printf(&quot;p=%x pointer=%p ptr=%p\\n&quot;, p, pointer, ptr) // %p输出地址，%x十六进制\n    var q byte = 100                                        // byte是uint，取值范围[0,255]\n    fmt.Printf(&quot;q=%d, binary of q is %b\\n&quot;, q, q)           // %b输出二进制\n    var r rune = &#39;☻&#39;                                        // rune实际上是int32，即可以表示2147483647种字符，包括所有汉字和各种特殊符号\n    fmt.Printf(&quot;r=%d, r=%U\\n&quot;, r, r)                        // %U Unicode 字符\n    var s string = &quot;I&#39;m 绝迹之春&quot;\n    fmt.Printf(&quot;s=%s\\n&quot;, s)\n    var t error = errors.New(&quot;my error&quot;)\n    fmt.Printf(&quot;error is %v\\n&quot;, t)\n    fmt.Printf(&quot;error is %+v\\n&quot;, t) // 在 %v 基础上，对结构体字段名和值进行展开\n    fmt.Printf(&quot;error is %#v\\n&quot;, t) // 输出 Go 语言语法格式的值\n&#125;\n数值型变量的默认值是0，字符串的默认值是空字符串，布尔型变量的默认值是false，引用类型、函数、指针、接口的默认值是nil。数组的默认值取每个元素对应类型的默认值，结构体的默认值取每个成员变量对应类型的默认值。\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    var a int\n    var b byte\n    var f float32\n    var t bool\n    var s string\n    var r rune\n    var arr [3]int\n    var slc []int\n\n    fmt.Printf(&quot;default value of int %d\\n&quot;, a)\n    fmt.Printf(&quot;default value of byte %d\\n&quot;, b)\n    fmt.Printf(&quot;default value of float %.2f\\n&quot;, f)\n    fmt.Printf(&quot;default value of bool %t\\n&quot;, t)\n    fmt.Printf(&quot;default value of string [%s]\\n&quot;, s)\n    fmt.Printf(&quot;default value of rune %d, [%c]\\n&quot;, r, r)\n    fmt.Printf(&quot;default int array is %v\\n&quot;, arr) // 取每个元素对应类型的默认值\n    fmt.Printf(&quot;default slice is nil %t\\n&quot;, slc == nil)\n\n    //输出\n    /*\n       default value of int 0\n       default value of byte 0\n       default value of float 0.00\n       default value of bool false\n       default value of string []\n       default value of rune 0, []\n       default int array is [0 0 0]\n       default slice is nil true\n    */\n&#125;\n4.2.2 复合数据类型\n\n\n类型\n默认值\n说明\n\n\n\narray\n取每个元素对应类型的默认值\n值类型\n\n\nstruct\n取每个成员变量对应类型的默认值\n值类型\n\n\nstring\n“”\nUTF-8 字符串\n\n\nslice\nnil\n引用类型\n\n\nmap\nnil\n引用类型\n\n\nchannel\nnil\n引用类型\n\n\ninterface\nnil\n接口\n\n\nfunction\nnil\n函数\n\n\n4.2.3 自定义类型类型别名\ngotype byte = uint8\ntype rune = int32\ntype semaphore = uint8自定义类型\ngotype user struct &#123;name string;age int&#125;  // 用分号把多行代码隔开\ntype signal uint8\ntype ms map[string]string\ntype add func(a, b int) int4.3 数组\n\n数组：是同一种数据类型的固定长度的序列。\n\n数组定义：var a [len]int，比如：var a [5]int，数组长度必须是常量，且是类型的组成部分。一旦定义，长度不能变。\n\n长度是数组类型的一部分，因此，var a[5] int和var a[10]int是不同的类型。\n\n数组可以通过下标进行访问，下标是从0开始，最后一个元素下标是：len-1\n\n\ngo   for i := 0; i &lt; len(a); i++ &#123;\n   &#125;\n   for index, v := range a &#123;\n   &#125;\n访问越界，如果下标在数组合法范围之外，则触发访问越界，会panic\n\n数组是值类型，赋值和传参会复制整个数组，而不是指针。因此改变副本的值，不会改变本身的值。\n\n支持 ==、!= 操作符，因为内存总是被初始化过的。\n\n指针数组 [n]*T，数组指针 *[n]T。\n\n\n4.3.1 数组的初始化4.3.1.1 一维数组初始化gopackage main\n\nimport &quot;fmt&quot;\n\nvar arr0 [5]int = [5]int&#123;1, 2, 3&#125;\nvar arr1 = [5]int&#123;1, 2, 3, 4, 5&#125;\nvar arr2 = [...]int&#123;1, 2, 3, 4, 5, 6&#125;\nvar str = [5]string&#123;3: &quot;hello world&quot;, 4: &quot;tom&quot;&#125;\n\nfunc main() &#123;\n    a := [3]int&#123;1, 2&#125;           // 未初始化元素值为 0。\n    b := [...]int&#123;1, 2, 3, 4&#125;   // 通过初始化值确定数组长度。\n    c := [5]int&#123;2: 100, 4: 200&#125; // 使用引号初始化元素。\n    d := [...]struct &#123;\n        name string\n        age  uint8\n    &#125;&#123;\n        &#123;&quot;user1&quot;, 10&#125;, // 可省略元素类型。\n        &#123;&quot;user2&quot;, 20&#125;, // 别忘了最后一行的逗号。\n    &#125;\n    fmt.Println(arr0, arr1, arr2, str)\n    fmt.Println(a, b, c, d)\n&#125;\n4.3.1.2 二维数组初始化gopackage main\n\nimport &quot;fmt&quot;\n\nvar arr0 [5][3]int\nvar arr1 [2][3]int = [...][3]int&lt;!--swig￼0--&gt;\n\nfunc main() &#123;\n    a := [2][3]int&lt;!--swig￼1--&gt;\n    b := [...][2]int&lt;!--swig￼2--&gt; // 第 2 纬度不能用 &quot;...&quot;。\n    fmt.Println(arr0, arr1)\n    fmt.Println(a, b)\n&#125;\n4.3.2 访问数组里的元素\n通过index访问\n首元素 arr[0]\n末元素 arr[len(arr)-1]\n\n\n访问二维数组里的元素\n位于第三行第四列的元素 arr[2][3]\n\n\n\n4.3.3 遍历数组go// 遍历数组里的元素\nfor i, ele := range arr &#123;\n    fmt.Printf(&quot;index=%d, element=%d\\n&quot;, i, ele)\n&#125;\n// 或者这样遍历数组\nfor i := 0; i &lt; len(arr); i++ &#123; // len(arr)获取数组的长度\n    fmt.Printf(&quot;index=%d, element=%d\\n&quot;, i, arr[i])\n&#125;\n// 遍历二维数组\nfor row, array := range arr &#123; // 先取出某一行\n    for col, ele := range array &#123; // 再遍历这一行\n        fmt.Printf(&quot;arr[%d][%d]=%d\\n&quot;, row, col, ele)\n    &#125;\n&#125;通过for range遍历数组时取得的是数组里每一个元素的拷贝。\ngoarr := [...]int&#123;1, 2, 3&#125;\nfor i, ele := range arr &#123; // ele是arr中元素的拷贝\n    arr[i] += 8 // 修改arr里的元素，不影响ele\n    fmt.Printf(&quot;%d %d %d\\n&quot;, i, arr[i], ele)\n    ele += 1 // 修改ele不影响arr\n    fmt.Printf(&quot;%d %d %d\\n&quot;, i, arr[i], ele)\n&#125;\nfor i := 0; i &lt; len(arr); i++ &#123;\n    fmt.Printf(&quot;%d %d\\n&quot;, i, arr[i])\n&#125;4.3.4 内置函数 len() 和 cap()在数组上调用cap()函数表示capacity容量，即给数组分配的内存空间可以容纳多少个元素；len()函数代表length长度，即目前数组里有几个元素。由于数组初始化之后长度不会改变，不需要给它预留内存空间，所以len(arr)==cap(arr)。对于多维数组，其cap和len指第一维的长度。\n4.3.5 数组拷贝和传参数组的长度和类型都是数组类型的一部分，函数传递数组类型时这两部分都必须吻合。Go没有按引用传参，全都是按值传参，即传递数组实际上传的是数组的拷贝，当数组的长度很大时，仅传参开销都很大。如果想修改函数外部的数组，就把它的指针（数组在内存里的地址）传进来。\ngo// 参数必须是长度为5的int型数组（注意长度必须是5）\nfunc update_array1(arr [5]int) &#123;\n    fmt.Printf(&quot;array in function, address is %p\\n&quot;, &amp;arr[0])\n    arr[0] = 888\n\n&#125;\n\nfunc update_array2(arr *[5]int) &#123;\n    fmt.Printf(&quot;array in function, address is %p\\n&quot;, &amp;((*arr)[0]))\n    arr[0] = 888 // 因为传的是数组指针，所以直接在原来的内存空间上进行修改\n&#125;4.4 切片切片是一个结构体，包含三个成员变量，array指向一块连续的内存空间，cap表示这块内存的大小，len表示目前该内存里存储了多少元素。\ngotype slice struct &#123;\n    array unsafe.Pointer\n    len   int\n    cap   int\n&#125;\n需要说明，slice并不是数组或数组指针。它通过内部指针和相关属性引用数组片段，以实现变长方案。\n\n切片：切片是数组的一个引用，因此切片是引用类型。但自身是结构体，值拷贝传递。\n切片的长度可以改变，因此，切片是一个可变的数组。\n切片遍历方式和数组一样，可以用len()求长度。表示可用元素数量，读写操作不能超过该限制。\ncap可以求出slice最大扩张容量，不能超出数组限制。0 &lt;= len(slice) &lt;= len(array)，其中array是slice引用的数组。\n切片的定义：var 变量名 []类型，比如var str []string var arr []int。\n如果slice == nil，那么len、cap结果都等于0。\n\n4.4.1 创建切片的各种方式gopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    //1.声明切片\n    var s1 []int\n    if s1 == nil &#123;\n        fmt.Println(&quot;是空&quot;)\n    &#125; else &#123;\n        fmt.Println(&quot;不是空&quot;)\n    &#125;\n    // 2.:=\n    s2 := []int&#123;&#125;\n    // 3.make()\n    var s3 []int = make([]int, 0)\n    fmt.Println(s1, s2, s3)\n    // 4.初始化赋值\n    var s4 []int = make([]int, 0, 0)\n    fmt.Println(s4)\n    s5 := []int&#123;1, 2, 3&#125;\n    fmt.Println(s5)\n    // 5.从数组切片\n    arr := [5]int&#123;1, 2, 3, 4, 5&#125;\n    var s6 []int\n    // 前包后不包\n    s6 = arr[1:4]\n    fmt.Println(s6)\n&#125;\n4.4.2 切片的初始化go全局：\nvar arr = [...]int&#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9&#125;\nvar slice0 []int = arr[start:end] \nvar slice1 []int = arr[:end]        \nvar slice2 []int = arr[start:]        \nvar slice3 []int = arr[:] \nvar slice4 = arr[:len(arr)-1]      // 去掉切片的最后一个元素\n局部：\narr2 := [...]int&#123;9, 8, 7, 6, 5, 4, 3, 2, 1, 0&#125;\nslice5 := arr[start:end]\nslice6 := arr[:end]        \nslice7 := arr[start:]     \nslice8 := arr[:]  \nslice9 := arr[:len(arr)-1] // 去掉切片的最后一个元素\n\n\n操作\n含义\n\n\n\ns[n]\n切片 s 中索引位置为 n 的项\n\n\ns[:]\n从切片 s 的索引位置 0 到 len(s)-1 处所获得的切片\n\n\ns[low:]\n从切片 s 的索引位置 low到 len(s)-1 处所获得的切片\n\n\ns[:high]\n从切片 s 的索引位置 0 到 high 处所获得的切片，len&#x3D;high\n\n\ns[low:high]\n从切片 s 的索引位置 low到 high 处所获得的切片，len&#x3D;high-low\n\n\ns[low:high:max]\n从切片 s 的索引位置 low到 high 处所获得的切片，len&#x3D;high-low，cap&#x3D;max-low\n\n\nlen(s)\n切片 s 的长度，总是&lt;&#x3D;cap(s)\n\n\ncap(s)\n切片 s 的容量，总是&gt;&#x3D;len(s)\n\n\ngopackage main\n\nimport &quot;fmt&quot;\n\nvar arr = [...]int&#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9&#125;\nvar slice0 []int = arr[2:8]\nvar slice1 []int = arr[0:6]        // 可以简写为 var slice []int = arr[:end]\nvar slice2 []int = arr[5:10]       // 可以简写为 var slice[]int = arr[start:]\nvar slice3 []int = arr[0:len(arr)] // var slice []int = arr[:]\nvar slice4 = arr[:len(arr)-1]      // 去掉切片的最后一个元素\nfunc main() &#123;\n    fmt.Printf(&quot;全局变量：arr %v\\n&quot;, arr)\n    fmt.Printf(&quot;全局变量：slice0 %v\\n&quot;, slice0)\n    fmt.Printf(&quot;全局变量：slice1 %v\\n&quot;, slice1)\n    fmt.Printf(&quot;全局变量：slice2 %v\\n&quot;, slice2)\n    fmt.Printf(&quot;全局变量：slice3 %v\\n&quot;, slice3)\n    fmt.Printf(&quot;全局变量：slice4 %v\\n&quot;, slice4)\n    fmt.Printf(&quot;-----------------------------------\\n&quot;)\n    arr2 := [...]int&#123;9, 8, 7, 6, 5, 4, 3, 2, 1, 0&#125;\n    slice5 := arr[2:8]\n    slice6 := arr[0:6]         // 可以简写为 slice := arr[:end]\n    slice7 := arr[5:10]        // 可以简写为 slice := arr[start:]\n    slice8 := arr[0:len(arr)]  // slice := arr[:]\n    slice9 := arr[:len(arr)-1] // 去掉切片的最后一个元素\n    fmt.Printf(&quot;局部变量： arr2 %v\\n&quot;, arr2)\n    fmt.Printf(&quot;局部变量： slice5 %v\\n&quot;, slice5)\n    fmt.Printf(&quot;局部变量： slice6 %v\\n&quot;, slice6)\n    fmt.Printf(&quot;局部变量： slice7 %v\\n&quot;, slice7)\n    fmt.Printf(&quot;局部变量： slice8 %v\\n&quot;, slice8)\n    fmt.Printf(&quot;局部变量： slice9 %v\\n&quot;, slice9)\n&#125;\n4.4.3 通过make来创建切片go    var slice []type = make([]type, len)\n    slice  := make([]type, len)\n    slice  := make([]type, len, cap)代码：\ngopackage main\n\nimport &quot;fmt&quot;\n\nvar slice0 []int = make([]int, 10)\nvar slice1 = make([]int, 10)\nvar slice2 = make([]int, 10, 10)\n\nfunc main() &#123;\n    fmt.Printf(&quot;make全局slice0 ：%v\\n&quot;, slice0)\n    fmt.Printf(&quot;make全局slice1 ：%v\\n&quot;, slice1)\n    fmt.Printf(&quot;make全局slice2 ：%v\\n&quot;, slice2)\n    fmt.Println(&quot;--------------------------------------&quot;)\n    slice3 := make([]int, 10)\n    slice4 := make([]int, 10)\n    slice5 := make([]int, 10, 10)\n    fmt.Printf(&quot;make局部slice3 ：%v\\n&quot;, slice3)\n    fmt.Printf(&quot;make局部slice4 ：%v\\n&quot;, slice4)\n    fmt.Printf(&quot;make局部slice5 ：%v\\n&quot;, slice5)\n&#125;\n4.4.4 用append内置函数操作切片（切片追加）切片相对于数组最大的特点就是可以追加元素，可以自动扩容。追加的元素放到预留的内存空间里，同时len加1。如果预留空间已用完，则会重新申请一块更大的内存空间，capacity大约变成之前的2倍(cap&lt;1024)或1.25倍(cap&gt;1024)。把原内存空间的数据拷贝过来，在新内存空间上执行append操作。\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    s := make([]int, 3, 5)\n    for i := 0; i &lt; 3; i++ &#123;\n        s[i] = i + 1\n    &#125; // s=[1,2,3]\n    fmt.Printf(&quot;s[0] address %p, s=%v\\n&quot;, &amp;s[0], s)\n    /*\n       capacity还够用，直接把追加的元素放到预留的内存空间上\n    */\n    s = append(s, 4, 5) // 可以一次append多个元素\n    fmt.Printf(&quot;s[0] address %p, s=%v\\n&quot;, &amp;s[0], s)\n    /*\n       capacity不够用了，得申请一片新的内存，把老数据先拷贝过来，在新内存上执行append操作\n    */\n    s = append(s, 6)\n    fmt.Printf(&quot;s[0] address %p, s=%v\\n&quot;, &amp;s[0], s)\n&#125;\n4.4.5 slice中cap重新分配规律go// 探究capacity扩容规律\nfunc expansion() &#123;\n    s := make([]int, 0, 3)\n    prevCap := cap(s)\n    for i := 0; i &lt; 100; i++ &#123;\n        s = append(s, i)\n        currCap := cap(s)\n        if currCap &gt; prevCap &#123;\n            // 每次扩容都是扩到原先的2倍\n            fmt.Printf(&quot;capacity从%d变成%d\\n&quot;, prevCap, currCap)\n            prevCap = currCap\n        &#125;\n    &#125;\n&#125;4.4.6 切片的拷贝gopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n\n    s1 := []int&#123;1, 2, 3, 4, 5&#125;\n    fmt.Printf(&quot;slice s1 : %v\\n&quot;, s1)\n    s2 := make([]int, 10)\n    fmt.Printf(&quot;slice s2 : %v\\n&quot;, s2)\n    copy(s2, s1)\n    fmt.Printf(&quot;copied slice s1 : %v\\n&quot;, s1)\n    fmt.Printf(&quot;copied slice s2 : %v\\n&quot;, s2)\n    s3 := []int&#123;1, 2, 3&#125;\n    fmt.Printf(&quot;slice s3 : %v\\n&quot;, s3)\n    s3 = append(s3, s2...)\n    fmt.Printf(&quot;appended slice s3 : %v\\n&quot;, s3)\n    s3 = append(s3, 4, 5, 6)\n    fmt.Printf(&quot;last slice s3 : %v\\n&quot;, s3)\n&#125;\n输出结果：\ntxtslice s1 : [1 2 3 4 5]\nslice s2 : [0 0 0 0 0 0 0 0 0 0]\ncopied slice s1 : [1 2 3 4 5]\ncopied slice s2 : [1 2 3 4 5 0 0 0 0 0]\nslice s3 : [1 2 3]\nappended slice s3 : [1 2 3 1 2 3 4 5 0 0 0 0 0]\nlast slice s3 : [1 2 3 1 2 3 4 5 0 0 0 0 0 4 5 6]copy ：copy()函数在两个slice间复制数据，复制长度以len小的为准。两个slice可指向同一底层数组，允许元素区间重叠。\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    data := [...]int&#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9&#125;\n    fmt.Println(&quot;array data : &quot;, data)\n    s1 := data[8:]\n    s2 := data[:5]\n    fmt.Printf(&quot;slice s1 : %v\\n&quot;, s1)\n    fmt.Printf(&quot;slice s2 : %v\\n&quot;, s2)\n    copy(s2, s1)\n    fmt.Printf(&quot;copied slice s1 : %v\\n&quot;, s1)\n    fmt.Printf(&quot;copied slice s2 : %v\\n&quot;, s2)\n    fmt.Println(&quot;last array data : &quot;, data)\n&#125;\n输出结果:\ntxtarray data :  [0 1 2 3 4 5 6 7 8 9]\nslice s1 : [8 9]\nslice s2 : [0 1 2 3 4]\ncopied slice s1 : [8 9]\ncopied slice s2 : [8 9 2 3 4]\nlast array data :  [8 9 2 3 4 5 6 7 8 9]应及时将所需数据copy到较小的slice，以便释放超大号底层数组内存。\n4.4.7 切片resize（调整大小）通过指定起止下标，可以从大切片中截取一个子切片。\ngos := make([]int, 3, 5)\t// len=3, cap=5\nsub_slice = s[1:3]\t\t// len=2, cap=4刚开始，子切片和母切片共享底层的内存空间，修改子切片会反映到母切片上，在子切片上执行append()函数会把新元素放到母切片预留的内存空间上。当子切片不断执行append()函数，耗完了母切片预留的内存空间，子切片跟母切片就会发生内存分离，此后两个切片没有任何关系。\n\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc sub_slice() &#123;\n    /*\n        截取一部分，创造子切片，此时子切片与母切片(或母数组)共享底层内存空间，母切片的capacity子切片可能直接用\n    */\n    s := make([]int, 3, 5)\n    for i := 0; i &lt; 3; i++ &#123;\n        s[i] = i + 1\n    &#125; // s=[1,2,3]\n    fmt.Printf(&quot;s[1] address %p\\n&quot;, &amp;s[1])\n    sub_slice := s[1:3] // 从切片创造子切片，len=cap=2\n    fmt.Printf(&quot;len %d cap %d\\n&quot;, len(sub_slice), cap(sub_slice))\n    /*\n        母切片的capacity还允许子切片执行append操作\n    */\n    sub_slice = append(sub_slice, 6, 7) // 可以一次append多个元素\n    sub_slice[0] = 8\n    fmt.Printf(&quot;s=%v, sub_slice=%v, s[1] address %p, sub_slice[0] address %p\\n&quot;, s, sub_slice, &amp;s[1], &amp;sub_slice[0])\n    /*\n        母切片的capacity用完了，子切片再执行append就得申请一片新的内存，把老数据先拷贝过来，在新内存上执行append操作。此时的append操作跟母切片没有任何关系\n    */\n    sub_slice = append(sub_slice, 8)\n    sub_slice[0] = 9\n    fmt.Printf(&quot;s=%v, sub_slice=%v, s[1] address %p, sub_slice[0] address %p\\n&quot;, s, sub_slice, &amp;s[1], &amp;sub_slice[0])\n\n    arr := [5]int&#123;1, 2, 3, 4, 5&#125;\n    fmt.Printf(&quot;arr[1] address %p\\n&quot;, &amp;arr[1])\n    sub_slice = arr[1:3] // 从数组创造子切片，len=cap=2\n    fmt.Printf(&quot;len %d cap %d\\n&quot;, len(sub_slice), cap(sub_slice))\n    /*\n        母数组的capacity还允许子切片执行append操作\n    */\n    sub_slice = append(sub_slice, 6, 7) // 可以一次append多个元素\n    sub_slice[0] = 8\n    fmt.Printf(&quot;arr=%v, sub_slice=%v, arr[1] address %p, sub_slice[0] address %p\\n&quot;, arr, sub_slice, &amp;arr[1], &amp;sub_slice[0])\n    /*\n        母数组的capacity用完了，子切片再执行append就得申请一片新的内存，把老数据先拷贝过来，在新内存上执行append操作。此时的append操作跟母数组没有任何关系\n    */\n    sub_slice = append(sub_slice, 8)\n    sub_slice[0] = 9\n    fmt.Printf(&quot;arr=%v, sub_slice=%v, arr[1] address %p, sub_slice[0] address %p\\n&quot;, arr, sub_slice, &amp;arr[1], &amp;sub_slice[0])\n&#125;\n\nfunc main() &#123;\n    sub_slice()\n&#125;\n输出结果：\ntxts[1] address 0xc00000e368\nlen 2 cap 4\ns=[1 8 3], sub_slice=[8 3 6 7], s[1] address 0xc00000e368, sub_slice[0] address 0xc00000e368\ns=[1 8 3], sub_slice=[9 3 6 7 8], s[1] address 0xc00000e368, sub_slice[0] address 0xc000012240\narr[1] address 0xc00000e398\nlen 2 cap 4\narr=[1 8 3 6 7], sub_slice=[8 3 6 7], arr[1] address 0xc00000e398, sub_slice[0] address 0xc00000e398\narr=[1 8 3 6 7], sub_slice=[9 3 6 7 8], arr[1] address 0xc00000e398, sub_slice[0] address 0xc0000122c04.4.8 切片的传参Go函数传参，传的都是值，即传切片会把切片的{arrayPointer, len, cap}这3个字段拷贝一份传进来。由于传的是底层数组的指针，所以可以直接修改底层数组里的元素。\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc update_slice(s []int) &#123;\n    s[0] = 888\n&#125;\n\nfunc main() &#123;\n    s := []int&#123;1, 2, 3&#125;\n    update_slice(s)\n    fmt.Printf(&quot;s=%v\\n&quot;, s)\n&#125;\n4.5 字符串字符串里可以包含任意Unicode字符。\ngos := &quot; My name is 绝迹之春☻&quot;字符串里可以包含转义字符。\ngos := &quot;He say:\\&quot;I&#39;m fine.\\&quot; \\n\\\\Thank\\tyou.\\\\&quot;字符串也可以用反引号来定义，反引号里的转义字符无效。反引号里的内容原封不动地输出，包括空白符和换行符。\ngos := `here is first line. \n\n  there is third line.\n`4.5.1 字符串常用操作\n\n\n方法\n介绍\n\n\n\nlen(str)\n求长度\n\n\nstrings.Split\n分割\n\n\nstrings.Contains\n判断是否包含\n\n\nstrings.HasPrefix,strings.HasSuffix\n前缀&#x2F;后缀判断\n\n\nstrings.Index(),strings.LastIndex()\n子串出现的位置\n\n\ngos := &quot;born to win, born to die.&quot;\nfmt.Printf(&quot;sentence length %d\\n&quot;, len(s))\nfmt.Printf(&quot;\\&quot;s\\&quot; length %d\\n&quot;, len(&quot;s&quot;))  // 英文字母的长度为1\nfmt.Printf(&quot;\\&quot;中\\&quot;  length %d\\n&quot;, len(&quot;中&quot;)) // 一个汉字占3个长度\narr := strings.Split(s, &quot; &quot;)\nfmt.Printf(&quot;arr[3]=%s\\n&quot;, arr[3])\nfmt.Printf(&quot;contain die %t\\n&quot;, strings.Contains(s, &quot;die&quot;))          // 包含子串\nfmt.Printf(&quot;contain wine %t\\n&quot;, strings.Contains(s, &quot;wine&quot;))        // 包含子串\nfmt.Printf(&quot;first index of born %d\\n&quot;, strings.Index(s, &quot;born&quot;))    // 寻找子串第一次出现的位置\nfmt.Printf(&quot;last index of born %d\\n&quot;, strings.LastIndex(s, &quot;born&quot;)) // 寻找子串最后一次出现的位置\nfmt.Printf(&quot;begin with born %t\\n&quot;, strings.HasPrefix(s, &quot;born&quot;))    // 以xxx开头\nfmt.Printf(&quot;end with die. %t\\n&quot;, strings.HasSuffix(s, &quot;die.&quot;))      // 以xxx结尾4.5.2 字符串拼接把多个字符串拼接成一个长的字符串有多种方式。\n\ntxt加号连接。\ntxt`func fmt.Sprintf(format string, a ...interface&#123;&#125;) string`\ntxt`func strings.Join(elems []string, sep string) string`\ntxt当有大量的`string`需要拼接时，用`strings.Builder`效率最高\n\ngos1 := &quot;Hello&quot;\ns2 := &quot;how&quot;\ns3 := &quot;are&quot;\ns4 := &quot;you&quot;\nmerged := s1 + &quot; &quot; + s2 + &quot; &quot; + s3 + &quot; &quot; + s4\nfmt.Println(merged)\nmerged = fmt.Sprintf(&quot;%s %s %s %s&quot;, s1, s2, s3, s4)\nfmt.Println(merged)\nmerged = strings.Join([]string&#123;s1, s2, s3, s4&#125;, &quot; &quot;)\nfmt.Println(merged)\n// 当有大量的string需要拼接时，用strings.Builder效率最高\nsb := strings.Builder&#123;&#125;\nsb.WriteString(s1)\nsb.WriteString(&quot; &quot;)\nsb.WriteString(s2)\nsb.WriteString(&quot; &quot;)\nsb.WriteString(s3)\nsb.WriteString(&quot; &quot;)\nsb.WriteString(s4)\nsb.WriteString(&quot; &quot;)\nmerged = sb.String()\nfmt.Println(merged)4.5.3 字符串中的字符string中每个元素叫“字符”，字符有两种：\n\nbyte：1个字节， 代表ASCII码的一个字符。\nrune：4个字节，代表一个UTF-8字符，一个汉字可用一个rune表示。\n\nstring是常量，不能修改其中的字符。string可以转换为[]byte或[]rune类型。string底层是byte数组，string的长度就是该byte数组的长度， UTF-8编码下一个汉字占3个byte，即一个汉字占3个长度。\ngos1 := &quot;My name is 绝迹之春&quot;\narr := []byte(s1)\nbrr := []rune(s1)\nfmt.Printf(&quot;last byte %d\\n&quot;, arr[len(arr)-1]) // string可以转换为[]byte或[]rune类型\nfmt.Printf(&quot;last byte %c\\n&quot;, arr[len(arr)-1]) // byte或rune可以转为string\nfmt.Printf(&quot;last rune %d\\n&quot;, brr[len(brr)-1])\nfmt.Printf(&quot;last rune %c\\n&quot;, brr[len(brr)-1])\nL := len(s1)\nfmt.Printf(&quot;string len %d byte array len %d rune array len %d\\n&quot;, L, len(arr), len(brr))\nfor _, ele := range s1 &#123;\n    fmt.Printf(&quot;%c &quot;, ele) // string中的每个元素是字符\n&#125;\nfmt.Println()\nfor i := 0; i &lt; L; i++ &#123;\n    fmt.Printf(&quot;%c &quot;, s1[i]) // [i]前面应该出现数组或切片，这里自动把string转成了[]byte（而不是[]rune）\n&#125;4.6 数据类型转换强制类型转换的基本方法就是把目标类型放在变量前面，把变量括起来。\ngovar i int = 9\nvar by byte = byte(i) // int转为byte\ni = int(by)           // byte转为int\n低精度向高精度转换没问题，高精度向低精度转换会丢失位数。\n无符号向有符号转换，最高位是符号位。\nbyte和int可以互相转换。\nfloat和int可以互相转换，小数位会丢失。\nbool和int不能相互转换。\n不同长度的int或float之间可以相互转换。\n\ngo// 高精度向低精度转换，数字很小时这种转换没问题\nvar ua uint64 = 1\ni8 := int8(ua)\nfmt.Printf(&quot;i8=%d\\n&quot;, i8)\n\n// 最高位的1变成了符号位\nua = uint64(math.MaxUint64)\ni64 := int64(ua)\nfmt.Printf(&quot;i64=%d\\n&quot;, i64)\n\n// 位数丢失\nui32 := uint32(ua)\nfmt.Printf(&quot;ui32=%d\\n&quot;, ui32)\n\n// 单个字符可以转为int\nvar i int = int(&#39;a&#39;)\nfmt.Printf(&quot;i=%d\\n&quot;, i)\n\n// bool和int不能相互转换\n\n// byte和int可以互相转换\nvar by byte = byte(i)\ni = int(by)\nfmt.Printf(&quot;i=%d\\n&quot;, i)\n\n// float和int可以互相转换，小数位会丢失\nvar ft float32 = float32(i)\ni = int(ft)\nfmt.Printf(&quot;i=%d\\n&quot;, i)4.6.1 string和其他数据类型互转govar err error\nvar i int = 8\nvar i64 int64 = int64(i)\n// int转string\nvar s string = strconv.Itoa(i) //内部调用FormatInt\ns = strconv.FormatInt(i64, 10)\n// string转int\ni, err = strconv.Atoi(s)\n// string转int64\ni64, err = strconv.ParseInt(s, 10, 64)\n\n// float转string\nvar f float64 = 8.123456789\ns = strconv.FormatFloat(f, &#39;f&#39;, 2, 64) //保留2位小数\nfmt.Println(s)\n// string转float\nf, err = strconv.ParseFloat(s, 64)\n\n// string&lt;--&gt;[]byte\nvar arr []byte = []byte(s)\ns = string(arr)\n\n// string&lt;--&gt;[]rune\nvar brr []rune = []rune(s)\ns = string(brr)\n\nfmt.Printf(&quot;err %v\\n&quot;, err)4.7 mapmap是一种无序的基于key-value的数据结构，Go中的map是引用类型，必须初始化才能使用。\n4.7.1 map定义Go语言map的底层实现是hash table，根据key查找value的时间复杂度是O(1)。\n\nGo中 map的定义语法如下\ngo    map[KeyType]ValueType其中，\ntxt    KeyType:表示键的类型。\n\n    ValueType:表示键对应的值的类型。map类型的变量默认初始值为nil，需要使用make()函数来分配内存。语法为：\ngo    make(map[KeyType]ValueType, [cap])其中cap表示map的容量，该参数虽然不是必须的，但是我们应该在初始化map的时候就为其指定一个合适的容量。\n4.7.2 map的初始化govar m map[string]int                  // 声明map，指定key和value的数据类型\nm = make(map[string]int)              // 初始化，容量为0\nm = make(map[string]int, 200)         // 初始化，容量为5。强烈建议初始化时给一个合适的容量，减少扩容的概率\nm = map[string]int&#123;&quot;语文&quot;: 0, &quot;数学&quot;: 39&#125; // 初始化时直接赋值4.7.3 添加和删除keygom[&quot;英语&quot;] = 59    // 往map里添加key-value对\nm[&quot;英语&quot;] = 70    // 会覆盖之前的值\ndelete(m, &quot;数学&quot;) // 从map里删除key-value对len(m)获取map的长度，Go不支持对map上执行cap函数。\n4.7.4 判断某个键是否存在Go中有个判断map中key是否存在的特殊写法，格式如下:\ngo    value, ok := map[key]   读取key对应的value时，如果key不存在，则返回value类型的默认值，所以强烈建议先判断key是否存在。\ngoif value, ok := m[&quot;语文&quot;]; exists &#123;\n    fmt.Println(value)\n&#125; else &#123;\n    fmt.Println(&quot;map里不存在[语文]这个key&quot;)\n&#125;4.7.5 遍历mapgo// 遍历map\nfor key, value := range m &#123;\n    fmt.Printf(&quot;%s=%d\\n&quot;, key, value)\n&#125;\nfmt.Println(&quot;-----------&quot;)\n// 多次遍历map返回的顺序是不一样的，但相对顺序是一样的，因为每次随机选择一个开始位置，然后顺序遍历\nfor key, value := range m &#123;\n    fmt.Printf(&quot;%s=%d\\n&quot;, key, value)\n&#125;\nfmt.Println(&quot;-----------&quot;)\n\n// 一边遍历一边修改\nfor key, value := range m &#123;\n    m[key] = value + 1\n&#125;\nfor key, value := range m &#123;\n    fmt.Printf(&quot;%s=%d\\n&quot;, key, value)\n&#125;\nfmt.Println(&quot;-----------&quot;)\n\n// for range取得的是值拷贝\nfor _, value := range m &#123;\n    value = value + 1\n&#125;\nfor key, value := range m &#123;\n    fmt.Printf(&quot;%s=%d\\n&quot;, key, value)\n&#125;但我们只想遍历key的时候，可以按下面的写法：\ngofor key := range m &#123;\n    fmt.Println(key)\n&#125;注意： 遍历map时的元素顺序与添加键值对的顺序无关。\n4.7.6 按照指定顺序遍历mapgopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;math/rand&quot;\n    &quot;sort&quot;\n    &quot;time&quot;\n)\n\nfunc main() &#123;\n    rand.Seed(time.Now().UnixNano()) //初始化随机数种子\n\n    var scoreMap = make(map[string]int, 200)\n\n    for i := 0; i &lt; 100; i++ &#123;\n        key := fmt.Sprintf(&quot;stu%02d&quot;, i) //生成stu开头的字符串\n        value := rand.Intn(100)          //生成0~99的随机整数\n        scoreMap[key] = value\n    &#125;\n    //取出map中的所有key存入切片keys\n    var keys = make([]string, 0, 200)\n    for key := range scoreMap &#123;\n        keys = append(keys, key)\n    &#125;\n    //对切片进行排序\n    sort.Strings(keys)\n    //按照排序后的key遍历map\n    for _, key := range keys &#123;\n        fmt.Println(key, scoreMap[key])\n    &#125;\n&#125;\n4.7.7 元素为map类型的切片下面的代码演示了切片中的元素为map类型时的操作：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    var mapSlice = make([]map[string]string, 3)\n    for index, value := range mapSlice &#123;\n        fmt.Printf(&quot;index:%d value:%v\\n&quot;, index, value)\n    &#125;\n    fmt.Println(&quot;after init&quot;)\n    // 对切片中的map元素进行初始化\n    mapSlice[0] = make(map[string]string, 10)\n    mapSlice[0][&quot;name&quot;] = &quot;王五&quot;\n    mapSlice[0][&quot;password&quot;] = &quot;123456&quot;\n    mapSlice[0][&quot;address&quot;] = &quot;红旗大街&quot;\n    for index, value := range mapSlice &#123;\n        fmt.Printf(&quot;index:%d value:%v\\n&quot;, index, value)\n    &#125;\n&#125;\n输出结果：\ntxtindex:0 value:map[]\nindex:1 value:map[]\nindex:2 value:map[]\nafter init\nindex:0 value:map[address:红旗大街 name:王五 password:123456]\nindex:1 value:map[]\nindex:2 value:map[]4.7.8 值为切片类型的map下面的代码演示了map中值为切片类型的操作：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    var sliceMap = make(map[string][]string, 3)\n    fmt.Println(sliceMap)\n    fmt.Println(&quot;after init&quot;)\n    key := &quot;中国&quot;\n    value, ok := sliceMap[key]\n    if !ok &#123;\n        value = make([]string, 0, 2)\n    &#125;\n    value = append(value, &quot;北京&quot;, &quot;上海&quot;)\n    sliceMap[key] = value\n    fmt.Println(sliceMap)\n&#125;\n输出结果：\ntxtmap[]\nafter init\nmap[中国:[北京 上海]]4.7.9 map的键值类型map中的key可以是任意能够用==操作符比较的类型，不能是函数、map、切片，以及包含上述3中类型成员变量的的struct。map的value可以是任意类型。\ngotype f func(int) bool\ntype m map[int]byte\ntype s []int\n\ntype i int\n\nvar m1 map[i]f\nfmt.Println(m1)\n\n/** 函数、map、切片不能当key **/\n// var m2 map[f]bool\n// fmt.Println(m2)\n// var m3 map[m]bool\n// fmt.Println(m3)\n// var m4 map[s]bool\n// fmt.Println(m4)\n\ntype user struct &#123;\n    scores float32 // 如果scores是slice，则user不能作为map的key\n&#125;\n\nu := user&#123;&#125;\nm5 := make(map[user]interface&#123;&#125;)\nm5[u] = 5\nfmt.Println(m5)4.8 channel（初步了解）channel(管道)底层是一个环形队列(先进先出)，send(插入)和recv(取走)从同一个位置沿同一个方向顺序执行。sendx表示最后一次插入元素的位置，recvx表示最后一次取走元素的位置。\ngovar ch chan int        // 管道的声明\nch = make(chan int, 8) // 管道的初始化，环形队列里可容纳8个int\nch &lt;- 1                // 往管道里写入(send)数据\nch &lt;- 2\nch &lt;- 3\nch &lt;- 4\nch &lt;- 5\nv := &lt;-ch // 从管道里取走(recv)数据\nfmt.Println(v)\nv = &lt;-ch\nfmt.Println(v)goread_only := make(&lt;-chan int)  // 定义只读的channel\nwrite_only := make(chan&lt;- int) // 定义只写的channel定义只读和只写的channel意义不大，一般用于在参数传递中。\ngo// 只能向channel里写数据\nfunc send(c chan&lt;- int) &#123;\n    c &lt;- 1\n&#125;\n\n// 只能取channel中的数据\nfunc recv(c &lt;-chan int) &#123;\n    _ = &lt;-c\n&#125;\n\n// 返回一个只读channel\nfunc (c *Context) Done() &lt;-chan struct&#123;&#125; &#123;\n    return nil\n&#125;可以通过for range的方式遍历管道，遍历前必须先关闭管道，禁止再写入元素。\ngoclose(ch) // 遍历前必须先关闭管道，禁止再写入元素\n// 遍历管道里剩下的元素\nfor ele := range ch &#123;\n    fmt.Println(ele)\n&#125;slice、map和channel是Go里的3种引用类型，都可以通过make()函数来进行初始化（申请内存分配）。因为它们都包含一个指向底层数据结构的指针，所以称之为“引用”类型。引用类型未初始化时都是nil，可以对它们执行len()函数，返回0。\n拓展：使用map实现hashset利用空struct（struct{}）内存占用为0的特点，在Go中可以这么实现：\ngovar hashset =  make(map[interface&#123;&#125;]struct&#123;&#125;)\t// 空接口可以存储任意类型的值下面以LeetCode的题目作为参考：\nLeetCode：1207. 独一无二的出现次数\n1207. 独一无二的出现次数\n给你一个整数数组 arr，请你帮忙统计数组中每个数的出现次数。\n如果每个数的出现次数都是独一无二的，就返回 true；否则返回 false。\n示例 1：\ntxt输入：arr = [1,2,2,1,1,3]\n输出：true\n解释：在该数组中，1 出现了 3 次，2 出现了 2 次，3 只出现了 1 次。没有两个数的出现次数相同。示例 2：\ntxt输入：arr = [1,2]\n输出：false示例 3：\ntxt输入：arr = [-3,0,1,-3,1,1,1,-3,10,0]\n输出：true提示：\n\n1 &lt;= arr.length &lt;= 1000\n-1000 &lt;= arr[i] &lt;= 1000\n\n分析：\n首先使用map记录每个数字的出现次数。（键为数字，值为次数）\n随后再利用hashset，统计不同的出现次数的数目。（利用hashset去重的原理，进行查重）\n如果不同的出现次数的数目等于不同数字的数目，则返回true，否则返回false。\n参考答案（或许博主的答案不是最佳答案，但是只是用于理解）：\ngofunc uniqueOccurrences(arr []int) bool &#123;\n    numberMap := make(map[int]int) // 新建map，用于记录每个数字的出现次数\n    for _, ele := range arr &#123;      // 遍历数组元素\n        v, ok := numberMap[ele] // 使key为当前数组元素的value加一，即统计当前数组元素总共出现的次数\n        if ok &#123;                 // 如果key存在当前数组元素，则value加一\n            numberMap[ele] = v + 1\n        &#125; else &#123; // 如果key不存在当前数组元素，则将当前数组元素作为key添加进map，并将其value初始化为1\n            numberMap[ele] = 1\n        &#125;\n    &#125;\n    occurrencesMap := make(map[int]struct&#123;&#125;) // 使用map构建hashset，用于对不同数字的出现次数进行查重\n    for _, v := range numberMap &#123;            // 遍历map的value，即遍历不同数字的出现次数\n        _, ok := occurrencesMap[v] // 判断当前hashset是否存在相同的出现次数\n        if ok &#123;                    // 如果存在，则说明不同的出现次数的数目不等于不同数字的数目，返回false\n            return false\n        &#125; // 如果hashset当前不存在该次数，则将该次数放入hashset\n        occurrencesMap[v] = struct&#123;&#125;&#123;&#125;\n    &#125;\n    return true // 最后不同的出现次数的数目等于不同数字的数目，返回true\n&#125;\n模块五：流程控制5.1 学习目标\n了解基本的流程控制语句\n熟练使用if、switch、for、break、continue、goto以及Label解决复杂问题\n\n5.2 if基本格式：\ngoif 布尔表达式 &#123;\n    /* 在布尔表达式为 true 时执行 */\n&#125;举例：\ngoif 5 &gt; 9 &#123;\n    fmt.Println(&quot;5&gt;9&quot;)\n&#125;\n如果逻辑表达式成立，就会执行&#123;&#125;里的内容。\n逻辑表达式不需要加()。\n&#123;&lt;!-- --&gt;必须紧跟在逻辑表达式后面，不能另起一行。\n\nif条件判断还有一种特殊的写法，可以在if表达式之前添加一个执行语句，再根据变量值进行判断。\ngoif c, d, e := 5, 9, 2; c &lt; d &amp;&amp; (c &gt; e || c &gt; 3) &#123; // 初始化多个局部变量。复杂的逻辑表达式\n    fmt.Println(&quot;fit&quot;)\n&#125;注意：\n\n逻辑表达中可以含有变量或常量。\nif句子中允许包含1个(仅1个)分号，在分号前初始化一些局部变量(即只在if块内可见)。\n\n5.2.1 if-else的用法gocolor := &quot;black&quot;\nif color == &quot;red&quot; &#123; // if只能有一个\n    fmt.Println(&quot;stop&quot;)\n&#125; else if color == &quot;green&quot; &#123;\n    fmt.Println(&quot;go&quot;)\n&#125; else if color == &quot;yellow&quot; &#123; // else if可以有0个、一个或者连续多个\n    fmt.Println(&quot;stop&quot;)\n&#125; else &#123; // else有0个或1个\n    fmt.Printf(&quot;invalid traffic signal: %s\\n&quot;, strings.ToUpper(color))\n&#125;5.2.2 if表达式嵌套goif xxx &#123;\n    if xxx &#123;\n    &#125; else if xxx &#123;\n    &#125; else &#123;\n    &#125;\n&#125; else &#123;\n    if xxx &#123;\n    &#125; else &#123;\n    &#125;\n&#125;注意太深的嵌套不利于代码的维护，比如\ngoif true &#123;\n    if true &#123;\n        if true &#123;\n            if true &#123;\n                if true &#123;\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;5.3 switch5.3.1 switch-case使用switch语句可方便地对大量的值进行条件判断。\ngocolor := &quot;black&quot;\nswitch color &#123;\ncase &quot;green&quot;: // 相当于if color== &quot;green&quot;\n    fmt.Println(&quot;go&quot;)\n    // break\ncase &quot;red&quot;: // 相当于else if color== &quot;red&quot;\n    fmt.Println(&quot;stop&quot;)\n    // break\ndefault: // 相当于else\n    fmt.Printf(&quot;invalid traffic signal: %s\\n&quot;, strings.ToUpper(color))\n&#125;\nswitch-case-default可能模拟if-else if-else，但只能实现相等判断。\nswitch和case后面可以跟常量、变量或函数表达式，只要它们表示的数据类型相同就行。\ncase后面可以跟多个值，只要有一个值满足就行。\nGo语言规定每个switch只能有一个default。\nGo语言中每个case语句中可以不写break，不加break也不会出现穿透的现象。\n\ngofunc add(a int) int &#123;\n    return a + 10\n&#125;\n\nfunc switch_expression() &#123;\n    var a int = 5\n    switch add(a) &#123; // switch后跟一个函数表达式\n    case 15: // case后跟一个常量\n        fmt.Println(&quot;right&quot;)\n    default:\n        fmt.Println(&quot;wrong&quot;)\n    &#125;\n\n    const B = 15\n    switch B &#123; // switch后跟一个常量\n    case add(a): // case后跟一个函数表达式\n        fmt.Println(&quot;right&quot;)\n    default:\n        fmt.Println(&quot;wrong&quot;)\n    &#125;\n&#125;5.3.2 switch不带与后带表达式switch后带表达式时，switch-case只能模拟相等的情况；\n如果switch后不带表达式，case后就可以跟任意的条件表达式。\ngofunc add(a int) int &#123;\n    return a + 10\n&#125;\n\nfunc switch_condition() &#123;\n    color := &quot;yellow&quot;\n    switch color &#123;\n    case &quot;green&quot;:\n        fmt.Println(&quot;go&quot;)\n    case &quot;red&quot;, &quot;yellow&quot;: // 用逗号分隔多个condition，它们之间是“或”的关系，只需要有一个condition满足就行\n        fmt.Println(&quot;stop&quot;)\n    &#125;\n\n    // switch后带表达式时，switch-case只能模拟相等的情况；如果switch后不带表达式，case后就可以跟任意的条件表达式\n    switch &#123;\n    case add(5) &gt; 10:\n        fmt.Println(&quot;right&quot;)\n    default:\n        fmt.Println(&quot;wrong&quot;)\n    &#125;\n&#125;5.3.3 switch Typegofunc switch_type() &#123;\n    var num interface&#123;&#125; = 6.5\n    switch num.(type) &#123; // 获取interface的具体类型。.(type)只能用在switch后面\n    case int:\n        fmt.Println(&quot;int&quot;)\n    case float32:\n        fmt.Println(&quot;float32&quot;)\n    case float64:\n        fmt.Println(&quot;float64&quot;)\n    case byte:\n        fmt.Println(&quot;byte&quot;)\n    default:\n        fmt.Println(&quot;neither&quot;)\n    &#125;\n\n    switch value := num.(type) &#123; // 相当于在每个case内部申明了一个变量value\n    case int: // value已被转换为int类型\n        fmt.Printf(&quot;number is int %d\\n&quot;, value)\n    case float64: // value已被转换为float64类型\n        fmt.Printf(&quot;number is float64 %f\\n&quot;, value)\n    case byte, string: // 如果case后有多个类型，则value还是interface&#123;&#125;类型\n        fmt.Printf(&quot;number is inerface %v\\n&quot;, value)\n    default:\n        fmt.Println(&quot;neither&quot;)\n    &#125;\n\n    // 等价形式\n    switch num.(type) &#123;\n    case int:\n        value := num.(int)\n        fmt.Printf(&quot;number is int %d\\n&quot;, value)\n    case float64:\n        value := num.(float64)\n        fmt.Printf(&quot;number is float64 %f\\n&quot;, value)\n    case byte:\n        value := num.(byte)\n        fmt.Printf(&quot;number is byte %d\\n&quot;, value)\n    default:\n        fmt.Println(&quot;neither&quot;)\n    &#125;\n&#125;5.3.4 fallthrough当命中某一个case时，强行进入下一个case。\ngofunc fall_throth(age int) &#123;\n    fmt.Printf(&quot;您的年龄是%d, 您可以：\\n&quot;, age)\n    switch &#123;\n    case age &gt; 50:\n        fmt.Println(&quot;出任国家首脑&quot;)\n        fallthrough\n    case age &gt; 25:\n        fmt.Println(&quot;生育子女&quot;)\n        fallthrough\n    case age &gt; 22:\n        fmt.Println(&quot;结婚&quot;)\n        fallthrough\n    case age &gt; 38:\n        fmt.Println(&quot;开车&quot;)\n        fallthrough\n    case age &gt; 16:\n        fmt.Println(&quot;参加工作&quot;)\n    case age &gt; 15:\n        fmt.Println(&quot;上高中&quot;)\n        fallthrough\n    case age &gt; 3:\n        fmt.Println(&quot;上幼儿园&quot;)\n    &#125;\n&#125;5.4 forGo语言中的所有循环类型均可以使用for关键字来完成。\n5.4.1 for init; condition; post { }基本格式：\ngofor 初始语句; 条件表达式; 结束语句 &#123;\n    循环体语句\n&#125;举例：\ngoarr := []int&#123;1, 2, 3, 4, 5&#125;\nfor i := 0; i &lt; len(arr); i++ &#123; // 正序遍历切片\n    fmt.Printf(&quot;%d: %d\\n&quot;, i, arr[i])\n&#125;\n条件表达式返回true时循环体不停地进行循环，直到条件表达式返回false时自动退出循环。\n\n局部变量指仅在for块内可见。\n\n初始化变量可以放在for上面（但是初始语句后的分号必须要写）。\n\n\n计算1~100的和\ngosum := 0\nfor i := 0; i &lt;= 100; i++ &#123;\n    sum += i\n&#125;\nfmt.Println(sum)5.4.2 for condition { }只有条件判断时，前后的分号可以不要（这种写法类似于其他编程语言中的while，在while后添加一个条件表达式，满足条件表达 式时持续循环，否则结束循环）。\ngoi := 0\nfor i &lt; 10 &#123;\n    fmt.Println(i)\n    i++\n&#125;注意：Go语言中是没有while语句的，我们可以通过for代替。\n5.4.3 for { }gofor &#123;\n    循环体语句\n&#125;for循环可以通过break、goto、return、panic语句强制退出循环。\ngok := 1\nfor &#123; // 这里也等价for ; ; &#123;\n    if k &lt;= 10 &#123;\n        fmt.Println(&quot;ok~~&quot;, k)\n    &#125; else &#123;\n        break // break就是跳出这个for循环\n    &#125;\n    k++\n&#125;5.4.4 for循环嵌套在for循环中嵌套一个或多个for循环\n基本格式（双层for循环）：\ngofor 初始语句; 条件表达式; 结束语句 &#123;\n    for 初始语句; 条件表达式; 结束语句 &#123;\n        循环体语句\n    &#125;\n    循环体语句\n&#125;以下实例使用循环嵌套来输出2到100间的素数：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    /* 定义局部变量 */\n    var i, j int\n\n    for i = 2; i &lt; 100; i++ &#123;\n        for j = 2; j &lt;= (i / j); j++ &#123;\n            if i%j == 0 &#123;\n                break // 如果发现因子，则不是素数\n            &#125;\n        &#125;\n        if j &gt; (i / j) &#123;\n            fmt.Printf(&quot;%d  是素数\\n&quot;, i)\n        &#125;\n    &#125;\n&#125;\n百钱百鸡：\n我国古代数学家张丘建在《算经》一书中提出的数学问题：鸡翁一值钱五，鸡母一值钱三，鸡雏三值钱一。百钱买百鸡，问鸡翁、鸡母、鸡雏各几何？\n使用三层for循环嵌套（暴力解法且没有剪枝，只是用于理解）\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    for i := 0; i &lt;= 100; i++ &#123;\n        for j := 0; j &lt;= 100; j++ &#123;\n            for k := 0; k &lt;= 100; k++ &#123;\n                if i+j+k == 100 &amp;&amp; i*5+j*3+k/3 == 100 &#123;\n                    fmt.Printf(&quot;公鸡=%d 母鸡=%d 小鸡=%d\\n&quot;, i, j, k)\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;输出结果：\ntxt公鸡=0 母鸡=25 小鸡=75\n公鸡=3 母鸡=20 小鸡=77\n公鸡=4 母鸡=18 小鸡=78\n公鸡=7 母鸡=13 小鸡=80\n公鸡=8 母鸡=11 小鸡=81\n公鸡=11 母鸡=6 小鸡=83\n公鸡=12 母鸡=4 小鸡=845.4.5 for rangeGolang range类似迭代器操作，返回 (索引, 值) 或 (键, 值)。\nfor循环的range格式可以对slice、map、数组、字符串等进行迭代循环。\n\n遍历数组或切片\nfor i, ele := range arr\n\n\n遍历string\nfor i, ele := range &quot;我会唱ABC&quot; //ele是rune类型\n\n\n遍历map，go不保证遍历的顺序\nfor key, value := range m\n\n\n遍历channel，遍历前一定要先close\nfor ele := range ch\nfor range拿到的是数据的拷贝\n\n\n\n格式如下：\ngofor key, value := range oldMap &#123;\n    newMap[key] = value\n&#125;\n\n\n\n1st value\n2nd value\n\n\n\n\nstring\nindex\ns[index]\nunicode, rune\n\n\narray&#x2F;slice\nindex\ns[index]\n\n\n\nmap\nkey\nm[key]\n\n\n\nchannel\nelement\n\n\n\n\n可忽略不想要的返回值，或 _ 这个特殊变量。\ngopackage main\n\nfunc main() &#123;\n    s := &quot;abc&quot;\n    // 忽略 2nd value，支持 string/array/slice/map。\n    for i := range s &#123;\n        println(s[i])\n    &#125;\n    // 忽略 index。\n    for _, c := range s &#123;\n        println(c)\n    &#125;\n    // 忽略全部返回值，仅迭代。\n    for range s &#123;\n\n    &#125;\n\n    m := map[string]int&#123;&quot;a&quot;: 1, &quot;b&quot;: 2&#125;\n    // 返回 (key, value)。\n    for k, v := range m &#123;\n        println(k, v)\n    &#125;\n&#125;\n  输出结果：\ntxt97\n98\n99\n97\n98\n99\na 1\nb 2注意：range会复制对象。\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    a := [3]int&#123;0, 1, 2&#125;\n\n    for i, v := range a &#123; // index、value 都是从复制品中取出。\n\n        if i == 0 &#123; // 在修改前，我们先修改原数组。\n            a[1], a[2] = 999, 999\n            fmt.Println(a) // 确认修改有效，输出 [0, 999, 999]。\n        &#125;\n\n        a[i] = v + 100 // 使用复制品中取出的 value 修改原数组。\n\n    &#125;\n\n    fmt.Println(a) // 输出 [100, 101, 102]。\n&#125;\n输出结果：\ntxt[0 999 999]\n[100 101 102]建议改用引用类型，其底层数据不会被复制。\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    s := []int&#123;1, 2, 3, 4, 5&#125;\n\n    for i, v := range s &#123; // 复制 struct slice &#123; pointer, len, cap &#125;。\n\n        if i == 0 &#123;\n            s = s[:3]  // 对 slice 的修改，不会影响 range。\n            s[2] = 100 // 对底层数据的修改。\n        &#125;\n\n        fmt.Println(i, v)\n    &#125;\n&#125;\n输出结果:\ntxt0 1\n1 2\n2 100\n3 4\n4 5另外两种引用类型map、channel是指针包装，而不像slice是 struct。\n5.4.5.1 for和for range有什么区别?主要是使用场景不同\nfor可以\n遍历array和slice\n遍历key为整型递增的map\n遍历string\nfor range可以完成所有for可以做的事情，却能做到for不能做的，包括\n遍历key为string类型的map并同时获取key和value\n遍历channel\n5.4.6 break与continue\nbreak与continue用于控制for循环的代码流程，并且只针对最靠近自己的外层for循环。\nbreak：退出for循环，且本轮break下面的代码不再执行。\ncontinue：本轮continue下面的代码不再执行，进入for循环的下一轮。\n\ngo// break和continue都是针对for循环的，不针对if或switch\n// break和continue都是针对套在自己外面的最靠里的那层for循环，不针对更外层的for循环（除非使用Label）\nfunc complex_break_continue() &#123;\n    const SIZE = 5\n    arr := [SIZE][SIZE]int&#123;&#125;\n    for i := 0; i &lt; SIZE; i++ &#123;\n        fmt.Printf(&quot;开始检查第%d行\\n&quot;, i)\n        if i%2 == 1 &#123;\n            for j := 0; j &lt; SIZE; j++ &#123;\n                fmt.Printf(&quot;开始检查第%d列\\n&quot;, j)\n                if arr[i][j]%2 == 0 &#123;\n                    continue // 针对第二层for循环\n                &#125;\n                fmt.Printf(&quot;将要检查第%d列\\n&quot;, j+1)\n            &#125;\n            break // 针对第一层for循环\n        &#125;\n    &#125;\n&#125;5.4.7 goto与Labelgo    var i int = 4\nMY_LABEL:\n    i += 3\n    fmt.Println(i)\n    goto MY_LABEL //返回定义MY_LABEL的那一行，把代码再执行一遍（会进入一个无限循环）go    if i%2 == 0 &#123;\n        goto L1 //Label指示的是某一行代码，并没有圈定一个代码块，所以goto L1也会执行L2后的代码\n    &#125; else &#123;\n        goto L2 //先使用Label\n    &#125;\nL1:\n    i += 3\nL2: //后定义Label。Label定义后必须在代码的某个地方被使用\n    i *= 3goto与Label结合可以实现break的功能，甚至比break更强大。\ngo    for i := 0; i &lt; SIZE; i++ &#123;\n    L2:\n        for j := 0; j &lt; SIZE; j++ &#123;\n            goto L1\n        &#125;\n    &#125;\nL1:\n    xxx\nbreak、continue与Label结合使用可以跳转到更外层的for循环。\ncontinue和break针对的Label必须写在for前面，而goto可以针对任意位置的Label。\n\ngofunc break_label() &#123;\n    const SIZE = 5\n    arr := [SIZE][SIZE]int&#123;&#125;\nL1:\n    for i := 0; i &lt; SIZE; i++ &#123;\n    L2:\n        fmt.Printf(&quot;开始检查第%d行\\n&quot;, i)\n\n        if i%2 == 1 &#123;\n        L3:\n            for j := 0; j &lt; SIZE; j++ &#123;\n                fmt.Printf(&quot;开始检查第%d列\\n&quot;, j)\n                if arr[i][j]%3 == 0 &#123;\n                    break L1 //直接退出最外层的fot循环\n                &#125; else if arr[i][j]%3 == 1 &#123;\n                    goto L2 //continue和break针对的Label必须写在for前面，而goto可以针对任意位置的Label\n                &#125; else &#123;\n                    break L3\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n模块六：结构体6.1 学习目标\n了解结构体的基本概念\n熟练掌握并使用结构体解决相关问题\n了解匿名结构体、结构体指针、结构体方法、嵌套结构体等知识点\n\n6.2 Golang结构体Golang中没有“类”的概念，Golang中的结构体和其他语言中的类有点相似。和其他面向对象语言中的类相比，Golang中的结构体具有更高的扩展性和灵活性。\nGolang中的基础数据类型可以表示一些事物的基本属性，但是当我们想表达一个事物的全部或部分属性时，这时候再用单一的基本数据类型就无法满足需求了，Golang提供了一种自定义数据类型，可以封装多个基本数据类型，这种数据类型叫结构体，英文名称struct。也就是我们可以通过struct来定义自己的类型了。\n6.3 Golang type关键词自定义类型和类型别名Golang中通过type关键词定义一个结构体，在讲解结构体之前，我们首先给大家看看通过type自定义类型以及定义类型别名。\n\ntxt自定义类型 在Go语言中有一些基本的数据类型，如`string`、`整型`、`浮点型`、`布尔`等数据类型， Go语言中可以使用`type`关键字来定义自定义类型\n\ngotype myInt int上面代码表示：将myInt定义为int类型，通过type关键字的定义，myInt就是一种新的类型，它具有int的特性。\n\ntxt类型别名Golang1.9版本以后添加的新功能。 类型别名规定：`TypeAlias`只是`Type`的别名，本质上`TypeAlias`与`Type`是同一个类型。就像 一个孩子小时候有大名、小名、英文名，但这些名字都指的是他本人。\n\ngotype TypeAlias = Type我们之前见过的rune和byte就是类型别名，他们的底层定义如下：\ngotype byte = uint8\ntype rune = int323、自定义类型和类型别名的区别\n类型别名与自定义类型表面上看只有一个等号的差异，我们通过下面的这段代码来理解它们 之间的区别。\ngopackage main\n\nimport &quot;fmt&quot;\n\n// 类型定义\ntype newInt int\n\n// 类型别名\ntype myInt = int\n\nfunc main() &#123;\n    var a newInt\n    var b myInt\n    fmt.Printf(&quot;type of a:%T\\n&quot;, a) //type of a:main.newInt\n    fmt.Printf(&quot;type of b:%T\\n&quot;, b) //type of b:int\n&#125;\n结果显示a的类型是main.newInt，表示main包下定义的newInt类型。b的类型是int类型。\n6.4 结构体的定义gotype 类型名 struct &#123;\n    字段名 字段类型\n    字段名 字段类型\n    …\n&#125;其中：\n\n类型名：标识自定义结构体的名称，在同一个包内不能重复。\n字段名：表示结构体字段名。结构体中的字段名必须唯一。\n字段类型：表示结构体字段的具体类型。\n\n举个例子，我们定义一个Person（人）结构体，代码如下：\ngotype person struct &#123;\n    name string\n    city string\n    age  int8\n&#125;同样类型的字段也可以写在一行，\ngotype person1 struct &#123;\n    name, city string\n    age        int8\n&#125;6.4.1 结构体实例化（第一种方法）只有当结构体实例化时，才会真正地分配内存。也就是必须实例化后才能使用结构体的字段。\n结构体本身也是一种类型，我们可以像声明内置类型一样使用var关键字声明结构体类型。\ngovar 结构体实例 结构体类型gopackage main\n\nimport &quot;fmt&quot;\n\ntype person struct &#123;\n    name string\n    city string\n    age  int\n&#125;\n\nfunc main() &#123;\n    var p1 person\n    p1.name = &quot;绝迹之春&quot;\n    p1.city = &quot;芜湖&quot;\n    p1.age = 18\n    fmt.Printf(&quot;p1=%v\\n&quot;, p1)  // p1=&#123;绝迹之春 芜湖 18&#125;\n    fmt.Printf(&quot;p1=%#v\\n&quot;, p1) // p1=main.person&#123;name:&quot;绝迹之春&quot;, city:&quot;芜湖&quot;, age:18&#125;\n&#125;\n6.4.2 结构体实例化（第二种方法）我们还可以通过使用new关键字对结构体进行实例化，得到的是结构体的地址。\n格式如下：\ngopackage main\n\nimport &quot;fmt&quot;\n\ntype person struct &#123;\n    name string\n    city string\n    age  int\n&#125;\n\nfunc main() &#123;\n    var p2 = new(person)\n    p2.name = &quot;绝迹之春&quot;\n    p2.age = 18\n    p2.city = &quot;芜湖&quot;\n    fmt.Printf(&quot;%T\\n&quot;, p2)     // *main.person\n    fmt.Printf(&quot;p2=%#v\\n&quot;, p2) //p2=&amp;main.person&#123;name:&quot;绝迹之春&quot;, city:&quot;芜湖&quot;, age:18&#125;\n&#125;\n从打印的结果中我们可以看出p2是一个结构体指针。\n注意：在Golang中支持对结构体指针直接使用.来访问结构体的成员。p2.name = &quot;张三&quot;其实在底层是(*p2).name = &quot;张三&quot;\n6.4.3 结构体实例化（第三种方法）使用&amp;对结构体进行取地址操作相当于对该结构体类型进行了一次new实例化操作。\ngopackage main\n\nimport &quot;fmt&quot;\n\ntype person struct &#123;\n    name string\n    city string\n    age  int\n&#125;\n\nfunc main() &#123;\n    p3 := &amp;person&#123;&#125;\n    fmt.Printf(&quot;%T\\n&quot;, p3)     // *main.person\n    fmt.Printf(&quot;p3=%#v\\n&quot;, p3) // p3=&amp;main.person&#123;name:&quot;&quot;, city:&quot;&quot;, age:0&#125;\n    p3.name = &quot;绝迹之春&quot;\n    p3.age = 18\n    p3.city = &quot;芜湖&quot;\n    (*p3).age = 12             // 这样也是可以的\n    fmt.Printf(&quot;p3=%#v\\n&quot;, p3) // p3=&amp;main.person&#123;name:&quot;绝迹之春&quot;, city:&quot;芜湖&quot;, age:12&#125;\n&#125;\n6.4.4 结构体实例化（第四种方法） 键值对初始化gopackage main\n\nimport &quot;fmt&quot;\n\ntype person struct &#123;\n    name string\n    city string\n    age  int\n&#125;\n\nfunc main() &#123;\n    p4 := person&#123;\n        name: &quot;绝迹之春&quot;,\n        city: &quot;芜湖&quot;,\n        age:  18,\n    &#125;\n    fmt.Printf(&quot;p4=%#v\\n&quot;, p4) // p4=main.person&#123;name:&quot;绝迹之春&quot;, city:&quot;芜湖&quot;, age:18&#125;\n&#125;\n注意：最后一个属性的,要加上（Goland好像不会报错，但是不符合语法规范，要么给最后一个属性的后面加上,，要么将&#125;直接写在最后一个属性的后面，如：age: 18&#125;，也许Goland不报错是因为这个的原因）\n6.4.5 结构体实例化（第五种方法） 结构体指针进行键值对初始化gopackage main\n\nimport &quot;fmt&quot;\n\ntype person struct &#123;\n    name string\n    city string\n    age  int\n&#125;\n\nfunc main() &#123;\n    p5 := &amp;person&#123;\n        name: &quot;绝迹之春&quot;,\n        city: &quot;芜湖&quot;,\n        age:  18,\n    &#125;\n    fmt.Printf(&quot;p5=%#v\\n&quot;, p5) // p5=&amp;main.person&#123;name:&quot;绝迹之春&quot;, city:&quot;芜湖&quot;, age:18&#125;\n&#125;\n当某些字段没有初始值的时候，这个字段可以不写。此时，没有指定初始值的字段的值就是该字段类型的零值。\ngopackage main\n\nimport &quot;fmt&quot;\n\ntype person struct &#123;\n    name string\n    city string\n    age  int\n&#125;\n\nfunc main() &#123;\n    p6 := &amp;person&#123;\n        city: &quot;芜湖&quot;,\n    &#125;\n    fmt.Printf(&quot;p6=%#v\\n&quot;, p6) // p6=&amp;main.person&#123;name:&quot;&quot;, city:&quot;芜湖&quot;, age:0&#125;\n&#125;\n6.4.6 结构体实例化（第六种方法） 使用值的列表初始化gopackage main\n\nimport &quot;fmt&quot;\n\ntype person struct &#123;\n    name string\n    city string\n    age  int\n&#125;\n\nfunc main() &#123;\n    // 初始化结构体的时候可以简写，也就是初始化的时候不写键，直接写值：\n    p7 := &amp;person&#123;\n        &quot;绝迹之春&quot;,\n        &quot;芜湖&quot;,\n        18,\n    &#125;\n    fmt.Printf(&quot;p7=%#v\\n&quot;, p7) // p7=&amp;main.person&#123;name:&quot;绝迹之春&quot;, city:&quot;芜湖&quot;, age:18&#125;\n&#125;\n使用这种格式初始化时，需要注意：\n\n必须初始化结构体的所有字段。\n\n初始值的填充顺序必须与字段在结构体中的声明顺序一致。\n\n该方式不能和键值初始化方式混用。\n\n\n6.5 结构体内存布局Go 语言中，结构体和它所包含的数据在内存中是以连续块的形式存在的。\n即使结构体中嵌套有其他的结构体，这在性能上带来了很大的优势。\n（实际上，Golang对结构体的内存处理还有很多细节，如果是不同类型的数据类型，所占内存的大小是不一样的，有时会采用内存对齐这一概念，这里不详细说明，感兴趣的可以去搜一下）\ngotype test struct &#123;\n    a int8\n    b int8\n    c int8\n    d int8\n&#125;\nn := test&#123;\n    1, 2, 3, 4,\n&#125;\nfmt.Printf(&quot;n.a %p\\n&quot;, &amp;n.a)\nfmt.Printf(&quot;n.b %p\\n&quot;, &amp;n.b)\nfmt.Printf(&quot;n.c %p\\n&quot;, &amp;n.c)\nfmt.Printf(&quot;n.d %p\\n&quot;, &amp;n.d)输出结果：\ntxtn.a 0xc00000a0b8\nn.b 0xc00000a0b9\nn.c 0xc00000a0ba\nn.d 0xc00000a0bb6.6 结构体的可见性\nGo语言关于可见的统一规则：大写字母开头跨package也可以访问；否则只能本package内部访问。\n结构体名称以大写开头时，package外部可见，在此前提下，结构体中以大写开头在成员变量或成员方法在package外部也可见。\n\n6.7 匿名结构体govar stu struct &#123; // 声明stu是一个结构体，但这个结构体是匿名的\n    Name string\n    Addr string\n&#125;\nstu.Name = &quot;绝迹之春&quot;\nstu.Addr = &quot;芜湖&quot;匿名结构体通常用于只使用一次的情况。\n6.8 结构体中含有匿名成员结构体允许其成员字段在声明时没有字段名而只有类型，这种没有名字的字段就称为匿名字段。\ngotype Student struct &#123;\n    Id      int\n    string  // 匿名字段\n    float32 // 直接使用数据类型作为字段名，所以匿名字段中不能出现重复的数据类型\n&#125;\nvar stu = Student&#123;Id: 749, string: &quot;绝迹之春&quot;, float32: 425.0&#125;\nfmt.Printf(&quot;anonymous_member string member=%s float member=%f\\n&quot;, stu.string, stu.float32) // 直接使用数据类型访问匿名成员匿名字段默认采用类型名作为字段名，结构体要求字段名称必须唯一，因此一个结构体中同种类型的匿名字段只能有一个。\n6.9 结构体指针由于结构体是值类型，在方法传递时希望传递结构体地址，可以使用结构体指针完成。\ngovar p8 Person\nperson1 := &amp;p8      // 通过取址符&amp;得到指针\nperson2 := &amp;Person&#123; // 直接创建结构体指针\n    Name: &quot;绝迹之春&quot;,\n    city: &quot;anhui&quot;,\n    age:  18,\n&#125;\nperson3 := new(Person) // 通过new()函数实体化一个结构体，并返回其指针代码：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    type Person struct &#123;\n        Name string\n        city string\n        age  int\n    &#125;\n\n    var p8 Person\n    person1 := &amp;p8      // 通过取址符&amp;得到指针\n    person2 := &amp;Person&#123; // 直接创建结构体指针\n        Name: &quot;绝迹之春&quot;,\n        city: &quot;anhui&quot;,\n        age:  18,\n    &#125;\n    person3 := new(Person) // 通过new()函数实体化一个结构体，并返回其指针\n\n    fmt.Println(person1)\n    fmt.Println(person2)\n    fmt.Println(person3)\n&#125;\n6.9.1 构造函数Go语言的结构体没有构造函数，我们可以自己实现。\n例如，下方的代码就实现了一个person的构造函数。 因为struct是值类型，如果结构体比较复杂的话，值拷贝性能开销会比较大，所以该构造函数返回的是结构体指针类型。\ngofunc newPerson(name, city string, age int) *person &#123;\n    return &amp;person&#123;\n        name: name,\n        city: city,\n        age:  age,\n    &#125;\n&#125;调用构造函数\ngop9 := newPerson(&quot;绝迹之春&quot;, &quot;芜湖&quot;, 18)\nfmt.Printf(&quot;%#v\\n&quot;, p9)代码：\ngopackage main\n\nimport &quot;fmt&quot;\n\ntype person struct &#123;\n    name string\n    city string\n    age  int\n&#125;\n\nfunc newPerson(name, city string, age int) *person &#123;\n    return &amp;person&#123;\n        name: name,\n        city: city,\n        age:  age,\n    &#125;\n&#125;\n\nfunc main() &#123;\n    p9 := newPerson(&quot;绝迹之春&quot;, &quot;芜湖&quot;, 18)\n    fmt.Printf(&quot;%#v\\n&quot;, p9)\n&#125;\n6.10 方法和接收者Go语言中的方法（Method）是一种作用于特定类型变量的函数。这种特定类型变量叫做接收者（Receiver）。接收者的概念就类似于其他语言中的this或者self。\n方法的定义格式如下：\ngofunc (接收者变量 接收者类型) 方法名(参数列表) (返回参数) &#123;\n    函数体\n&#125;其中，\n\n接收者变量：接收者中的参数变量名在命名时，官方建议使用接收者类型名的第一个小写字母，而不是self、this之类的命名。例如，Person类型的接收者变量应该命名为p，Connector类型的接收者变量应该命名为c等。\n接收者类型：接收者类型和参数类似，可以是指针类型和非指针类型。\n方法名、参数列表、返回参数：具体格式与函数定义相同。\n\n举个例子：\ngopackage main\n\nimport &quot;fmt&quot;\n\n// Person 结构体\ntype Person struct &#123;\n    name string\n    age  int\n&#125;\n\n// NewPerson 构造函数\nfunc NewPerson(name string, age int) *Person &#123;\n    return &amp;Person&#123;\n        name: name,\n        age:  age,\n    &#125;\n&#125;\n\n// Dream Person做梦的方法\nfunc (p Person) Dream() &#123;\n    fmt.Printf(&quot;%s的梦想是学好Go语言！\\n&quot;, p.name)\n&#125;\n\nfunc main() &#123;\n    p1 := NewPerson(&quot;绝迹之春&quot;, 18)\n    p1.Dream()\n&#125;\n方法与函数的区别是，函数不属于任何类型，方法属于特定的类型。\n6.10.1 指针类型的接收者指针类型的接收者由一个结构体的指针组成，由于指针的特性，调用方法时修改接收者指针的任意成员变量，在方法结束后，修改都是有效的。这种方式就十分接近于其他语言中面向对象中的this或者self。 例如我们为Person添加一个SetAge方法，来修改实例变量的年龄。\ngo// SetAge 设置p的年龄\n// 使用指针接收者\nfunc (p *Person) SetAge(newAge int) &#123;\n    p.age = newAge\n&#125;调用该方法：\ngofunc main() &#123;\n    p1 := NewPerson(&quot;绝迹之春&quot;, 18)\n    fmt.Println(p1.age) // 18\n    p1.SetAge(12)\n    fmt.Println(p1.age) // 12\n&#125;6.10.2 值类型的接收者当方法作用于值类型接收者时，Go语言会在代码运行时将接收者的值复制一份。在值类型接收者的方法中可以获取接收者的成员值，但修改操作只是针对副本，无法修改接收者变量本身。\ngo// SetAge2 设置p的年龄\n// 使用值接收者\nfunc (p Person) SetAge2(newAge int) &#123;\n    p.age = newAge\n&#125;\n\nfunc main() &#123;\n    p1 := NewPerson(&quot;绝迹之春&quot;, 18)\n    p1.Dream()\n    fmt.Println(p1.age) // 18\n    p1.SetAge2(12)      // (*p1).SetAge2(12)\n    fmt.Println(p1.age) // 18\n&#125;6.10.3 什么时候应该使用指针类型接收者\n需要修改接收者中的值。\n接收者是拷贝代价比较大的大对象。\n保证一致性，如果有某个方法使用了指针接收者，那么其他的方法也应该使用指针接收者。\n\n6.10.4 任意类型添加方法在Go语言中，接收者的类型可以是任何类型，不仅仅是结构体，任何类型都可以拥有方法。\n举个例子，我们基于内置的int类型使用type关键字可以定义新的自定义类型，然后为我们的自定义类型添加方法。\ngopackage main\n\nimport &quot;fmt&quot;\n\n// MyInt 将int定义为自定义MyInt类型\ntype MyInt int\n\n// SayHello 为MyInt添加一个SayHello的方法\nfunc (m MyInt) SayHello() &#123;\n    fmt.Println(&quot;Hello, 我是一个int。&quot;)\n&#125;\nfunc main() &#123;\n    var m1 MyInt\n    m1.SayHello() // Hello, 我是一个int。\n    m1 = 100\n    fmt.Printf(&quot;%#v  %T\\n&quot;, m1, m1) //  100  main.MyInt\n&#125;\n注意事项：非本地类型不能定义方法，也就是说我们不能给别的包的类型定义方法。\n6.11 嵌套结构体一个结构体中可以嵌套包含另一个结构体或结构体指针。\ngopackage main\n\nimport &quot;fmt&quot;\n\n// Address 地址结构体\ntype Address struct &#123;\n    Province string\n    City     string\n&#125;\n\n// User 用户结构体\ntype User struct &#123;\n    Name    string\n    Gender  string\n    Address Address\n&#125;\n\nfunc main() &#123;\n    user1 := User&#123;\n        Name:   &quot;绝迹之春&quot;,\n        Gender: &quot;男&quot;,\n        Address: Address&#123;\n            Province: &quot;安徽&quot;,\n            City:     &quot;芜湖&quot;,\n        &#125;,\n    &#125;\n    fmt.Printf(&quot;user1=%#v\\n&quot;, user1) // user1=main.User&#123;Name:&quot;绝迹之春&quot;, Gender:&quot;男&quot;, Address:main.Address&#123;Province:&quot;安徽&quot;, City:&quot;芜湖&quot;&#125;&#125;\n&#125;\n6.11.1 嵌套匿名结构体gopackage main\n\nimport &quot;fmt&quot;\n\n// Address 地址结构体\ntype Address struct &#123;\n    Province string\n    City     string\n&#125;\n\n// User 用户结构体\ntype User struct &#123;\n    Name    string\n    Gender  string\n    Address //匿名结构体\n&#125;\n\nfunc main() &#123;\n    var user2 User\n    user2.Name = &quot;绝迹之春&quot;\n    user2.Gender = &quot;男&quot;\n    user2.Address.Province = &quot;安徽&quot;    // 通过匿名结构体.字段名访问\n    user2.City = &quot;芜湖&quot;                // 直接访问匿名结构体的字段名\n    fmt.Printf(&quot;user2=%#v\\n&quot;, user2) // user2=main.User&#123;Name:&quot;绝迹之春&quot;, Gender:&quot;男&quot;, Address:main.Address&#123;Province:&quot;安徽&quot;, City:&quot;芜湖&quot;&#125;&#125;\n&#125;\n当访问结构体成员时会先在结构体中查找该字段，找不到再去匿名结构体中查找。\n6.11.2 嵌套结构体的字段名冲突嵌套结构体内部可能存在相同的字段名。这个时候为了避免歧义需要指定具体的内嵌结构体的字段。\ngopackage main\n\n// Address 地址结构体\ntype Address struct &#123;\n    Province   string\n    City       string\n    CreateTime string\n&#125;\n\n// Email 邮箱结构体\ntype Email struct &#123;\n    Account    string\n    CreateTime string\n&#125;\n\n// User 用户结构体\ntype User struct &#123;\n    Name   string\n    Gender string\n    Address\n    Email\n&#125;\n\nfunc main() &#123;\n    var user3 User\n    user3.Name = &quot;绝迹之春&quot;\n    user3.Gender = &quot;男&quot;\n    // user3.CreateTime = &quot;2010&quot; // ambiguous selector user3.CreateTime\n    user3.Address.CreateTime = &quot;2010&quot; // 指定Address结构体中的CreateTime\n    user3.Email.CreateTime = &quot;2010&quot;   // 指定Email结构体中的CreateTime\n&#125;\n6.12 结构体的“继承”Go语言中使用结构体也可以实现其他编程语言中面向对象的继承。\ngopackage main\n\nimport &quot;fmt&quot;\n\n// Animal 动物\ntype Animal struct &#123;\n    name string\n&#125;\n\nfunc (a *Animal) move() &#123;\n    fmt.Printf(&quot;%s会动！\\n&quot;, a.name)\n&#125;\n\n// Dog 狗\ntype Dog struct &#123;\n    Feet    int8\n    *Animal // 通过嵌套匿名结构体实现继承\n&#125;\n\nfunc (d *Dog) wang() &#123;\n    fmt.Printf(&quot;%s会汪汪汪~\\n&quot;, d.name)\n&#125;\n\nfunc main() &#123;\n    d1 := &amp;Dog&#123;\n        Feet: 4,\n        Animal: &amp;Animal&#123; // 注意嵌套的是结构体指针\n            name: &quot;狗狗&quot;,\n        &#125;,\n    &#125;\n    d1.wang() // 狗狗会汪汪汪~\n    d1.move() // 狗狗会动！\n&#125;\n6.13 深拷贝与浅拷贝gotype User struct &#123;\n    Name string\n&#125;\ntype Vedio struct &#123;\n    Length int\n    Author User\n&#125;Go语言里的赋值都会发生值拷贝。\n\ngotype User struct &#123;\n    Name string\n&#125;\ntype Vedio struct &#123;\n    Length int\n    Author *User\n&#125;\n\n深拷贝，拷贝的是值，比如Vedio.Length。\n浅拷贝，拷贝的是指针，比如Vedio.Author。\n深拷贝开辟了新的内存空间，修改操作不影响原先的内存。\n浅拷贝指向的还是原来的内存空间，修改操作直接作用在原内存空间上。\n\n传slice，对sclice的3个字段进行了拷贝，拷贝的是底层数组的指针，所以修改底层数组的元素会反应到原数组上。\ngousers := []User&lt;!--swig￼3--&gt;\nfunc update_users(users []User) &#123;\n    users[0].Name = &quot;光绪&quot;\n&#125;拓展：Golang结构体序列化与反序列化1.关于 JSON 数据JSON（JavaScript Object Notation）是一种轻量级的数据交换格式。易于人阅读和编写。同时也易于机器解析和生成。\nJSON的基本格式如下：\njson&#123;\n  &quot;a&quot;: &quot;Hello&quot;,\n  &quot;b&quot;: &quot;World&quot;\n&#125;稍微复杂点的JSON：\njson&#123;\n  &quot;squadName&quot;: &quot;Super hero squad&quot;,\n  &quot;homeTown&quot;: &quot;Metro City&quot;,\n  &quot;formed&quot;: 2016,\n  &quot;secretBase&quot;: &quot;Super tower&quot;,\n  &quot;active&quot;: true,\n  &quot;members&quot;: [\n    &#123;\n      &quot;name&quot;: &quot;Molecule Man&quot;,\n      &quot;age&quot;: 29,\n      &quot;secretIdentity&quot;: &quot;Dan Jukes&quot;,\n      &quot;powers&quot;: [&quot;Radiation resistance&quot;, &quot;Turning tiny&quot;, &quot;Radiation blast&quot;]\n    &#125;,\n    &#123;\n      &quot;name&quot;: &quot;Madame Uppercut&quot;,\n      &quot;age&quot;: 39,\n      &quot;secretIdentity&quot;: &quot;Jane Wilson&quot;,\n      &quot;powers&quot;: [\n        &quot;Million tonne punch&quot;,\n        &quot;Damage resistance&quot;,\n        &quot;Superhuman reflexes&quot;\n      ]\n    &#125;,\n    &#123;\n      &quot;name&quot;: &quot;Eternal Flame&quot;,\n      &quot;age&quot;: 1000000,\n      &quot;secretIdentity&quot;: &quot;Unknown&quot;,\n      &quot;powers&quot;: [\n        &quot;Immortality&quot;,\n        &quot;Heat Immunity&quot;,\n        &quot;Inferno&quot;,\n        &quot;Teleportation&quot;,\n        &quot;Interdimensional travel&quot;\n      ]\n    &#125;\n  ]\n&#125;2.结构体与 JSON 序列化比如我们Golang要给App或者小程序提供Api接口数据，这个时候就需要涉及到结构体和JSON之间的相互转换。\nGolang JSON序列化是指把结构体数据转化成JSON格式的字符串，Golang JSON的反序列化 是指把JSON数据转化成Golang中的结构体对象。\nGolang中的序列化和反序列化主要通过&quot;encoding/json&quot;包中的json.Marshal()和json.Unmarshal()方法实现。\n\ntxt结构体对象转化成`JSON`字符串\n\ngopackage main\n\nimport (\n    &quot;encoding/json&quot;\n    &quot;fmt&quot;\n)\n\ntype Student struct &#123; // 私有属性不能被 json 包访问\n    ID     int\n    Gender string\n    Name   string\n    Sno    string\n&#125;\n\nfunc main() &#123;\n    var s1 = Student&#123;\n        ID:     1,\n        Gender: &quot;男&quot;,\n        Name:   &quot;张三&quot;,\n        Sno:    &quot;s0001&quot;,\n    &#125;\n    fmt.Printf(&quot;%#v\\n&quot;, s1)\n    var s, _ = json.Marshal(s1)\n    jsonStr := string(s)\n    fmt.Println(jsonStr)\n&#125;\n\ntxt`JSON`字符串转换成结构体对象\n\ngopackage main\n\nimport (\n    &quot;encoding/json&quot;\n    &quot;fmt&quot;\n)\n\ntype Student struct &#123;\n    ID     int\n    Gender string\n    Name   string\n    Sno    string\n&#125;\n\nfunc main() &#123;\n    // var jsonStr = &quot;&#123;\\&quot;ID\\&quot;:1,\\&quot;Gender\\&quot;:\\&quot;男\\&quot;,\\&quot;Name\\&quot;:\\&quot;李四\\&quot;,\\&quot;Sno\\&quot;:\\&quot;s0001\\&quot;&#125;&quot;\n    var jsonStr = `&#123;&quot;ID&quot;:1,&quot;Gender&quot;:&quot;男&quot;,&quot;Name&quot;:&quot;李四&quot;,&quot;Sno&quot;:&quot;s0001&quot;&#125;`\n    // 定义一个 Monster 实例\n    var student Student\n    err := json.Unmarshal([]byte(jsonStr), &amp;student)\n    if err != nil &#123;\n        fmt.Printf(&quot;unmarshal err=%v\\n&quot;, err)\n    &#125;\n    fmt.Printf(&quot;反序列化后 student=%#v student.Name=%v \\n&quot;, student, student.Name)\n&#125;\n3.结构体标签 TagTag是结构体的元信息，可以在运行的时候通过反射的机制读取出来。Tag在结构体字段的后方定义，由一对反引号包裹起来，具体的格式如下：\ngo`key1:&quot;value1&quot; key2:&quot;value2&quot;`结构体Tag由一个或多个键值对组成。键与值使用冒号分隔，值用双引号括起来。同一个结 构体字段可以设置多个键值对Tag，不同的键值对之间使用空格分隔。\n注意事项：为结构体编写Tag时，必须严格遵守键值对的规则。结构体标签的解析代码的容错能力很差，一旦格式写错，编译和运行时都不会提示任何错误，通过反射也无法正确取值。例如不要在key和value之间添加空格。\ngopackage main\n\nimport (\n    &quot;encoding/json&quot;\n    &quot;fmt&quot;\n)\n\ntype Student struct &#123;\n    ID     int    `json:&quot;id&quot;` //通过指定 tag 实现 json 序列化该字段时的 key\n    Gender string `json:&quot;gender&quot;`\n    Name   string\n    Sno    string\n&#125;\n\nfunc main() &#123;\n    var s1 = Student&#123;\n        ID: 1, Gender: &quot;男&quot;, Name: &quot;李四&quot;, Sno: &quot;s0001&quot;&#125;\n    fmt.Printf(&quot;%#v\\n&quot;, s1)\n    var s, _ = json.Marshal(s1)\n    jsonStr := string(s)\n    fmt.Println(jsonStr)\n&#125;\ngopackage main\n\nimport (\n    &quot;encoding/json&quot;\n    &quot;fmt&quot;\n)\n\ntype Student struct &#123;\n    ID     int    `json:&quot;id&quot;` //通过指定 tag 实现 json 序列化该字段时的 key\n    Gender string `json:&quot;gender&quot;`\n    Name   string\n    Sno    string\n&#125;\n\nfunc main() &#123;\n    var s2 Student\n    var str = &quot;&#123;\\&quot;id\\&quot;:1,\\&quot;gender\\&quot;:\\&quot;男\\&quot;,\\&quot;Name\\&quot;:\\&quot;李四\\&quot;,\\&quot;Sno\\&quot;:\\&quot;s0001\\&quot;&#125;&quot;\n    err := json.Unmarshal([]byte(str), &amp;s2)\n    if err != nil &#123;\n        fmt.Println(err)\n    &#125;\n    fmt.Printf(&quot;%#v&quot;, s2)\n&#125;\n关于嵌套结构体的序列化与反序列化以及其他相关知识点，这里不详细说明，感兴趣的可以去搜一下。\n\n模块七：函数7.1 学习目标\n了解函数的定义方式\n熟练掌握函数的相关知识点，并可以在复杂的环境下熟练解决相关问题\n了解并熟练掌握defer语句\n掌握基础的Go语言异常处理方式\n\n7.2 函数定义函数是组织好的、可重复使用的、用于执行指定任务的代码块。\nGo语言中定义函数使用func关键字，具体格式如下：\ngofunc 函数名(参数) (返回值) &#123;\n    函数体\n&#125;其中，\n\n函数名：由字母、数字、下划线组成。但函数名的第一个字母不能是数字。在同一个包内，函数名也称不能重名。\n\n参数：参数由参数变量和参数变量的类型组成，多个参数之间使用,分隔。\n\n返回值：返回值由返回值变量和其变量类型组成，也可以只写返回值的类型，多个返回值必须用()包裹，并用,分隔。\n\n函数体：实现指定功能的代码块。\n\n\n定义一个求两个数之和的函数：\ngofunc intSum(x int, y int) int &#123;\n    return x + y\n&#125;函数的参数和返回值都是可选的，例如我们可以实现一个既不需要参数也没有返回值的函数：\ngofunc sayHello() &#123;\n    fmt.Println(&quot;Hello&quot;)\n&#125;7.3 函数调用定义了函数之后，我们可以通过函数名()的方式调用函数。\n例如我们调用上面定义的两个函数，代码如下：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc intSum(x int, y int) int &#123;\n    return x + y\n&#125;\n\nfunc sayHello() &#123;\n    fmt.Println(&quot;Hello&quot;)\n&#125;\n\nfunc main() &#123;\n    sayHello()\n    result := intSum(1, 2)\n    fmt.Println(result)\n&#125;\n注意：调用有返回值的函数时，可以不接收其返回值。\n7.4 函数参数7.4.1 类型简写函数的参数中如果相邻变量的类型相同，则可以省略类型，例如：\ngofunc intSum(x, y int) int &#123;\n    return x + y\n&#125;上面的代码中，intSum函数有两个参数，这两个参数的类型均为int，因此可以省略x的类型，因为y后面有类型说明，x参数也是该类型。\n7.4.2 可变参数可变参数是指函数的参数数量不固定。Go语言中的可变参数通过在参数名后加...来标识。\n注意：可变参数通常要作为函数的最后一个参数。\n举个例子：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc intSum2(x ...int) int &#123;\n    fmt.Println(x) //x 是一个切片\n    sum := 0\n    for _, v := range x &#123;\n        sum = sum + v\n    &#125;\n    return sum\n&#125;\n\nfunc main() &#123;\n    ret1 := intSum2()\n    ret2 := intSum2(10)\n    ret3 := intSum2(10, 20)\n    ret4 := intSum2(10, 20, 30)\n    fmt.Println(ret1, ret2, ret3, ret4) // 0 10 30 60\n&#125;\n固定参数搭配可变参数使用时，可变参数要放在固定参数的后面，示例代码如下：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc intSum3(x int, y ...int) int &#123;\n    fmt.Println(x, y)\n    sum := x\n    for _, v := range y &#123;\n        sum = sum + v\n    &#125;\n    return sum\n&#125;\n\nfunc main() &#123;\n    ret5 := intSum3(100)\n    ret6 := intSum3(100, 10)\n    ret7 := intSum3(100, 10, 20)\n    ret8 := intSum3(100, 10, 20, 30)\n    fmt.Println(ret5, ret6, ret7, ret8) // 100 110 130 160\n&#125;\n本质上，函数的可变参数是通过切片来实现的。\n7.5 函数返回值Go语言中通过return关键字向外输出返回值。\n7.5.1 函数多返回值Go语言中函数支持多返回值，函数如果有多个返回值时必须用()将所有返回值包裹起来。\n举个例子：\ngofunc calc(x, y int) (int, int) &#123;\n    sum := x + y\n    sub := x - y\n    return sum, sub\n&#125;不能用容器对象接收多返回值。只能用多个变量，或_忽略。\n举个例子：\ngopackage main\n\nfunc calc(x, y int) (int, int) &#123;\n    sum := x + y\n    sub := x - y\n    return sum, sub\n&#125;\n\nfunc main() &#123;\n    // s := make([]int, 2)\n    // s = calc(1, 2) // assignment mismatch: 1 variable but calc returns 2 values\n\n    x, _ := calc(1, 2)\n    println(x)\n&#125;\n7.5.2 返回值命名函数定义时可以给返回值命名，并在函数体中直接使用这些变量，最后通过return关键字返回。\n举个例子：\ngofunc calc(x, y int) (sum, sub int) &#123;\n    sum = x + y\n    sub = x - y\n    return\n&#125;7.6 函数变量作用域7.6.1 全局变量全局变量是定义在函数外部的变量，它在程序整个运行周期内都有效。\n在函数中可以访问到全局变量。\ngopackage main\n\nimport &quot;fmt&quot;\n\n// 定义全局变量 num\nvar num int64 = 10\n\nfunc testGlobalVar() &#123;\n    fmt.Printf(&quot;num=%d\\n&quot;, num) // 函数中可以访问全局变量 num\n&#125;\n\nfunc main() &#123;\n    testGlobalVar() // num=10\n&#125;\n7.6.2 局部变量局部变量是函数内部定义的变量。\n\n函数内定义的变量无法在该函数外使用\n\n例如下面的示例代码main()函数中无法使用testLocalVar函数中定义的变量x：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc testLocalVar() &#123;\n    // 定义一个函数局部变量 x,仅在该函数内生效\n    var x int64 = 100\n    fmt.Printf(&quot;x=%d\\n&quot;, x)\n&#125;\n\nfunc main() &#123;\n    testLocalVar()\n    // fmt.Println(x) // 此时无法使用变量 x\n&#125;\n\ntxt如果局部变量和全局变量重名，优先访问局部变量\n\ngopackage main\n\nimport &quot;fmt&quot;\n\n// 定义全局变量 num\nvar num int64 = 10\n\nfunc testNum() &#123;\n    num := 100\n    fmt.Printf(&quot;num=%d\\n&quot;, num) // 函数中优先使用局部变量\n&#125;\n\nfunc main() &#123;\n    testNum() // num=100\n&#125;\n7.7 函数类型与变量7.7.1 定义函数类型我们可以使用type关键字来定义一个函数类型，具体格式如下：\ngotype calculation func(int, int)上面语句定义了一个calculation类型，它是一种函数类型，这种函数接收两个int类型的参 数并且返回一个int类型的返回值。\n简单来说，凡是满足这个条件的函数都是calculation类型的函数，例如下面的add和sub是calculation类型。\ngofunc add(x, y int) int &#123;\n    return x + y\n&#125;\n\nfunc sub(x, y int) int &#123;\n    return x - y\n&#125;add和sub都能赋值给calculation类型的变量。\ngovar c calculation\nc = add7.7.2 函数类型变量我们可以声明函数类型的变量并且为该变量赋值：\ngopackage main\n\nimport &quot;fmt&quot;\n\ntype calculation func(int, int) int\n\nfunc add(x, y int) int &#123;\n    return x + y\n&#125;\n\nfunc sub(x, y int) int &#123;\n    return x - y\n&#125;\n\nfunc main() &#123;\n    var c calculation               // 声明一个 calculation 类型的变量 c\n    c = add                         // 把 add 赋值给 c\n    fmt.Printf(&quot;type of c:%T\\n&quot;, c) // type of c:main.calculation\n    fmt.Println(c(1, 2))            // 像调用 add 一样调用 c\n    f := add                        // 将函数 add 赋值给变量 f1\n    fmt.Printf(&quot;type of f:%T\\n&quot;, f) // type of f:func(int, int) int\n    fmt.Println(f(10, 20))          // 像调用 add 一样调用 f\n&#125;\n7.8 高阶函数高阶函数分为函数作为参数和函数作为返回值两部分。\n7.8.1 函数作为参数函数可以作为参数：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc add(x, y int) int &#123;\n    return x + y\n&#125;\n\nfunc calc(x, y int, op func(int, int) int) int &#123;\n    return op(x, y)\n&#125;\n\nfunc main() &#123;\n    ret2 := calc(10, 20, add)\n    fmt.Println(ret2) // 30\n&#125;\n7.8.2 函数作为返回值函数也可以作为返回值：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc add(x, y int) int &#123;\n    return x + y\n&#125;\n\nfunc sub(x, y int) int &#123;\n    return x - y\n&#125;\n\nfunc do(s string) func(int, int) int &#123;\n    switch s &#123;\n    case &quot;+&quot;:\n        return add\n    case &quot;-&quot;:\n        return sub\n    default:\n        return nil\n    &#125;\n&#125;\n\nfunc main() &#123;\n    var a = do(&quot;+&quot;)\n    fmt.Println(a(10, 20))\n&#125;\n7.9 匿名函数和闭包7.9.1 匿名函数函数当然还可以作为返回值，但是在 Go 语言中函数内部不能再像之前那样定义函数了，只能定义匿名函数。匿名函数就是没有函数名的函数，匿名函数的定义格式如下：\ngofunc(参数) (返回值) &#123;\n    函数体\n&#125;匿名函数因为没有函数名，所以没办法像普通函数那样调用，所以匿名函数需要保存到某个变量或者作为立即执行函数:\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    // 将匿名函数保存到变量\n    add := func(x, y int) &#123;\n        fmt.Println(x + y)\n    &#125;\n    add(10, 20) // 通过变量调用匿名函数\n    // 自执行函数：匿名函数定义完加()直接执行\n    func(x, y int) &#123;\n        fmt.Println(x + y)\n    &#125;(10, 20)\n&#125;\n匿名函数多用于实现回调函数和闭包。\n7.9.2 闭包闭包可以理解成”定义在一个函数内部的函数“。在本质上，闭包是将函数内部和函数外部连接起来的桥梁。或者说是函数和其引用环境的组合体。\n首先我们来看一个例子：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc adder() func(int) int &#123;\n    var x int\n    return func(y int) int &#123;\n        x += y\n        return x\n    &#125;\n&#125;\n\nfunc main() &#123;\n    var f = adder()\n    fmt.Println(f(10)) // 10\n    fmt.Println(f(20)) // 30\n    fmt.Println(f(30)) // 60\n    f1 := adder()\n    fmt.Println(f1(40)) // 40\n    fmt.Println(f1(50)) // 90\n&#125;\n变量f是一个函数并且它引用了其外部作用域中的x变量，此时f就是一个闭包。 在f的生命周期内，变量x也一直有效。\n7.9.2.1 闭包进阶示例 1gopackage main\n\nimport &quot;fmt&quot;\n\nfunc adder2(x int) func(int) int &#123;\n    return func(y int) int &#123;\n        x += y\n        return x\n    &#125;\n&#125;\n\nfunc main() &#123;\n    var f = adder2(10)\n    fmt.Println(f(10)) // 20\n    fmt.Println(f(20)) // 40\n    fmt.Println(f(30)) // 70\n    f1 := adder2(20)\n    fmt.Println(f1(40)) // 60\n    fmt.Println(f1(50)) // 110\n&#125;\n7.9.2.2 闭包进阶示例 2gopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;strings&quot;\n)\n\nfunc makeSuffixFunc(suffix string) func(string) string &#123;\n    return func(name string) string &#123;\n        if !strings.HasSuffix(name, suffix) &#123;\n            return name + suffix\n        &#125;\n        return name\n    &#125;\n&#125;\n\nfunc main() &#123;\n    jpgFunc := makeSuffixFunc(&quot;.jpg&quot;)\n    txtFunc := makeSuffixFunc(&quot;.txt&quot;)\n    fmt.Println(jpgFunc(&quot;test&quot;)) // test.jpg\n    fmt.Println(txtFunc(&quot;test&quot;)) // test.txt\n&#125;\n7.9.2.3 闭包进阶示例 3gopackage main\n\nimport &quot;fmt&quot;\n\nfunc calc(base int) (func(int) int, func(int) int) &#123;\n    add := func(i int) int &#123;\n        base += i\n        return base\n    &#125;\n    sub := func(i int) int &#123;\n        base -= i\n        return base\n    &#125;\n    return add, sub\n&#125;\n\nfunc main() &#123;\n    f1, f2 := calc(10)\n    fmt.Println(f1(1), f2(2)) // 11 9\n    fmt.Println(f1(3), f2(4)) // 12 8\n    fmt.Println(f1(5), f2(6)) // 13 7\n&#125;\n闭包其实并不复杂，只要牢记闭包&#x3D;函数+引用环境\n7.10 defer 语句简介：\n\ndefer用于注册一个延迟调用（在函数返回之前调用）。\ndefer典型的应用场景是释放资源，比如关闭文件句柄，释放数据库连接等。\n如果同一个函数里有多个defer，则后注册的先执行。\ndefer后可以跟一个func，func内部如果发生panic，会把panic暂时搁置，当把其他defer执行完之后再来执行这个。\ndefer后不是跟func，而直接跟一条执行语句，则相关变量在注册defer时被拷贝或计算。\n\ngofunc basic() &#123;\n    fmt.Println(&quot;A&quot;)\n    defer fmt.Println(1)\n    fmt.Println(&quot;B&quot;)\n    // 如果同一个函数里有多个defer，则后注册的先执行\n    defer fmt.Println(2)\n    fmt.Println(&quot;C&quot;)\n&#125;gofunc defer_exe_time() (i int) &#123;\n    i = 9\n    defer func() &#123; // defer后可以跟一个func\n        fmt.Printf(&quot;first i=%d\\n&quot;, i) // 打印5，而非9。充分理解“defer在函数返回前执行”的含义，不是在“return语句前执行defer”\n    &#125;()\n    defer func(i int) &#123;\n        fmt.Printf(&quot;second i=%d\\n&quot;, i) // 打印9\n    &#125;(i)\n    defer fmt.Printf(&quot;third i=%d\\n&quot;, i) // defer后不是跟func，而直接跟一条执行语句，则相关变量在注册defer时被拷贝或计算\n    return 5\n&#125;由于defer语句延迟调用的特性，所以defer语句能非常方便的处理资源释放问题。比如： 资源清理、文件关闭、解锁及记录时间等。\n7.10.1 规则一：延迟函数的参数在defer语句出现时就已经确定下来了官方给出一个例子，如下所示：\ngofunc a() &#123;\n    i := 0\n    defer fmt.Println(i)\n    i++\n    return\n&#125;defer语句中的fmt.Println()参数i值在defer出现时就已经确定下来，实际上是拷贝了一份。后面对变量i的修改不会影响fmt.Println()函数的执行，仍然打印”0”。\n注意：对于指针类型参数，规则仍然适用，只不过延迟函数的参数是一个地址值，这种情况下，defer后面的语句对变量的修改可能会影响延迟函数。\n7.10.2 规则二：延迟函数执行按后进先出顺序执行，即先出现的defer最后执行这个规则很好理解，定义defer类似于入栈操作，执行defer类似于出栈操作。\n设计defer的初衷是简化函数返回时资源清理的动作，资源往往有依赖顺序，比如先申请A资源，再根据A资源申请B资源，根据B资源申请C资源，即申请顺序是：A–&gt;B–&gt;C，释放时往往又要反向进行。这就是把defer设计成LIFO的原因。\n每申请到一个用完需要释放的资源时，立即定义一个defer来释放资源是个很好的习惯。\n7.10.3 规则三：延迟函数可能操作主函数的具名返回值定义defer的函数，即主函数可能有返回值，返回值有没有名字没有关系，defer所作用的函数，即延迟函数可能会影响到返回值。\n若要理解延迟函数是如何影响主函数返回值的，只要明白函数是如何返回的就足够了。\n7.10.3.1 函数返回过程有一个事实必须要了解，关键字return不是一个原子操作，实际上return只代理汇编指令ret，即将跳转程序执行。比如语句return i，实际上分两步进行，即将i值存入栈中作为返回值，然后执行跳转，而defer的执行时机正是跳转前，所以说defer执行时还是有机会操作返回值的。\n\n举个实际的例子进行说明这个过程：\ngofunc deferFuncReturn() (result int) &#123;\n    i := 1\n\n    defer func() &#123;\n        result++\n    &#125;()\n\n    return i\n&#125;该函数的return语句可以拆分成下面两行：\ngoresult = i\nreturn而延迟函数的执行正是在return之前，即加入defer后的执行过程如下：\ngoresult = i\nresult++\nreturn所以上面函数实际返回i++值。\n关于主函数有不同的返回方式，但返回机制就如上机介绍所说，只要把return语句拆开都可以很好的理解，下面分别举例说明\n7.10.3.2 主函数拥有匿名返回值，返回字面值一个主函数拥有一个匿名的返回值，返回时使用字面值，比如返回”1”、”2”、”Hello”这样的值，这种情况下defer语句是无法操作返回值的。\n一个返回字面值的函数，如下所示：\ngofunc foo() int &#123;\n    var i int\n\n    defer func() &#123;\n        i++\n    &#125;()\n\n    return 1\n&#125;上面的return语句，直接把1写入栈中作为返回值，延迟函数无法操作该返回值，所以就无法影响返回值。\n7.10.3.3 主函数拥有匿名返回值，返回变量一个主函数拥有一个匿名的返回值，返回使用本地或全局变量，这种情况下defer语句可以引用到返回值，但不会改变返回值。\n一个返回本地变量的函数，如下所示：\ngofunc foo() int &#123;\n    var i int\n\n    defer func() &#123;\n        i++\n    &#125;()\n\n    return i\n&#125;上面的函数，返回一个局部变量，同时defer函数也会操作这个局部变量。对于匿名返回值来说，可以假定仍然有一个变量存储返回值，假定返回值变量为anony，上面的返回语句可以拆分成以下过程：\ngoanony = i\ni++\nreturn由于i是整型，会将值拷贝给anony，所以defer语句中修改i值，对函数返回值不造成影响。\n7.10.3.4 主函数拥有具名返回值主函声明语句中带名字的返回值，会被初始化成一个局部变量，函数内部可以像使用局部变量一样使用该返回值。如果defer语句操作该返回值，可能会改变返回结果。\n一个影响函返回值的例子：\ngofunc foo() (ret int) &#123;\n    defer func() &#123;\n        ret++\n    &#125;()\n\n    return 0\n&#125;上面的函数拆解出来，如下所示：\ngoret = 0\nret++\nreturn函数真正返回前，在defer中对返回值做了+1操作，所以函数最终返回1。\n7.11 异常处理Go语言没有try catch，它提倡返回error。\ngopackage main\n\nimport (\n    &quot;errors&quot;\n    &quot;fmt&quot;\n)\n\nfunc divide(a, b int) (int, error) &#123;\n    if b == 0 &#123;\n        return -1, errors.New(&quot;divide by zero&quot;)\n    &#125;\n    return a / b, nil\n&#125;\n\nfunc main() &#123;\n    if res, err := divide(3, 0); err != nil &#123; // 函数调用方判断error是否为nil\n        fmt.Println(err.Error())\n        fmt.Println(res)\n    &#125;\n&#125;\nGo语言定义了error这个接口，自定义的error要实现Error()方法。\ngotype PathError struct &#123; // 自定义error\n    path       string\n    op         string\n    createTime string\n    message    string\n&#125;\n\nfunc (err PathError) Error() string &#123; // error接口要求实现Error() string方法\n    return err.createTime + &quot;: &quot; + err.op + &quot; &quot; + err.path + &quot; &quot; + err.message\n&#125;7.11.1 内置函数 panic&#x2F;recove\n\n\n内置函数\n介绍\n\n\n\nclose\n主要用来关闭 channel\n\n\nlen\n用来求长度，比如 string、array、slice、map、channel\n\n\nnew\n用来分配内存，主要用来分配值类型，比如 int、struct，返回的是指针\n\n\nmake\n用来分配内存，主要用来分配引用类型，比如 chan、map、slice\n\n\nappend\n用来追加元素到数组、slice 中\n\n\npanic 和 recover\n用来做错误处理\n\n\n何时会发生panic:\n\n运行时错误会导致panic，比如数组越界、除0。\n程序主动调用panic(error)。\n\npanic会执行什么：\n\n逆序执行当前goroutine的defer链（recover从这里介入）。\n打印错误信息和调用堆栈。\n调用exit(2)结束整个进程。\n\n7.11.1.1 panic&#x2F;recover 的基本使用gopackage main\n\nimport &quot;fmt&quot;\n\nfunc funcA() &#123;\n    fmt.Println(&quot;func A&quot;)\n&#125;\n\nfunc funcB() &#123;\n    panic(&quot;panic in B&quot;)\n&#125;\n\nfunc funcC() &#123;\n    fmt.Println(&quot;func C&quot;)\n&#125;\n\nfunc main() &#123;\n    funcA()\n    funcB()\n    funcC()\n&#125;\n输出结果：\ntxtfunc A\npanic: panic in B\n\ngoroutine 1 [running]:\nmain.funcB(...)\n        D:/go_project/src/day1/main.go:10\nmain.main()\n        D:/go_project/src/day1/main.go:19 +0x65\nexit status 2程序运行期间funcB中引发了panic导致程序崩溃，异常退出了。这个时候我们就可以通过recover将程序恢复回来，继续往后执行。\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc funcA() &#123;\n    fmt.Println(&quot;func A&quot;)\n&#125;\n\nfunc funcB() &#123;\n    defer func() &#123;\n        err := recover()\n        //如果程序出出现了 panic 错误,可以通过 recover 恢复过来\n        if err != nil &#123;\n            fmt.Println(&quot;recover in B&quot;)\n        &#125;\n    &#125;()\n    panic(&quot;panic in B&quot;)\n&#125;\n\nfunc funcC() &#123;\n    fmt.Println(&quot;func C&quot;)\n&#125;\n\nfunc main() &#123;\n    funcA()\n    funcB()\n    funcC()\n&#125;\n输出结果：\ntxtfunc A\nrecover in B\nfunc C注意：\n\nrecover()必须搭配defer使用。\ndefer一定要在可能引发panic的语句之前定义。\n\n7.11.1.2 defer 、recover 实现异常处理gopackage main\n\nimport &quot;fmt&quot;\n\nfunc fn2() &#123;\n    defer func() &#123;\n        err := recover()\n        if err != nil &#123;\n            fmt.Println(&quot;抛出异常给管理员发送邮件&quot;)\n            fmt.Println(err)\n        &#125;\n    &#125;()\n    num1 := 10\n    num2 := 0\n    res := num1 / num2\n    fmt.Println(&quot;res=&quot;, res)\n&#125;\n\nfunc main() &#123;\n    fn2()\n&#125;\n输出结果：\ntxt抛出异常给管理员发送邮件\nruntime error: integer divide by zero7.11.1.3 defer 、panic、recover 抛出异常gopackage main\n\nimport (\n    &quot;errors&quot;\n    &quot;fmt&quot;\n)\n\nfunc readFile(fileName string) error &#123;\n    if fileName == &quot;main.go&quot; &#123;\n        return nil\n    &#125;\n    return errors.New(&quot;读取文件错误&quot;)\n&#125;\n\nfunc fn3() &#123;\n    defer func() &#123;\n        err := recover()\n        if err != nil &#123;\n            fmt.Println(&quot;抛出异常给管理员发送邮件&quot;)\n        &#125;\n    &#125;()\n    var err = readFile(&quot;xxx.go&quot;)\n    if err != nil &#123;\n        panic(err)\n    &#125;\n    fmt.Println(&quot;继续执行&quot;)\n&#125;\n\nfunc main() &#123;\n    fn3()\n&#125;\n输出结果：\ntxt抛出异常给管理员发送邮件\n模块八：面向对象8.1 学习目标\n了解接口的定义，并熟练掌握及使用\n了解面向对象的思想\n学会使用面向对象的思想解决具体问题\n了解泛型的概念\n\n8.2 接口8.2.1 接口的介绍Golang中的接口是一种抽象数据类型，Golang中接口定义了对象的行为规范，只定义规范不实现。接口中定义的规范由具体的对象来实现。\n通俗的讲接口就一个标准，它是对一个对象的行为和规范进行约定，约定实现接口的对象必须得按照接口的规范。\n8.2.2 接口的定义在Golang中接口（interface）是一种类型，一种抽象的类型。接口（interface）是一组函数method的集合，Golang中的接口不能包含任何变量。\n在Golang中接口中的所有方法都没有方法体，接口定义了一个对象的行为规范，只定 义规范不实现。接口体现了程序设计的多态和高内聚低耦合的思想。\nGolang中的接口也是一种数据类型，不需要显示实现。只需要一个变量含有接口类型中的所有方法，那么这个变量就实现了这个接口。\nGolang中每个接口由数个方法组成，接口的定义格式如下：\ngotype 接口名 interface &#123;\n    方法名1(参数列表1) 返回值列表1\n    方法名2(参数列表2) 返回值列表2\n    …\n&#125;其中：\n\n接口名：使用type将接口定义为自定义的类型名。Go语言的接口在命名时，一般会在单词后面添加er，如有写操作的接口叫Writer，有字符串功能的接口叫Stringer等。接口名最好要能突出该接口的类型含义。\n\n方法名：当方法名首字母是大写且这个接口类型名首字母也是大写时，这个方法可以被接口所在的包（package）之外的代码访问。\n\n参数列表、返回值列表：参数列表和返回值列表中的参数变量名可以省略。\n\n\n定义一个Usber接口让Phone和Camera结构体实现这个接口：\ngopackage main\n\nimport &quot;fmt&quot;\n\ntype Usber interface &#123;\n    Start()\n    Stop()\n&#125;\n\ntype Phone struct &#123;\n    Name string\n&#125;\n\nfunc (p Phone) Start() &#123;\n    fmt.Println(p.Name, &quot;开始工作&quot;)\n&#125;\nfunc (p Phone) Stop() &#123;\n    fmt.Println(&quot;phone 停止&quot;)\n&#125;\n\ntype Camera struct &#123;\n&#125;\n\nfunc (c Camera) Start() &#123;\n    fmt.Println(&quot;相机 开始工作&quot;)\n&#125;\nfunc (c Camera) Stop() &#123;\n    fmt.Println(&quot;相机 停止工作&quot;)\n&#125;\n\nfunc main() &#123;\n    phone := Phone&#123;\n        Name: &quot;小米手机&quot;,\n    &#125;\n    var p Usber = phone // phone 实现了 Usb 接口\n    p.Start()\n    camera := Camera&#123;&#125;\n    var c Usber = camera // camera 实现了 Usb 接口\n    c.Start()\n&#125;\nComputer结构体中的Work方法必须传入一个Usb的接口：\ngopackage main\n\nimport &quot;fmt&quot;\n\ntype Usber interface &#123;\n    Start()\n    Stop()\n&#125;\n\ntype Phone struct &#123;\n    Name string\n&#125;\n\nfunc (p Phone) Start() &#123;\n    fmt.Println(p.Name, &quot;开始工作&quot;)\n&#125;\nfunc (p Phone) Stop() &#123;\n    fmt.Println(&quot;phone 停止&quot;)\n&#125;\n\ntype Camera struct &#123;\n&#125;\n\nfunc (c Camera) Start() &#123;\n    fmt.Println(&quot;相机 开始工作&quot;)\n&#125;\nfunc (c Camera) Stop() &#123;\n    fmt.Println(&quot;相机 停止工作&quot;)\n&#125;\n\n// 电脑的结构体\ntype Computer struct &#123;\n    Name string\n&#125;\n\n// 电脑的 Work 方法要求必须传入 Usb 接口类型数据\nfunc (c Computer) Work(usb Usber) &#123;\n    usb.Start()\n    usb.Stop()\n&#125;\n\nfunc main() &#123;\n    phone := Phone&#123;\n        Name: &quot;小米手机&quot;&#125;\n    camera := Camera&#123;&#125;\n    computer := Computer&#123;&#125;\n    // 把手机插入电脑的 Usb 接口开始工作\n    computer.Work(phone)\n    // 把相机插入电脑的 Usb 接口开始工作\n    computer.Work(camera)\n&#125;\n8.2.3 空接口Golang中的接口可以不定义任何方法，没有定义任何方法的接口就是空接口。空接口表示没有任何约束，因此任何类型变量都可以实现空接口。\n空接口在实际项目中用的是非常多的，用空接口可以表示任意数据类型。\n空接口类型用interface&#123;&#125;表示，注意有&#123;&#125;。\ngovar i interface&#123;&#125;举例：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    // 定义一个空接口 x, x 变量可以接收任意的数据类型\n    var x interface&#123;&#125;\n    s := &quot;你好 golang&quot;\n    x = s\n    fmt.Printf(&quot;type:%T value:%v\\n&quot;, x, x)\n    i := 100\n    x = i\n    fmt.Printf(&quot;type:%T value:%v\\n&quot;, x, x)\n    b := true\n    x = b\n    fmt.Printf(&quot;type:%T value:%v\\n&quot;, x, x)\n&#125;\n8.2.3.1 空接口作为函数的参数使用空接口实现可以接收任意类型的函数参数\ngo// 空接口作为函数参数\nfunc show(a interface&#123;&#125;) &#123;\n    fmt.Printf(&quot;type:%T value:%v\\n&quot;, a, a)\n&#125;8.2.3.2 map的值实现空接口使用空接口实现可以保存任意值的字典：\ngo    // 空接口作为 map 值\n    var studentInfo = make(map[string]interface&#123;&#125;)\n    studentInfo[&quot;name&quot;] = &quot;绝迹之春&quot;\n    studentInfo[&quot;age&quot;] = 18\n    studentInfo[&quot;married&quot;] = false\n    fmt.Println(studentInfo)8.2.3.3 切片实现空接口go    var slice = []interface&#123;&#125;&#123;&quot;张三&quot;, 20, true, 32.2&#125;\n    fmt.Println(slice)8.2.4 类型断言一个接口的值（简称接口值）是由一个具体类型和具体类型的值两部分组成的。这两部分分别称为接口的动态类型和动态值。\n如果我们想要判断空接口中值的类型，那么这个时候就可以使用类型断言，其语法格式：\ngo    x.(T) 其中：\ntxt    x：表示类型为interface&#123;&#125;的变量\n    T：表示断言x可能是的类型。该语法返回两个参数，第一个参数是x转化为T类型后的变量，第二个值是一个布尔值，若为true则表示断言成功，为false则表示断言失败。\n举个例子：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    var x interface&#123;&#125;\n    x = &quot;pprof.cn&quot;\n    v, ok := x.(string)\n    if ok &#123;\n        fmt.Println(v)\n    &#125; else &#123;\n        fmt.Println(&quot;类型断言失败&quot;)\n    &#125;\n&#125;\n上面的示例中如果要断言多次就需要写多个if判断，这个时候我们可以使用switch语句来实现：\ngofunc justifyType(x interface&#123;&#125;) &#123;\n    switch v := x.(type) &#123;\n    case string:\n        fmt.Printf(&quot;x is a string，value is %v\\n&quot;, v)\n    case int:\n        fmt.Printf(&quot;x is a int is %v\\n&quot;, v)\n    case bool:\n        fmt.Printf(&quot;x is a bool is %v\\n&quot;, v)\n    default:\n        fmt.Println(&quot;unsupport type！&quot;)\n    &#125;\n&#125;因为空接口可以存储任意类型值的特点，所以空接口在Go语言中的使用十分广泛。\n关于接口需要注意的是，只有当有两个或两个以上的具体类型必须以相同的方式进行处理时才需要定义接口。不要为了接口而写接口，那样只会增加不必要的抽象，导致不必要的运行时损耗。\n8.2.5 值接收者和指针接收者实现接口的区别使用值接收者实现接口和使用指针接收者实现接口有什么区别呢？接下来我们通过一个例子看一下其中的区别。\n我们有一个Mover接口和一个dog结构体。\ngotype Mover interface &#123;\n    move()\n&#125;\n\ntype dog struct&#123;&#125;8.2.5.1 值接收者实现接口gofunc (d dog) move() &#123;\n    fmt.Println(&quot;狗会动&quot;)\n&#125;此时实现接口的是dog类型：\ngofunc main() &#123;\n    var x Mover\n    var wangcai = dog&#123;&#125; // 旺财是dog类型\n    x = wangcai         // x可以接收dog类型\n    var fugui = &amp;dog&#123;&#125;  // 富贵是*dog类型\n    x = fugui           // x可以接收*dog类型\n    x.move()\n&#125;从上面的代码中我们可以发现，使用值接收者实现接口之后，不管是dog结构体还是结构体指针*dog类型的变量都可以赋值给该接口变量。因为Go语言中有对指针类型变量求值的语法糖，dog指针fugui内部会自动求值*fugui。\n8.2.5.2 指针接收者实现接口同样的代码我们再来测试一下使用指针接收者有什么区别：\ngofunc (d *dog) move() &#123;\n    fmt.Println(&quot;狗会动&quot;)\n&#125;\n\nfunc main() &#123;\n    var x Mover\n    /*\n        var wangcai = dog&#123;&#125; // 旺财是dog类型\n        x = wangcai         // x不可以接收dog类型\n    */\n    var fugui = &amp;dog&#123;&#125; // 富贵是*dog类型\n    x = fugui          // x可以接收*dog类型\n    x.move()\n&#125;此时实现Mover接口的是*dog类型，所以不能给x传入dog类型的wangcai，此时x只能存储*dog类型的值。\n8.2.6 类型与接口的关系8.2.6.1 一个类型实现多个接口一个类型可以同时实现多个接口，而接口间彼此独立，不知道对方的实现。 例如，狗可以叫，也可以动。我们就分别定义Sayer接口和Mover接口，如下： Mover接口。\ngo// Sayer 接口\ntype Sayer interface &#123;\n    say()\n&#125;\n\n// Mover 接口\ntype Mover interface &#123;\n    move()\n&#125;dog既可以实现Sayer接口，也可以实现Mover接口。\ngotype dog struct &#123;\n    name string\n&#125;\n\n// 实现Sayer接口\nfunc (d dog) say() &#123;\n    fmt.Printf(&quot;%s会叫汪汪汪\\n&quot;, d.name)\n&#125;\n\n// 实现Mover接口\nfunc (d dog) move() &#123;\n    fmt.Printf(&quot;%s会动\\n&quot;, d.name)\n&#125;\n\nfunc main() &#123;\n    var x Sayer\n    var y Mover\n\n    var a = dog&#123;name: &quot;旺财&quot;&#125;\n    x = a\n    y = a\n    x.say()\n    y.move()\n&#125;8.2.6.2 多个类型实现同一接口Go语言中不同的类型还可以实现同一接口首先我们定义一个Mover接口，它要求必须由一个move方法。\ngo// Mover 接口\ntype Mover interface &#123;\n    move()\n&#125;例如狗可以动，汽车也可以动，可以使用如下代码实现这个关系：\ngotype dog struct &#123;\n    name string\n&#125;\n\ntype car struct &#123;\n    brand string\n&#125;\n\n// dog类型实现Mover接口\nfunc (d dog) move() &#123;\n    fmt.Printf(&quot;%s会跑\\n&quot;, d.name)\n&#125;\n\n// car类型实现Mover接口\nfunc (c car) move() &#123;\n    fmt.Printf(&quot;%s速度70迈\\n&quot;, c.brand)\n&#125;这个时候我们在代码中就可以把狗和汽车当成一个会动的物体来处理了，不再需要关注它们具体是什么，只需要调用它们的move方法就可以了。\ngofunc main() &#123;\n    var x Mover\n    var a = dog&#123;name: &quot;旺财&quot;&#125;\n    var b = car&#123;brand: &quot;保时捷&quot;&#125;\n    x = a\n    x.move()\n    x = b\n    x.move()\n&#125;上面的代码执行结果如下：\ntxt旺财会跑\n保时捷速度70迈并且一个接口的方法，不一定需要由一个类型完全实现，接口的方法可以通过在类型中嵌入其他类型或者结构体来实现。\ngo// WashingMachine 洗衣机\ntype WashingMachine interface &#123;\n    wash()\n    dry()\n&#125;\n\n// 甩干器\ntype dryer struct&#123;&#125;\n\n// 实现WashingMachine接口的dry()方法\nfunc (d dryer) dry() &#123;\n    fmt.Println(&quot;甩一甩&quot;)\n&#125;\n\n// 海尔洗衣机\ntype haier struct &#123;\n    dryer //嵌入甩干器\n&#125;\n\n// 实现WashingMachine接口的wash()方法\nfunc (h haier) wash() &#123;\n    fmt.Println(&quot;洗刷刷&quot;)\n&#125;8.2.7 接口嵌套接口与接口间可以通过嵌套创造出新的接口：\ngo// Sayer 接口\ntype Sayer interface &#123;\n    say()\n&#125;\n\n// Mover 接口\ntype Mover interface &#123;\n    move()\n&#125;\n\n// 接口嵌套\ntype animal interface &#123;\n    Sayer\n    Mover\n&#125;嵌套得到的接口的使用与普通接口一样，这里我们让cat实现animal接口：\ngotype cat struct &#123;\n    name string\n&#125;\n\nfunc (c cat) say() &#123;\n    fmt.Println(&quot;喵喵喵&quot;)\n&#125;\n\nfunc (c cat) move() &#123;\n    fmt.Println(&quot;猫会动&quot;)\n&#125;\n\nfunc main() &#123;\n    var x animal\n    x = cat&#123;name: &quot;花花&quot;&#125;\n    x.move()\n    x.say()\n&#125;8.3 面向对象8.3.1 面向对象的概念洗衣服过程剖析：\n\n给洗衣机里加脏衣服和洗衣粉。\n启动洗衣机。\n洗衣机自动注水，然后滚动。\n脏衣服从黑颜色变成白颜色。\n洗衣机自动停止。\n\n用面向过程的思想实现代码：\ngo// 准备洗衣服\n// 输入参数：\n// powder 洗衣机里放多少洗衣粉\n// closes 洗衣机里放多少衣服\n// clean 衣服是否是干净的\n// 返回值：\n// 洗衣机是否开启\n// 准备洗多少衣服\nfunc prepare(powder int, closes int, clean bool) (bool, int) &#123;\n    if powder &lt;= 0 || closes &lt;= 0 || clean == true &#123;\n        return false, 0\n    &#125;\n    return true, closes\n&#125;\n\n// 开始洗衣服\n// 输入参数：\n// washer_state 洗衣机是否开启\n// closes 准备洗多少衣服\n// 返回值：\n// 衣服是否是干净的\n// 洗了多少衣服\n// 洗衣机是否开启\nfunc wash(washer_state bool, closes int) (bool, int, bool) &#123;\n    if washer_state == false &#123;\n        return false, 0, false\n    &#125; else &#123;\n        fmt.Println(&quot;注水&quot;)\n        fmt.Println(&quot;滚动&quot;)\n        fmt.Println(&quot;关机&quot;)\n        return true, closes, false\n    &#125;\n&#125;\n\n// 检查最终状态\n// 输入参数：\n// clean 衣服是否是干净的\n// closes 洗了多少衣服\n// washer_state 洗衣机是否开启\nfunc check(clean bool, closes int, washer_state bool) &#123;\n    if clean &amp;&amp; closes &gt; 0 &#123;\n        fmt.Printf(&quot;洗干净了%d件衣服\\n&quot;, closes)\n        if washer_state &#123;\n            fmt.Println(&quot;你忘关洗衣机了&quot;)\n        &#125;\n    &#125; else &#123;\n        fmt.Println(&quot;洗衣失败&quot;)\n    &#125;\n&#125;\n\n// 整个洗衣服的过程\nfunc WashProcedure(powder, closes int) &#123;\n    washer_state := false\n    clean := false\n\n    washer_state, closes = prepare(powder, closes, clean)\n    clean, closes, washer_state = wash(washer_state, closes)\n    check(clean, closes, washer_state)\n&#125;面向过程编程整个过程分为若干步，每一步对应一个函数，函数之间要传递大量的参数。面向对象编程把大量参数封装到一个结构体里面，给结构体赋予方法，方法里面去修改结构体的成员变量。Go语言面向对象的好处：打包参数，继承，面向接口编程。\ngo// 洗衣机\ntype Washer struct &#123;\n    State  bool\n    Powder int\n&#125;\n\n// 衣服\ntype Closes struct &#123;\n    Clean bool\n&#125;\n\nfunc (washer *Washer) prepare(closes []*Closes) error &#123;\n    if washer.State == true || washer.Powder &lt;= 0 || len(closes) &lt;= 0 &#123;\n        return errors.New(&quot;请确保在关机的状态下加入适量衣物和洗衣粉&quot;)\n    &#125;\n    return nil\n&#125;\n\nfunc (washer *Washer) wash(closes []*Closes) error &#123;\n    if err := washer.prepare(closes); err != nil &#123;\n        return err\n    &#125;\n\n    fmt.Println(&quot;开机&quot;)\n    washer.State = true\n\n    // 检查是否有脏衣服\n    clean := true\n    for _, ele := range closes &#123;\n        if ele.Clean == false &#123;\n            clean = false\n            break\n        &#125;\n    &#125;\n    if clean &#123;\n        washer.State = false\n        return errors.New(&quot;所有衣服都是干净的，不需要洗&quot;)\n    &#125;\n\n    // 开始洗衣服\n    fmt.Println(&quot;注水&quot;)\n    fmt.Println(&quot;滚动&quot;)\n    fmt.Println(&quot;关机&quot;)\n    washer.State = false\n    for _, ele := range closes &#123;\n        ele.Clean = true\n    &#125;\n    return nil\n&#125;\n\nfunc (washer *Washer) check(err error, closes []*Closes) &#123;\n    if err != nil &#123;\n        fmt.Printf(&quot;洗衣失败:%v\\n&quot;, err)\n    &#125; else &#123;\n        fmt.Printf(&quot;洗干净了%d件衣服\\n&quot;, len(closes))\n        if washer.State == true &#123;\n            fmt.Println(&quot;你忘关洗衣机了&quot;)\n        &#125;\n    &#125;\n&#125;8.3.2 构造函数定义User结构体：\ngotype User struct &#123;\n    Name string // &quot;&quot;表示未知\n    Age  int    // -1表示未知\n    Sex  byte   // 1男，2女，3未知\n&#125;\nu := User&#123;&#125;构造一个空的User，各字段都取相应数据类型的默认值。\nup := new(User)构造一个空的User，并返回其指针。\n\n自定义构造函数：\ngofunc NewDefaultUser() *User &#123;\n    return &amp;User&#123;\n        Name: &quot;&quot;,\n        Age:  -1,\n        Sex:  3,\n    &#125;\n&#125;gofunc NewUser(name string, age int, sex byte) *User &#123;\n    return &amp;User&#123;\n        Name: name,\n        Age:  age,\n        Sex:  sex,\n    &#125;\n&#125;单例模式，确保在并发的情况下，整个进程里只会创建struct的一个实例：\ngovar (\n    sUser *User\n    uOnce sync.Once\n)\n\nfunc GetUserInstance() *User &#123;\n    uOnce.Do(func() &#123; // 确保即使在并发的情况下，下面的3行代码在整个go进程里只会被执行一次\n        if sUser == nil &#123;\n            sUser = NewDefaultUser()\n        &#125;\n    &#125;)\n    return sUser\n&#125;\nfunc main() &#123;\n    // 调用GetUserInstance()得到的是同一个User实例\n    su1 := GetUserInstance()\n    su2 := GetUserInstance()\n    // 修改su1会影响su2s\n    fmt.Println(su1, su2)\n    su1.Name = &quot;绝迹之春&quot;\n    su1.Age = 18\n    su1.Sex = 1\n    fmt.Println(su1, su2)\n&#125;输出结果：\ntxt&amp;&#123; -1 3&#125; &amp;&#123; -1 3&#125;\n&amp;&#123;绝迹之春 18 1&#125; &amp;&#123;绝迹之春 18 1&#125;8.3.3 继承与重写通过嵌入匿名结构体，变相实现“继承”的功能，因为访问匿名成员时可以跳过成员名直接访问它的内部成员：\ngotype Plane struct &#123;\n    color string\n&#125;\ntype Bird struct &#123;\n    Plane\n&#125;gobird := Bird&#123;&#125;\nbird.Plane.color\nbird.color重写：\ngofunc (plane Plane) fly() int &#123;\n    return 500\n&#125;\n\n// 重写父类(Plane)的fly方法\nfunc (bird Bird) fly() int &#123;\n    return bird.Plane.fly() + 100 // 调用父类的方法\n&#125;正规来讲，Go语言并不支持继承，它只是支持组合。\ngotype Plane struct&#123;&#125;\ntype Car struct&#123;&#125;\n\n// Bird组合了Plane和Car的功能\ntype Bird struct &#123;\n    Plane\n    Car\n&#125;8.4 泛型在有泛型之前，同样的功能需要为不同的参数类型单独实现一个函数。\ngofunc add4int(a, b int) int &#123;\n    return a + b\n&#125;\nfunc add4float32(a, b float32) float32 &#123;\n    return a + b\n&#125;\nfunc add4string(a, b string) string &#123;\n    return a + b\n&#125;使用泛型\ngotype Addable interface &#123;\n    int | int8 | int16 | int32 | int64 |\n        uint | uint8 | uint16 | uint32 | uint64 | uintptr |\n        float32 | float64 | complex64 | complex128 | string\n&#125;\n\nfunc add[T Addable](a, b T) T &#123;\n    return a + b\n&#125;Golang泛型在go 1.18正式发布，同时语法也变更成如上格式。\n此外在go1.17中泛型默认没有开启，如果想用需要加-gcflags=-G=3，或者设置环境变量export GOFLAGS=&quot;-gcflags=-G=3&quot;\n（这里就不做多详细介绍，因为就目前而言下载的go最新版本已经是1.21了，不需要配置环境变量）。\n\n模块九：反射9.1 学习目标\n了解什么是反射\n熟练掌握并使用reflect.TypeOf()和reflect.ValueOf()实现反射\n了解如何用反射调用函数或成员方法\n了解用反射实现创建对象\n\n9.2 反射介绍反射就是在运行期间（不是编译期间）探知对象的类型信息和内存结构、更新变量、调用它们的方法。反射的使用场景：\n\n函数的参数类型是interface&#123;&#125;，需要在运行时对原始类型进行判断，针对不同的类型采取不同的处理方式。比如json.Marshal(v interface&#123;&#125;)。\n\n在运行时根据某些条件动态决定调用哪个函数，比如根据配置文件执行相应的算子函数。\n\n\nGo标准库里的json序列化就使用了反射。\ngotype User struct &#123;\n    Name string\n    Age int\n    Sex byte `json:&quot;gender&quot;`\n&#125;\nuser := User&#123;\n    Name: &quot;绝迹之春&quot;,\n    Age: 18,\n    Sex: 1,\n&#125;\njson.Marshal(user)  //返回 &#123;&quot;Name&quot;:&quot;绝迹之春&quot;,&quot;Age&quot;:18,&quot;gender&quot;:1&#125;Go语言中的变量是分为两部分的：\n\n类型信息：预先定义好的元信息。\n\n值信息：程序运行过程中可动态变化的。\n\n\n在GoLang的反射机制中，任何接口值都由是一个具体类型和具体类型的值两部分组成的。\n在GoLang中，反射的相关功能由内置的reflect包提供，任意接口值在反射中都可以理解为由reflect.Type和reflect.Value两部分组成，并且reflect包提供了reflect.TypeOf()和reflect.ValueOf()两个重要函数来获取任意对象的Value和Type。\n反射的弊端：\n\n代码难以阅读，难以维护。\n编译期间不能发现类型错误，覆盖测试难度很大，有些bug需要到线上运行很长时间才能发现，可能会造成严重用后果。\n反射性能很差，通常比正常代码慢一到两个数量级。在对性能要求很高，或大量反复调用的代码块里建议不要使用反射。\n\n9.3 反射的基础数据类型\n9.4 reflect.TypeOf()在 Go语言中，使用reflect.TypeOf()函数可以接受任意interface&#123;&#125;参数，可以获得任意值的类型对象（reflect.Type），程序通过类型对象可以访问任意值的类型信息。\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;reflect&quot;\n)\n\nfunc reflectType(x interface&#123;&#125;) &#123;\n    v := reflect.TypeOf(x)\n    fmt.Printf(&quot;type:%v\\n&quot;, v)\n&#125;\nfunc main() &#123;\n    var a float32 = 12.5\n    reflectType(a) // type:float32\n    var b int64 = 100\n    reflectType(b) // type:int64\n&#125;\n9.4.1 type Name和type Kind在反射中关于类型还划分为两种：类型（Type）和种类（Kind）。因为在 Go 语言中我们可以使用type关键字构造很多自定义类型，而种类（Kind）就是指底层的类型，但在反射中，当需要区分指针、结构体等大品种的类型时，就会用到种类（Kind）。 举个例子，我们定义了两个指针类型和两个结构体类型，通过反射查看它们的类型和种类。\nGo 语言的反射中像数组、切片、Map、指针等类型的变量，它们的.Name()都是返回空。\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;reflect&quot;\n)\n\nfunc reflectType(x interface&#123;&#125;) &#123;\n    t := reflect.TypeOf(x)\n    fmt.Printf(&quot;TypeOf:%v Name:%v Kind:%v\\n&quot;, t, t.Name(), t.Kind())\n\n&#125;\n\ntype myInt int64\ntype Person struct &#123;\n    Name string\n    Age  int\n&#125;\ntype Animal struct &#123;\n    Name string\n&#125;\n\nfunc main() &#123;\n    var a *float32 // 指针\n    var b myInt    // 自定义类型\n    var c rune     // 类型别名\n    reflectType(a) // TypeOf:*float32 Name: Kind:ptr\n    reflectType(b) // TypeOf:main.myInt Name:myInt Kind:int64\n    reflectType(c) // TypeOf:int32 Name:int32 Kind:int32\n    var d = Person&#123;\n        Name: &quot;绝迹之春&quot;,\n        Age:  18,\n    &#125;\n    var e = Animal&#123;Name: &quot;小花&quot;&#125;\n    reflectType(d) // TypeOf:main.Person Name:Person Kind:struct\n    reflectType(e) // TypeOf:main.Animal Name:Animal Kind:struct\n    var f = []int&#123;1, 2, 3, 4, 5&#125;\n    reflectType(f) // TypeOf:[]int Name: Kind:slice\n&#125;\n9.4.2 reflect包中定义的Kind类型gotype Kind uint\n\nconst (\n    Invalid       Kind = iota // 非法类型\n    Bool                      // 布尔型\n    Int                       // 有符号整型\n    Int8                      // 有符号 8 位整型\n    Int16                     // 有符号 16 位整型\n    Int32                     // 有符号 32 位整型\n    Int64                     // 有符号 64 位整型\n    Uint                      // 无符号整型\n    Uint8                     // 无符号 8 位整型\n    Uint16                    // 无符号 16 位整型\n    Uint32                    // 无符号 32 位整型\n    Uint64                    // 无符号 64 位整型\n    Uintptr                   // 指针\n    Float32                   // 单精度浮点数\n    Float64                   // 双精度浮点数\n    Complex64                 // 64 位复数类型\n    Complex128                // 128 位复数类型\n    Array                     // 数组\n    Chan                      // 通道\n    Func                      // 函数\n    Interface                 // 接口\n    Map                       // 映射\n    Ptr                       // 指针\n    Slice                     // 切片\n    String                    // 字符串\n    Struct                    // 结构体\n    UnsafePointer             // 底层指针\n)9.5 reflect.ValueOf()reflect.ValueOf()返回的是reflect.Value类型，其中包含了原始值的值信息。reflect.Value与原始值之间可以互相转换。\nreflect.Value类型提供的获取原始值的方法如下：\n\n\n\n方法\n说明\n\n\n\nInterface() interface {}\n将值以 interface{} 类型返回，可以通过类型断言转换为指定类型\n\n\nInt() int64\n将值以 int 类型返回，所有有符号整型均可以此方式返回\n\n\nUint() uint64\n将值以 uint 类型返回，所有无符号整型均可以此方式返回\n\n\nFloat() float64\n将值以双精度（float64）类型返回，所有浮点数（float32、float64）均可以此方式返回\n\n\nBool() bool\n将值以 bool 类型返回\n\n\nBytes() []bytes\n将值以字节数组 []bytes 类型返回\n\n\nString() string\n将值以字符串类型返回\n\n\n…\n\n\n\n9.5.1 通过反射获取原始值演示1gopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;reflect&quot;\n)\n\nfunc reflectValue(x interface&#123;&#125;) &#123;\n    v := reflect.ValueOf(x)\n    var c = v.Int() + 6 // 获取反射的原始值\n    fmt.Println(c)\n&#125;\n\nfunc main() &#123;\n    var a int64 = 100\n    reflectValue(a)\n&#125;\n9.5.2 通过反射获取原始值演示2gopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;reflect&quot;\n)\n\nfunc reflectValue(x interface&#123;&#125;) &#123;\n    v := reflect.ValueOf(x)\n    k := v.Kind()\n    switch k &#123;\n    case reflect.Int64:\n        // v.Int()从反射中获取整型的原始值\n        fmt.Printf(&quot;type is int64, value is %d\\n&quot;, v.Int())\n    case reflect.Float32:\n        // v.Float()从反射中获取浮点型的原始值\n        fmt.Printf(&quot;type is float32, value is %f\\n&quot;, v.Float())\n    case reflect.Float64:\n        // v.Float()从反射中获取浮点型的原始值\n        fmt.Printf(&quot;type is float64, value is %f\\n&quot;, v.Float())\n    &#125;\n&#125;\n\nfunc main() &#123;\n    var a float32 = 3.14\n    var b int64 = 100\n    reflectValue(a) // type is float32, value is 3.140000\n    reflectValue(b) // type is int64, value is 100\n    // 将int类型的原始值转换为reflect.Value类型\n    c := reflect.ValueOf(10)\n    fmt.Printf(&quot;type c :%T\\n&quot;, c) // type c :reflect.Value\n&#125;\n9.5.3 通过反射设置变量的值9.5.3.1 通过Value修改原始数据的值gopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;reflect&quot;\n)\n\ntype User struct &#123;\n    Id     int\n    Name   string\n    Weight float32\n    Height float32\n&#125;\n\nfunc main() &#123;\n    var i int = 10\n    var s string = &quot;hello&quot;\n    user := User&#123;\n        Id:     749,\n        Name:   &quot;绝迹之春&quot;,\n        Weight: 60.5,\n        Height: 1.70,\n    &#125;\n\n    valueI := reflect.ValueOf(&amp;i) // 由于Go语言所有函数传的都是值，所以要想修改原来的值就需要传指针\n    valueS := reflect.ValueOf(&amp;s)\n    valueUser := reflect.ValueOf(&amp;user)\n    valueI.Elem().SetInt(1) // 由于valueI对应的原始对象是指针，通过Elem()返回指针指向的对象\n    valueS.Elem().SetString(&quot;golang&quot;)\n    valueUser.Elem().FieldByName(&quot;Weight&quot;).SetFloat(61.0) // FieldByName()通过Name返回类的成员变量\n\n    fmt.Println(i)    // 1\n    fmt.Println(s)    // golang\n    fmt.Println(user) // &#123;749 绝迹之春 61 1.7&#125;\n&#125;\n强调一下，要想修改原始数据的值，给reflect.ValueOf()传的必须是指针，而指针Value不能调用Set和FieldByName方法，所以得先通过Elem()转为非指针Value。未导出成员的值不能通过反射进行修改。\ngoaddrValue := valueUser.Elem().FieldByName(&quot;addr&quot;)\nif addrValue.CanSet() &#123;\n    addrValue.SetString(&quot;芜湖&quot;)\n&#125; else &#123;\n    fmt.Println(&quot;addr是未导出成员，不可Set&quot;) // 以小写字母开头的成员相当于是私有成员\n&#125;9.5.3.2 通过Value修改Slicegopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;reflect&quot;\n)\n\ntype User struct &#123;\n    Id     int\n    Name   string\n    Weight float32\n    Height float32\n&#125;\n\nfunc main() &#123;\n    users := make([]*User, 1, 5) // len=1，cap=5\n    users[0] = &amp;User&#123;\n        Id:     749,\n        Name:   &quot;绝迹之春&quot;,\n        Weight: 60.5,\n        Height: 1.70,\n    &#125;\n\n    sliceValue := reflect.ValueOf(&amp;users) // 准备通过Value修改users，所以传users的地址\n    if sliceValue.Elem().Len() &gt; 0 &#123;      // 取得slice的长度\n        sliceValue.Elem().Index(0).Elem().FieldByName(&quot;Name&quot;).SetString(&quot;憨批&quot;)\n        fmt.Printf(&quot;1st user name change to %s\\n&quot;, users[0].Name)\n    &#125;\n&#125;\n甚至可以修改slice的cap，新的cap必须位于原始的len到cap之间，即只能把cap改小。\ngosliceValue.Elem().SetCap(3)通过把len改大，可以实现向slice中追加元素的功能。\ngosliceValue.Elem().SetLen(2)\n// 调用reflect.Value的Set()函数修改其底层指向的原始数据\nsliceValue.Elem().Index(1).Set(reflect.ValueOf(&amp;User&#123;\n    Id:     750,\n    Name:   &quot;张三&quot;,\n    Weight: 80,\n    Height: 180,\n&#125;))\nfmt.Printf(&quot;2nd user name %s\\n&quot;, users[1].Name)9.5.3.3 通过Value修改mapValue.SetMapIndex()函数：往map里添加一个key-value对。\nValue.MapIndex()函数： 根据Key取出对应的map。\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;reflect&quot;\n)\n\ntype User struct &#123;\n    Id     int\n    Name   string\n    Weight float32\n    Height float32\n&#125;\n\nfunc main() &#123;\n    u1 := &amp;User&#123;\n        Id:     749,\n        Name:   &quot;绝迹之春&quot;,\n        Weight: 60.5,\n        Height: 1.70,\n    &#125;\n    u2 := &amp;User&#123;\n        Id:     750,\n        Name:   &quot;绝迹之春&quot;,\n        Weight: 60.5,\n        Height: 1.70,\n    &#125;\n    userMap := make(map[int]*User, 5)\n    userMap[u1.Id] = u1\n\n    mapValue := reflect.ValueOf(&amp;userMap)                                                       // 准备通过Value修改userMap，所以传userMap的地址\n    mapValue.Elem().SetMapIndex(reflect.ValueOf(u2.Id), reflect.ValueOf(u2))                    // SetMapIndex 往map里添加一个key-value对\n    mapValue.Elem().MapIndex(reflect.ValueOf(u1.Id)).Elem().FieldByName(&quot;Name&quot;).SetString(&quot;憨批&quot;) // MapIndex 根据Key取出对应的map\n    for k, user := range userMap &#123;\n        fmt.Printf(&quot;key %d name %s\\n&quot;, k, user.Name)\n    &#125;\n&#125;\n9.6 结构体反射9.6.1 与结构体相关的方法任意值通过reflect.TypeOf()获得反射对象信息后，如果它的类型是结构体，可以通过反射值对象（reflect.Type）的 NumField()和 Field()方法获得结构体成员的详细信息。\nreflect.Type中与获取结构体成员相关的的方法如下表所示。\n\n\n\n方法\n说明\n\n\n\nField(i int) StructField\n根据索引，返回索引对应的结构体字段的信息。\n\n\nNumField() int\n返回结构体成员字段数量。\n\n\nFieldByName(name string) (StructField, bool)\n根据给定字符串返回字符串对应的结构体字段的信息。\n\n\nFieldByIndex(index []int) StructField\n多层成员访问时，根据 []int 提供的每个结构体的字段索引，返回字段的信息。\n\n\nFieldByNameFunc(match func(string) bool) (StructField,bool)\n根据传入的匹配函数匹配需要的字段。\n\n\nNumMethod() int\n返回该类型的方法集中方法的数目\n\n\nMethod(int) Method\n返回该类型方法集中的第 i 个方法\n\n\nMethodByName(string)(Method, bool)\n根据方法名返回该类型方法集中的方法\n\n\n9.6.2 StructField类型StructField类型用来描述结构体中的一个字段的信息。StructField的定义如下：\ngotype StructField struct &#123;\n    // 参见http://golang.org/ref/spec#Uniqueness_of_identifiers\n    Name      string    // Name 是字段的名字\n    PkgPath   string    //PkgPath 是非导出字段的包路径，对导出字段该字段为&quot;&quot;\n    Type      Type      // 字段的类型\n    Tag       StructTag // 字段的标签\n    Offset    uintptr   // 字段在结构体中的字节偏移量\n    Index     []int     // 用于Type.FieldByIndex 时的索引切片\n    Anonymous bool      // 是否匿名字段\n&#125;9.6.3 结构体反射示例当我们使用反射得到一个结构体数据之后可以通过索引依次获取其字段信息，也可以通过字段名去获取指定的字段信息。\n\ntxt获取结构体属性，获取执行结构体方法\n\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;reflect&quot;\n)\n\n// student结构体\ntype Student struct &#123;\n    Name  string `json:&quot;name&quot;`\n    Age   int    `json:&quot;age&quot;`\n    Score int    `json:&quot;score&quot;`\n&#125;\n\nfunc (s Student) GetInfo() string &#123;\n    var str = fmt.Sprintf(&quot;姓名:%v 年龄:%v 成绩:%v&quot;, s.Name, s.Age, s.Score)\n    fmt.Println(str)\n    return str\n&#125;\nfunc (s *Student) SetInfo(name string, age int, score int) &#123;\n    s.Name = name\n    s.Age = age\n    s.Score = score\n&#125;\nfunc (s *Student) Print() &#123;\n    fmt.Println(&quot;打印方法...&quot;)\n&#125;\n\n// 打印字段\nfunc PrintStructField(s interface&#123;&#125;) &#123;\n    t := reflect.TypeOf(s)\n    // v := reflect.ValueOf(s)\n    // kind := t.Kind()\n    if t.Kind() != reflect.Struct &amp;&amp; t.Elem().Kind() != reflect.Struct &#123;\n        fmt.Println(&quot;传入的不是结构体&quot;)\n        return\n    &#125;\n    // 1、通过类型变量里面的Field可以获取结构体的字段\n    field0 := t.Field(0)\n    fmt.Println(field0.Name)\n    fmt.Println(field0.Type)\n    fmt.Println(field0.Tag.Get(&quot;json&quot;))\n    // 2、通过类型变量里面的FieldByName可以获取结构体的字段\n    field1, _ := t.FieldByName(&quot;Age&quot;)\n    fmt.Println(field1.Name)\n    fmt.Println(field1.Type)\n    fmt.Println(field1.Tag.Get(&quot;json&quot;))\n    // 3、获取到该结构体有几个字段\n    num := t.NumField()\n    fmt.Println(&quot;字段数量:&quot;, num)\n&#125;\n\n// 方法\nfunc PrintStructFn(s interface&#123;&#125;) &#123;\n    t := reflect.TypeOf(s)\n    v := reflect.ValueOf(s)\n    if t.Kind() != reflect.Struct &amp;&amp; t.Elem().Kind() != reflect.Struct &#123;\n        fmt.Println(&quot;传入的不是结构体&quot;)\n        return\n    &#125;\n    // 1、通过类型变量里面的Method可以获取结构体的方法\n    var tMethod = t.Method(0) // 注意\n    fmt.Println(tMethod.Name)\n    fmt.Println(tMethod.Type)\n    // 2、通过类型变量获取这个结构体有多少个方法\n    fmt.Println(t.NumMethod())\n    // 3、执行方法（注意需要使用值变量，并且要注意参数）\n    // v.Method(0).Call(nil)\n    v.MethodByName(&quot;Print&quot;).Call(nil)\n    // 4、执行方法传入参数（注意需要使用值变量，并且要注意参数）\n    var params []reflect.Value // 声明了[]reflect.Value\n    params = append(params, reflect.ValueOf(&quot;张三&quot;))\n    params = append(params, reflect.ValueOf(22))\n    params = append(params, reflect.ValueOf(100))\n    v.MethodByName(&quot;SetInfo&quot;).Call(params) // 传入的参数是[]reflect.Value, 返回[]reflect.Value\n    // 5、执行方法获取方法的值\n    info := v.MethodByName(&quot;GetInfo&quot;).Call(nil)\n    fmt.Println(info)\n&#125;\n\nfunc main() &#123;\n    stu1 := Student&#123;\n        Name:  &quot;绝迹之春&quot;,\n        Age:   18,\n        Score: 98,\n    &#125;\n    // PrintStructField(stu1)\n    PrintStructFn(&amp;stu1)\n&#125;\n\ntxt修改结构体方法\n\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;reflect&quot;\n)\n\n// student结构体\ntype Student struct &#123;\n    Name  string `json:&quot;name&quot;`\n    Age   int    `json:&quot;age&quot;`\n    Score int    `json:&quot;score&quot;`\n&#125;\n\nfunc (s Student) GetInfo() string &#123;\n    var str = fmt.Sprintf(&quot;姓名:%v 年龄:%v 成绩:%v&quot;, s.Name, s.Age, s.Score)\n    return str\n&#125;\n\n// 反射修改结构体属性\nfunc reflectChangeStruct(s interface&#123;&#125;) &#123;\n    t := reflect.TypeOf(s)\n    v := reflect.ValueOf(s)\n    if t.Elem().Kind() != reflect.Struct &#123;\n        fmt.Println(&quot;传入的不是结构体指针类型&quot;)\n        return\n    &#125;\n    name := v.Elem().FieldByName(&quot;Name&quot;)\n    name.SetString(&quot;李四&quot;) // 设置值\n    age := v.Elem().FieldByName(&quot;Age&quot;)\n    age.SetInt(20) // 设置值\n&#125;\n\nfunc main() &#123;\n    stu1 := Student&#123;\n        Name:  &quot;绝迹之春&quot;,\n        Age:   18,\n        Score: 98,\n    &#125;\n    // PrintStructField(stu1)\n    reflectChangeStruct(&amp;stu1)\n    fmt.Println(stu1.GetInfo())\n&#125;\n9.7 调用函数、成员方法9.7.1 调用函数gopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;reflect&quot;\n)\n\nfunc Add(a, b int) int &#123;\n    return a + b\n&#125;\n\nfunc main() &#123;\n    valueFunc := reflect.ValueOf(Add) // 函数也是一种数据类型\n    typeFunc := reflect.TypeOf(Add)\n    argNum := typeFunc.NumIn()            // 函数输入参数的个数\n    args := make([]reflect.Value, argNum) // 准备函数的输入参数\n    for i := 0; i &lt; argNum; i++ &#123;\n        if typeFunc.In(i).Kind() == reflect.Int &#123;\n            args[i] = reflect.ValueOf(3) // 给每一个参数都赋3\n        &#125;\n    &#125;\n    sumValue := valueFunc.Call(args) // 返回[]reflect.Value，因为Go语言的函数返回可能是一个列表\n    if typeFunc.Out(0).Kind() == reflect.Int &#123;\n        sum := sumValue[0].Interface().(int) // 从Value转为原始数据类型\n        fmt.Printf(&quot;sum=%d\\n&quot;, sum)\n    &#125;\n&#125;\n9.7.2 调用成员方法gopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;reflect&quot;\n)\n\ntype User struct &#123;\n    Id     int\n    Name   string\n    Weight float32\n    Height float32\n&#125;\n\nfunc (user *User) BMI() float32 &#123;\n    return user.Weight / (user.Height * user.Height)\n&#125;\n\nfunc (_ User) Think() &#123;\n    fmt.Println(&quot;hi, do you know my name?&quot;)\n&#125;\n\nfunc main() &#123;\n    user := User&#123;\n        Id:     749,\n        Name:   &quot;绝迹之春&quot;,\n        Weight: 60.5,\n        Height: 1.70,\n    &#125;\n\n    valueUser := reflect.ValueOf(&amp;user)              // 必须传指针，因为BMI()在定义的时候它是指针的方法\n    bmiMethod := valueUser.MethodByName(&quot;BMI&quot;)       // MethodByName()通过Name返回类的成员变量\n    resultValue := bmiMethod.Call([]reflect.Value&#123;&#125;) // 无参数时传一个空的切片\n    result := resultValue[0].Interface().(float32)\n    fmt.Printf(&quot;bmi=%.2f\\n&quot;, result)\n\n    // Think()在定义的时候用的不是指针，valueUser可以用指针也可以不用指针\n    thinkMethod := valueUser.MethodByName(&quot;Think&quot;)\n    thinkMethod.Call([]reflect.Value&#123;&#125;)\n\n    valueUser2 := reflect.ValueOf(user)\n    thinkMethod = valueUser2.MethodByName(&quot;Think&quot;)\n    thinkMethod.Call([]reflect.Value&#123;&#125;)\n&#125;\n9.8 创建对象9.8.1 创建structgopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;reflect&quot;\n)\n\ntype User struct &#123;\n    Id     int\n    Name   string\n    Weight float32\n    Height float32\n&#125;\n\nfunc main() &#123;\n    t := reflect.TypeOf(User&#123;&#125;)\n    value := reflect.New(t) // 根据reflect.Type创建一个对象，得到该对象的指针，再根据指针提到reflect.Value\n    value.Elem().FieldByName(&quot;Id&quot;).SetInt(749)\n    user := value.Interface().(*User) // 把反射类型转成go原始数据类型Call([]reflect.Value&#123;&#125;)\n\n    fmt.Println(user)\n&#125;\n9.8.2 创建slicegopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;reflect&quot;\n)\n\ntype User struct &#123;\n    Id     int\n    Name   string\n    Weight float32\n    Height float32\n&#125;\n\nfunc main() &#123;\n    var slice []User\n    sliceType := reflect.TypeOf(slice)\n    sliceValue := reflect.MakeSlice(sliceType, 1, 3)\n    sliceValue.Index(0).Set(reflect.ValueOf(User&#123;\n        Id:     749,\n        Name:   &quot;绝迹之春&quot;,\n        Weight: 60.5,\n        Height: 1.70,\n    &#125;))\n    users := sliceValue.Interface().([]User)\n    fmt.Printf(&quot;1st user name %s\\n&quot;, users[0].Name)\n&#125;\n9.8.3 创建mapgopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;reflect&quot;\n)\n\ntype User struct &#123;\n    Id     int\n    Name   string\n    Weight float32\n    Height float32\n&#125;\n\nfunc main() &#123;\n    var userMap map[int]*User\n    mapType := reflect.TypeOf(userMap)\n    // mapValue:=reflect.MakeMap(mapType)\n    mapValue := reflect.MakeMapWithSize(mapType, 10)\n\n    user := &amp;User&#123;\n        Id:     749,\n        Name:   &quot;绝迹之春&quot;,\n        Weight: 60.5,\n        Height: 1.70,\n    &#125;\n    key := reflect.ValueOf(user.Id)\n    mapValue.SetMapIndex(key, reflect.ValueOf(user))                    // SetMapIndex 往map里添加一个key-value对\n    mapValue.MapIndex(key).Elem().FieldByName(&quot;Name&quot;).SetString(&quot;憨批&quot;) // MapIndex 根据Key取出对应的map\n    userMap = mapValue.Interface().(map[int]*User)\n    fmt.Printf(&quot;user name %s %s\\n&quot;, userMap[749].Name, user.Name)\n&#125;\nreflect包里除了MakeSlice()和MakeMap()，还有MakeChan()和MakeFunc()。\n\n模块十：并发编程10.1 学习目标\n了解什么是goroutine（协程）\n了解channel管道的相关知识点\n了解什么是管道堵塞以及如何解决\n学会利用并发编程处理大量数据处理的问题\n\n10.2 为什么要使用goroutine需求：要统计1-10000000的数字中那些是素数，并打印这些素数？\n素数：就是除了1和它本身不能被其他数整除的数\n实现方法：\n\n传统方法，通过一个for循环判断各个数是不是素数\n\n使用并发或者并行的方式，将统计素数的任务分配给多个goroutine去完成，这个时候就用到了goroutine\n\ngoroutine结合channel\n\n\n10.3 进程、线程以及并行、并发10.3.1 关于进程和线程进程（Process）就是程序在操作系统中的一次执行过程，是系统进行资源分配和调度的基本单位，进程是一个动态概念，是程序在执行过程中分配和管理资源的基本单位，每一个进程都有一个自己的地址空间。一个进程至少有 5 种基本状态，它们是：初始态，执行态，等待状态，就绪状态，终止状态。\n通俗的讲进程就是一个正在执行的程序。\n线程是进程的一个执行实例，是程序执行的最小单元，它是比进程更小的能独立运行的基本单位。\n一个进程可以创建多个线程，同一个进程中的多个线程可以并发执行，一个程序要运行的话至少有一个进程。\n10.3.2 关于并行和并发并发：多个线程同时竞争一个位置，竞争到的才可以执行，每一个时间段只有一个线程在执行。\n并行：多个线程可以同时执行，每一个时间段，可以有多个线程同时执行。\n通俗的讲多线程程序在单核CPU上面运行就是并发，多线程程序在多核CPU上运行就是并行，如果线程数大于CPU核数，则多线程程序在多个CPU上面运行既有并行又有并发。\n\n10.4 Golang中的协程（goroutine）以及主线程Golang中的主线程：（可以理解为线程&#x2F;也可以理解为进程），在一个Golang程序的主线程上可以起多个协程。Golang中多协程可以实现并行或者并发。\n协程（goroutine）：可以理解为用户级线程，这是对内核透明的，也就是系统并不知道有协程的存在，是完全由用户自己的程序进行调度的。Golang的一大特色就是从语言层面原生支持协程，在函数或者方法前面加go关键字就可创建一个协程。可以说Golang中的协程就是goroutine。\n\nGolang中的多协程有点类似其他语言中的多线程。\n多协程和多线程：Golang中每个goroutine（协程） 默认占用内存远比Java、C的线程少。\nOS线程（操作系统线程）一般都有固定的栈内存（通常为2MB左右）,一个goroutine（协程）占用内存非常小，只有2KB左右，多协程goroutine切换调度开销方面远比线程要少。\n这也是为什么越来越多的大公司使用Golang的原因之一。\n\n\n\n\n协程\n线程\n\n\n\n创建数量\n轻松创建上百万个协程而不会导致系统资源衰竭\n通常最多不能超过1万个\n\n\n内存占用\n初始分配4k堆栈，随着程序的执行自动增长删除\n创建线程时必须指定堆栈且是固定的，通常以M为单位\n\n\n切换成本\n协程切换只需保存三个寄存器，耗时约200纳秒\n线程切换需要保存几十个寄存器，耗时约1000纳秒\n\n\n调度方式\n非抢占式，由Go runtime主动交出控制权（对于开发者而言是抢占式）\n在时间片用完后，由 CPU 中断任务强行将其调度走，这时必须保存很多信息\n\n\n创建销毁\ngoroutine因为是由Go runtime负责管理的，创建和销毁的消耗非常小，是用户级的\n创建和销毁开销巨大，因为要和操作系统打交道，是内核级的，通常解决的办法就是线程池\n\n\n10.5 Goroutine的使用以及sync.WaitGroup并行执行需求：\n在主线程(可以理解成进程)中，开启一个goroutine，该协程每隔50毫秒秒输出&quot;你好 golang&quot;。\n在主线程中也每隔50毫秒输出&quot;你好 golang&quot;, 输10次后，退出程序，要求主线程和goroutine同时执行。\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;strconv&quot;\n    &quot;time&quot;\n)\n\nfunc test() &#123;\n    for i := 1; i &lt;= 10; i++ &#123;\n        fmt.Println(&quot;tesst () hello,world &quot; + strconv.Itoa(i))\n        time.Sleep(time.Second)\n    &#125;\n&#125;\nfunc main() &#123;\n    go test() // 开启了一个协程\n    for i := 1; i &lt;= 10; i++ &#123;\n        fmt.Println(&quot;main() hello,golang&quot; + strconv.Itoa(i))\n        time.Sleep(time.Second)\n    &#125;\n&#125;\n上面代码看上去没有问题，但是要注意主线程执行完毕后即使协程没有执行完毕，程序会退出，所以我们需要对上面代码进行改造。\n\nsync.WaitGroup可以实现主线程等待协程执行完毕。\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;strconv&quot;\n    &quot;sync&quot;\n    &quot;time&quot;\n)\n\nvar wg sync.WaitGroup // 1、定义全局的 WaitGroup\n\nfunc test() &#123;\n    for i := 1; i &lt;= 10; i++ &#123;\n        fmt.Println(&quot;test () 你好 golang &quot; + strconv.Itoa(i))\n        time.Sleep(time.Millisecond * 50)\n    &#125;\n    wg.Done() // 4、goroutine 结束就登记-1\n&#125;\n\nfunc main() &#123;\n    wg.Add(1) // 2、启动一个 goroutine 就登记+1\n    go test()\n    for i := 1; i &lt;= 2; i++ &#123;\n        fmt.Println(&quot; main() 你好 golang&quot; + strconv.Itoa(i))\n        time.Sleep(time.Millisecond * 50)\n    &#125;\n    wg.Wait() // 3、等待所有登记的 goroutine 都结束\n&#125;\n10.6 启动多个Goroutine在Go语言中实现并发就是这样简单，我们还可以启动多个goroutine。让我们再来一个例子：\n（这里使用sync.WaitGroup来实现等待goroutine执行完毕）\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;sync&quot;\n)\n\nvar wg sync.WaitGroup\n\nfunc hello(i int) &#123;\n    defer wg.Done() // goroutine 结束就登记-1\n    fmt.Println(&quot;Hello Goroutine!&quot;, i)\n&#125;\n\nfunc main() &#123;\n    for i := 0; i &lt; 10; i++ &#123;\n        wg.Add(1) // 启动一个 goroutine 就登记+1\n        go hello(i)\n    &#125;\n    wg.Wait() // 等待所有登记的 goroutine 都结束\n&#125;\n多次执行上面的代码，会发现每次打印的数字的顺序都不一致。这是因为10个goroutine是并发执行的，而goroutine的调度是随机的。\n10.7 设置Golang并行运行的时候占用的CPU数量Go运行时的调度器使用GOMAXPROCS参数来确定需要使用多少个OS线程来同时执行Go代码。默认值是机器上的CPU核心数。例如在一个8核心的机器上，调度器会把Go代码同时调度到8个OS线程上。\nGo语言中可以通过runtime.GOMAXPROCS()函数设置当前程序并发时占用的CPU逻辑核心数。\nGo 1.5版本之前，默认使用的是单核心执行。Go 1.5版本之后，默认使用全部的CPU逻辑核心数。\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;runtime&quot;\n)\n\nfunc main() &#123;\n    //获取当前计算机上面的 CPU 个数\n    cpuNum := runtime.NumCPU()\n    fmt.Println(&quot;cpuNum=&quot;, cpuNum)\n    //可以自己设置使用多个 CPU\n    runtime.GOMAXPROCS(cpuNum - 1)\n    fmt.Println(&quot;ok&quot;)\n&#125;\n10.8 Goroutine统计素数需求：要统计1-120000的数字中那些是素数？\n\ntxt通过传统的for循环来统计\n\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;time&quot;\n)\n\nfunc main() &#123;\n    start := time.Now().Unix()\n    for num := 1; num &lt;= 120000; num++ &#123;\n        flag := true // 假设是素数\n        for i := 2; i &lt; num; i++ &#123;\n            if num%i == 0 &#123; // 说明该 num 不是素数\n                flag = false\n                break\n            &#125;\n        &#125;\n        if flag &#123;\n            // fmt.Println(num)\n        &#125;\n    &#125;\n    end := time.Now().Unix()\n    fmt.Println(&quot;普通的方法耗时=&quot;, end-start)\n&#125;\n\ntxt`goroutine`开启多个协程统计\n\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;sync&quot;\n    &quot;time&quot;\n)\n\nvar wg sync.WaitGroup\n\nfunc fn1(n int) &#123;\n    for num := (n-1)*30000 + 1; num &lt;= n*30000; num++ &#123;\n        flag := true // 假设是素数\n        for i := 2; i &lt; num; i++ &#123;\n            if num%i == 0 &#123;\n                flag = false\n                break\n            &#125;\n        &#125;\n        if flag &#123;\n            // fmt.Println(num)\n        &#125;\n    &#125;\n    wg.Done()\n&#125;\n\nfunc main() &#123;\n    start := time.Now().Unix()\n    for i := 1; i &lt;= 4; i++ &#123;\n        wg.Add(1)\n        go fn1(i)\n    &#125;\n    wg.Wait()\n    end := time.Now().Unix()\n    fmt.Println(&quot;普通的方法耗时=&quot;, end-start)\n&#125;\n问题：上面我们使用了goroutine已经能大大的提升新能了，但是如果我们想统计数据和打印数据同时进行，这个时候如何实现呢，这个时候我们就可以使用管道。\n10.9 Channel管道管道是Golang在语言级别上提供的goroutine间的通讯方式，我们可以使用channel在多个goroutine之间传递消息。如果说goroutine是Go程序并发的执行体，channel就是它们之间的连接。channel是可以让一个goroutine发送特定值到另一个goroutine的通信机制。\nGolang的并发模型是CSP（Communicating Sequential Processes），提倡通过通信共享内存而不是通过共享内存而实现通信。\nGo语言中的管道（channel）是一种特殊的类型。管道像一个传送带或者队列，总是遵循**先入先出（First In First Out）**的规则，保证收发数据的顺序。每一个管道都是一个具体类型的导管，也就是声明channel的时候需要为其指定元素类型。\n10.9.1 channel类型channel是一种类型，一种引用类型。\n声明管道类型的格式如下：\ngovar 变量 chan 元素类型举几个例子：\ngovar ch1 chan int   // 声明一个传递整型的管道\nvar ch2 chan bool  // 声明一个传递布尔型的管道\nvar ch3 chan []int // 声明一个传递int 切片的管道10.9.2 创建channel声明的管道后需要使用make()函数初始化之后才能使用。\n创建channel的格式如下：\ngomake(chan 元素类型, 容量)举几个例子：\ngo// 创建一个能存储 10 个 int 类型数据的管道\nch1 := make(chan int, 10)\n// 创建一个能存储 4 个 bool 类型数据的管道\nch2 := make(chan bool, 4)\n// 创建一个能存储 3 个[]int 切片类型数据的管道\nch3 := make(chan []int, 3)10.9.3 channel操作管道有发送（send）、接收(receive）和关闭（close）三种操作。\n发送和接收都使用&lt;-符号。\n现在我们先使用以下语句定义一个管道：\ngoch := make(chan int, 3)10.9.3.1 发送（将数据放在管道内）将一个值发送到管道中。\ngoch &lt;- 10  // 把10发送到ch中10.9.3.2 接收（从管道内取值）从一个管道中接收值。\ngox := &lt;-ch // 从ch 中接收值并赋值给变量x\n&lt;-ch      // 从ch 中接收值，忽略结果10.9.3.3 关闭管道我们通过调用内置的close()函数来关闭管道。\ngoclose(ch)关于关闭管道需要注意的事情是，只有在通知接收方goroutine所有的数据都发送完毕的时候才需要关闭管道。管道是可以被垃圾回收机制回收的，它和关闭文件是不一样的，在结束操作之后关闭文件是必须要做的，但关闭管道不是必须的。\n关闭后的管道有以下特点：\n\n对一个关闭的管道再发送值就会导致panic。\n对一个关闭的管道进行接收会一直获取值直到管道为空。\n对一个关闭的并且没有值的管道执行接收操作会得到对应类型的零值。\n关闭一个已经关闭的管道会导致panic。\n\n10.9.4 管道阻塞10.9.4.1 无缓冲的管道如果创建管道的时候没有指定容量，那么我们可以叫这个管道为无缓冲的管道无缓冲的管道又称为阻塞的管道。我们来看一下下面的代码：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    ch := make(chan int)\n    ch &lt;- 10\n    fmt.Println(&quot;发送成功&quot;)\n&#125;\n上面这段代码能够通过编译，但是执行的时候会出现以下错误：\ngogoroutine 1 [chan send]:\nmain.main()\n        D:/go_project/src/day1/main.go:7 +0x28\nexit status 210.9.4.2 有缓冲的管道解决上面问题的方法还有一种就是使用有缓冲区的管道。我们可以在使用make函数初始化管道的时候为其指定管道的容量，例如：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    ch := make(chan int, 1) // 创建一个容量为1 的有缓冲区管道\n    ch &lt;- 10\n    fmt.Println(&quot;发送成功&quot;)\n&#125;\n只要管道的容量大于零，那么该管道就是有缓冲的管道，管道的容量表示管道中能存放元素的数量。就像你小区的快递柜只有那么个多格子，格子满了就装不下了，就阻塞了，等到别人取走一个快递员就能往里面放一个。\n管道阻塞具体代码如下：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    ch := make(chan int, 1)\n    ch &lt;- 10\n    ch &lt;- 12\n    fmt.Println(&quot;发送成功&quot;)\n&#125;\n解决办法：\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    ch := make(chan int, 1)\n    ch &lt;- 10 // 放进去\n    &lt;-ch     // 取走\n    ch &lt;- 12 // 放进去\n    &lt;-ch     // 取走\n    ch &lt;- 17 // 还可以放进去\n    fmt.Println(&quot;发送成功&quot;)\n&#125;\n10.9.5 for range从管道循环取值当向管道中发送完数据时，我们可以通过close()函数来关闭管道。\n当管道被关闭时，再往该管道发送值会引发panic，从该管道取值的操作会先取完管道中的值，再然后取到的值一直都是对应类型的零值。那如何判断一个管道是否被关闭了呢？我们来看下面这个例子：\ngopackage main\n\nimport &quot;fmt&quot;\n\n// 循环遍历管道数据\nfunc main() &#123;\n    var ch1 = make(chan int, 5)\n    for i := 0; i &lt; 5; i++ &#123;\n        ch1 &lt;- i + 1\n    &#125;\n    close(ch1) // 关闭管道\n    // 使用 for range 遍历管道，当管道被关闭的时候就会退出 for range,如果没有关闭管道就会报个错误 fatal error: all goroutines are asleep - deadlock!\n    // 通过 for range 来遍历管道数据 管道没有 key\n    for val := range ch1 &#123;\n        fmt.Println(val)\n    &#125;\n&#125;\n从上面的例子中我们看到有两种方式在接收值的时候判断该管道是否被关闭，不过我们通常使用的是for range的方式。使用for range遍历管道，当管道被关闭的时候就会退出for range。\n10.10 Goroutine结合Channel管道需求1：定义两个方法，一个方法给管道里面写数据，一个给管道里面读取数据，要求同步进行。\n\n开启一个fn1的的协程给向管道inChan中写入 100 条数据\n\n开启一个fn2的协程读取inChan中写入的数据\n\n注意：fn1和fn2同时操作一个管道\n\n主线程必须等待操作完成后才可以退出\n\n\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;sync&quot;\n    &quot;time&quot;\n)\n\nvar wg sync.WaitGroup\n\nfunc fn1(intChan chan int) &#123;\n    for i := 0; i &lt; 100; i++ &#123;\n        intChan &lt;- i + 1\n        fmt.Println(&quot;writeData 写入数据-&quot;, i+1)\n        time.Sleep(time.Millisecond * 100)\n    &#125;\n    close(intChan)\n    wg.Done()\n&#125;\n\nfunc fn2(intChan chan int) &#123;\n    for v := range intChan &#123;\n        fmt.Printf(&quot;readData 读到数据=%v\\n&quot;, v)\n        time.Sleep(time.Millisecond * 50)\n    &#125;\n    wg.Done()\n&#125;\n\nfunc main() &#123;\n    allChan := make(chan int, 100)\n    wg.Add(1)\n    go fn1(allChan)\n    wg.Add(1)\n    go fn2(allChan)\n    wg.Wait()\n    fmt.Println(&quot;读取完毕...&quot;)\n&#125;\n需求2：goroutine结合channel实现统计1-120000的数字中那些是素数？\n\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;sync&quot;\n    &quot;time&quot;\n)\n\nvar wg sync.WaitGroup\n\n// 向intChan放入 1-120000 个数\nfunc putNum(intChan chan int) &#123;\n    for i := 1; i &lt;= 1000; i++ &#123;\n        intChan &lt;- i\n    &#125;\n    //关闭 intChan\n    close(intChan)\n    wg.Done()\n&#125;\n\n// 从intChan取出数据，并判断是否为素数,如果是，就放入到primeChan\nfunc primeNum(intChan chan int, primeChan chan int, exitChan chan bool) &#123;\n    for num := range intChan &#123;\n        var flag bool = true\n        for i := 2; i &lt; num; i++ &#123;\n            if num%i == 0 &#123; // 说明该num不是素数\n                flag = false\n                break\n            &#125;\n        &#125;\n        if flag &#123;\n            // 将这个数就放入到primeChan\n            primeChan &lt;- num\n        &#125;\n    &#125;\n    // 判断关闭\n    exitChan &lt;- true\n    wg.Done()\n&#125;\n\n// 打印素数的方法\nfunc printPrime(primeChan chan int) &#123;\n    for v := range primeChan &#123;\n        fmt.Println(v)\n    &#125;\n    wg.Done()\n&#125;\n\nfunc main() &#123;\n    start := time.Now().Unix()\n    intChan := make(chan int, 1000)\n    primeChan := make(chan int, 20000) // 放入结果\n    // 标识退出的管道\n    exitChan := make(chan bool, 8) // 8个\n    // 开启一个协程，向intChan放入1-8000个数\n    wg.Add(1)\n    go putNum(intChan)\n    // 开启4个协程，从intChan 取出数据，并判断是否为素数,如果是，就放入到primeChan\n    for i := 0; i &lt; 8; i++ &#123;\n        wg.Add(1)\n        go primeNum(intChan, primeChan, exitChan)\n    &#125;\n    // 打印素数\n    wg.Add(1)\n    go printPrime(primeChan)\n    // 判断什么时候退出\n    wg.Add(1)\n    go func() &#123;\n        for i := 0; i &lt; 8; i++ &#123;\n            &lt;-exitChan\n        &#125;\n        // 当我们从exitChan取出了8个结果，就可以放心的关闭prprimeChan\n        close(primeChan)\n        wg.Done()\n    &#125;()\n    wg.Wait()\n    end := time.Now().Unix()\n    fmt.Println(end - start)\n    fmt.Println(&quot;main 线程退出&quot;)\n&#125;\n10.11 单向管道有的时候我们会将管道作为参数在多个任务函数间传递，很多时候我们在不同的任务函数中使用管道都会对其进行限制，比如限制管道在函数中只能发送或只能接收。\n\nchan&lt;- int是一个只能发送的通道，可以发送但是不能接收；\n&lt;-chan int是一个只能接收的通道，可以接收但是不能发送。\n\n例如：\n(一般建议不要直接声明只读或只写管道，是没有多大意义的，以下举例代码仅供了解其意义)\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n    // 1. 在默认情况下下，管道是双向\n    // var chan1 chan int //可读可写\n\n    // 2 声明为只写\n    var chan2 chan&lt;- int\n    chan2 = make(chan int, 3)\n    chan2 &lt;- 20\n    // num := &lt;-chan2 //error\n    fmt.Println(&quot;chan2=&quot;, chan2)\n\n    var chan_temp chan int\n    chan_temp = make(chan int, 1)\n    chan_temp &lt;- 1\n\n    // 3. 声明为只读\n    var chan3 &lt;-chan int = chan_temp\n    num2 := &lt;-chan3\n    // chan3&lt;- 30 //err\n    fmt.Println(&quot;num2=&quot;, num2)\n\n&#125;\n一般用于对不同的任务函数中使用管道进行限制，限制管道在函数中只能发送或只能接收。\ngopackage main\n\nimport &quot;fmt&quot;\n\nfunc counter(out chan&lt;- int) &#123; // 限制为只写\n    for i := 0; i &lt; 100; i++ &#123;\n        out &lt;- i\n    &#125;\n    close(out)\n&#125;\n\nfunc squarer(out chan&lt;- int, in &lt;-chan int) &#123; // 限制为只写、只读\n    for i := range in &#123;\n        out &lt;- i * i\n    &#125;\n    close(out)\n&#125;\nfunc printer(in &lt;-chan int) &#123; // 限制为只读\n    for i := range in &#123;\n        fmt.Println(i)\n    &#125;\n&#125;\n\nfunc main() &#123;\n    ch1 := make(chan int)\n    ch2 := make(chan int)\n    go counter(ch1)\n    go squarer(ch2, ch1)\n    printer(ch2)\n&#125;\n10.12 select多路复用在某些场景下我们需要同时从多个通道接收数据。这个时候就可以用到Golang中给我们提供的select多路复用。\n通常情况通道在接收数据时，如果没有数据可以接收将会发生阻塞。\n比如说下面代码来实现从多个通道接受数据的时候就会发生阻塞：\ngofor &#123;\n    // 尝试从ch1 接收值\n    data, ok := &lt;-ch1\n    // 尝试从ch2 接收值\n    data, ok := &lt;-ch2\n    ...\n&#125;这种方式虽然可以实现从多个管道接收值的需求，但是运行性能会差很多。为了应对这种场景，Go内置了select关键字，可以同时响应多个管道的操作。\nselect的使用类似于switch语句，它有一系列case分支和一个默认的分支。每个case会对应一个管道的通信（接收或发送）过程。select会一直等待，直到某个case的通信操作完成时，就会执行case分支对应的语句。\n具体格式如下：\ngoselect &#123;\ncase &lt;-ch1:\n    ...\ncase data := &lt;-ch2:\n    ...\ncase ch3 &lt;- data:\n    ...\ndefault:\n    默认操作\n&#125;举个小例子来演示下select的使用：\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;time&quot;\n)\n\nfunc main() &#123;\n    // 1.定义一个管道 10 个数据 int\n    intChan := make(chan int, 10)\n    for i := 0; i &lt; 10; i++ &#123;\n        intChan &lt;- i\n    &#125;\n    // 2.定义一个管道 5 个数据 string\n    stringChan := make(chan string, 5)\n    for i := 0; i &lt; 5; i++ &#123;\n        stringChan &lt;- &quot;hello&quot; + fmt.Sprintf(&quot;%d&quot;, i)\n    &#125;\n    for &#123;\n        select &#123;\n        case v := &lt;-intChan:\n            fmt.Printf(&quot;从 intChan 读取的数据%d\\n&quot;, v)\n        case v := &lt;-stringChan:\n            fmt.Printf(&quot;从 stringChan 读取的数据%s\\n&quot;, v)\n        default:\n            fmt.Printf(&quot;都取不到了，不玩了, 程序员可以加入逻辑\\n&quot;)\n            time.Sleep(time.Second)\n            return\n        &#125;\n    &#125;\n&#125;\n10.13 Golang并发安全和锁10.13.1 互斥锁互斥锁是传统并发编程中对共享资源进行访问控制的主要手段，它由标准库sync中的Mutex结构体类型表示。sync.Mutex类型只有两个公开的指针方法，Lock和 Unlock。Lock锁定当前的共享资源，Unlock进行解锁。\n有问题代码：\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;time&quot;\n)\n\nvar count = 0\n\nfunc test() &#123;\n    count++\n    fmt.Println(&quot;the count is : &quot;, count)\n    time.Sleep(time.Millisecond)\n&#125;\n\nfunc main() &#123;\n    for r := 0; r &lt; 100; r++ &#123;\n        go test()\n    &#125;\n    time.Sleep(time.Second)\n&#125;\ngo build \\-race main.go然后我们运行main.exe就知道到底哪里存在互斥\n互斥锁解决这个问题：\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;time&quot;\n)\n\nvar count = 0\n\nfunc test() &#123;\n    count++\n    fmt.Println(&quot;the count is : &quot;, count)\n    time.Sleep(time.Millisecond)\n&#125;\nfunc main() &#123;\n    for r := 0; r &lt; 100; r++ &#123;\n        go test()\n    &#125;\n    time.Sleep(time.Second)\n&#125;\n使用互斥锁能够保证同一时间有且只有一个goroutine进入临界区，其他的goroutine则在等待锁；当互斥锁释放后，等待的goroutine才可以获取锁进入临界区，多个goroutine同时等待一个锁时，唤醒的策略是随机的。\n虽然使用互斥锁能解决资源争夺问题，但是并不完美，通过全局变量加锁同步来实现通讯，并不利于多个协程对全局变量的读写操作。这个时候我们也可以通过另一种方式来实现上面的功能管道（Channel）。\n10.13.2 读写互斥锁互斥锁的本质是当一个goroutine访问的时候，其他goroutine都不能访问。这样在资源同步，避免竞争的同时也降低了程序的并发性能。程序由原来的并行执行变成了串行执行。\n其实，当我们对一个不会变化的数据只做“读”操作的话，是不存在资源竞争的问题的。因为数据是不变的，不管怎么读取，多少 goroutine同时读取，都是可以的。\n所以问题不是出在“读”上，主要是修改，也就是“写”。修改的数据要同步，这样其他goroutine才可以感知到。所以真正的互斥应该是读取和修改、修改和修改之间，读和读是没有互斥操作的必要的。\n因此，衍生出另外一种锁，叫做读写锁。\n读写锁可以让多个读操作并发，同时读取，但是对于写操作是完全互斥的。也就是说，当一个goroutine进行写操作的时候，其他 goroutine既不能进行读操作，也不能进行写操作o。\nGo中的读写锁由结构体类型sync.RWMutex表示。此类型的方法集合中包含两对方法：\n一组是对写操作的锁定和解锁，简称“写锁定”和“写解锁”：\ngofunc (*RWMutex)Lock()\nfunc (*RWMutex)Unlock()另一组表示对读操作的锁定和解锁，简称为“读锁定”与“读解锁”：\ngofunc (*RWMutex)RLock()\nfunc (*RWMutex)RUnlock()读写锁示例：\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;sync&quot;\n    &quot;time&quot;\n)\n\nvar count int\nvar mutex sync.RWMutex\nvar wg sync.WaitGroup\n\n// 写的方法\nfunc write() &#123;\n    mutex.Lock()\n    fmt.Println(&quot;执行写操作&quot;)\n    time.Sleep(time.Second * 3)\n    mutex.Unlock()\n    wg.Done()\n&#125;\n\n// 读的方法\nfunc read() &#123;\n    mutex.RLock()\n    fmt.Println(&quot;执行读操作&quot;)\n    time.Sleep(time.Second * 3)\n    mutex.RUnlock()\n    wg.Done()\n&#125;\n\nfunc main() &#123;\n    // 开启10个协程执行写操作\n    for i := 0; i &lt; 10; i++ &#123;\n        wg.Add(1)\n        go read()\n    &#125;\n    // 开启10个协程执行读操作\n    for i := 0; i &lt; 10; i++ &#123;\n        wg.Add(1)\n        go write()\n    &#125;\n    wg.Wait()\n&#125;\n10.14 Goroutine Recover解决协程中出现的Panicgopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;time&quot;\n)\n\n// 函数\nfunc sayHello() &#123;\n    for i := 0; i &lt; 10; i++ &#123;\n        time.Sleep(time.Second)\n        fmt.Println(&quot;hello,world&quot;)\n    &#125;\n&#125;\n\n// 函数\nfunc test() &#123;\n    // 这里我们可以使用defer+recover\n    defer func() &#123;\n        // 捕获test抛出的panic\n        if err := recover(); err != nil &#123;\n            fmt.Println(&quot;test() 发生错误&quot;, err)\n        &#125;\n    &#125;()\n    // 定义了一个map\n    var myMap map[int]string\n    myMap[0] = &quot;golang&quot; // error\n&#125;\n\nfunc main() &#123;\n    go sayHello()\n    go test()\n    for i := 0; i &lt; 10; i++ &#123;\n        fmt.Println(&quot;main() ok=&quot;, i)\n        time.Sleep(time.Second)\n    &#125;\n&#125;\n\n模块十一：包与工程化11.1 学习目标\n了解Golang中的包的定义\n熟练并掌握学会包的使用\n\n11.2 Golang中包的介绍和定义包（package）是多个Go源码的集合，是一种高级的代码复用方案，Go语言为我们提供了很多内置包，如fmt、strconv、strings、sort、errors、time、encoding/json、os、io等。\nGolang中的包可以分为三种：1、系统内置包 2、自定义包 3、第三方包\n系统内置包: Golang 语言给我们提供的内置包，引入后可以直接使用，如fmt、strconv、strings、sort、errors、time、encoding/json、os、io 等。\n自定义包：开发者自己写的包。\n第三方包：属于自定义包的一种，需要下载安装到本地后才可以使用，如&quot;github.com/shopspring/decimal&quot;包解决float精度丢失问题。\n11.3 Golang包管理工具go mod在Golang 1.11版本之前如果我们要自定义包的话必须把项目放在GOPATH目录。Go1.11版本之后无需手动配置环境变量，使用go mod管理项目，也不需要非得把项目放到GOPATH指定目录下，你可以在你磁盘的任何位置新建一个项目，Go1.13以后可以彻底不要GOPATH了。\n11.3.1 go mod init初始化项目实际项目开发中我们首先要在我们项目目录中用go mod命令生成一个go.mod文件管理我们项目的依赖。\n比如我们的Golang项目文件要放在了day1这个文件夹，这个时候我们需要在day1文件夹 里面使用go mod命令生成一个go.mod文件。\n（2.9已经介绍过go mod的基础用法，这里不做过多讲述）\n11.3.2 go mod其他命令download download modules to local cache （下载依赖的module到本地cache）\nedit edit go.mod from tools or scripts （编辑go.mod文件）\ngraph print module requirement graph （打印模块依赖图）\ninit initialize new module in current directory （再当前文件夹下初始化一个新的module，创建go.mod文件）\ntidy add missing and remove unused modules （增加丢失的module，去掉未用的module）\nvendor make vendored copy of dependencies （将依赖复制到vendor下）\nverify verify dependencies have expected content （校验依赖检查下载的第三方库有没有本地修改，如果有修改，则会返回非0，否则验证成功）\nwhy explain why packages or modules are needed （解释为什么需要依赖）\n11.4 Golang中自定义包包（package）是多个Go源码的集合，一个包可以简单理解为一个存放多个.go文件的文件 夹。该文件夹下面的所有go文件都要在代码的第一行添加如下代码，声明该文件归属的包。\ngopackage 包名注意事项：\n\n一个文件夹下面直接包含的文件只能归属一个package，同样一个package的文件不能 在多个文件夹下。\n\n包名可以不和文件夹的名字一样，包名不能包含 - 符号。\n\n包名为main的包为应用程序的入口包，这种包编译后会得到一个可执行文件，而编译不包含。\n\n\n11.4.1 定义一个包如果想在一个包中引用另外一个包里的标识符（如变量、常量、类型、函数等）时，该标识符必须是对外可见的（public）。在Go语言中只需要将标识符的首字母大写就可以让标识符对外可见了。\n详细步骤：\n首先在工作区目录下生成并初始化一个go.work文件\n\n\n在src下新建一个包名为calc的包\n\n初始化该模块\n\n将该模块纳入工作区，即go work use一下该模块\n\n在calc包里新建一个.go文件\n\n写入以下代码，并保存\n代码如下：\ngopackage calc\n\n// 首字母大小表示公有，首字母小写表示私有\nvar a = 100  // 私有变量\nvar Age = 20 // 公有变量\nfunc Add(x, y int) int &#123; // 公有方法\n    return x + y\n&#125;\nfunc subTest(x, y int) int &#123; // 私有方法\n    return x - y\n&#125;\nfunc Sub(x, y int) int &#123; // 私有方法可以在本包中调用\n    return subTest(x, y)\n&#125;\nfunc Test() int &#123; // 私有变量可以在本包中调用\n    return a\n&#125;\n在src下新建main.go文件\n\n**写入以下代码，并保存\nmain.go中引入这个包，访问一个包里面的公有属性方法的时候需要通过包名称.去访问\ngopackage main\n\nimport (\n    &quot;calc&quot;\n    &quot;fmt&quot;\n)\n\nfunc main() &#123;\n    a := calc.Add(10, 20)\n    s := calc.Sub(20, 10)\n    fmt.Println(a)\n    fmt.Println(s)\n    fmt.Println(calc.Age)\n    fmt.Println(calc.Test())\n&#125;\n开始执行代码\n\n\n11.4.2 导入一个包11.4.2.1 单行导入单行导入的格式如下：\ngoimport &quot;包 1&quot;\nimport &quot;包 2&quot;11.4.2.2 多行导入多行导入的格式如下：\n11.4.2.3 匿名导入包如果只希望导入包，而不使用包内部的数据时，可以使用匿名导入包。具体的格式如下：\ngoimport  _ &quot;包的路径&quot;匿名导入的包与其他方式导入的包一样都会被编译到可执行文件中。\n11.4.3 自定义包名在导入包名的时候，我们还可以为导入的包设置别名。通常用于导入的包名太长或者导入的 包名冲突的情况。\n具体语法格式如下：\ngoimport 别名 &quot;包的路径&quot;举例：\ngopackage main\n\nimport (\n    c &quot;calc&quot;\n    &quot;fmt&quot;\n)\n\nfunc main() &#123;\n    a := c.Add(10, 20)\n    s := c.Sub(20, 10)\n    fmt.Println(a)\n    fmt.Println(s)\n    fmt.Println(c.Age)\n    fmt.Println(c.Test())\n&#125;\n11.5 Golang中init()初始化函数11.5.1 init()函数介绍在Go语言程序执行时导入包语句会自动触发包内部init()函数的调用。\n需要注意的是：init()函数没有参数也没有返回值。\ninit()函数在程序运行时自动被调用执行，不能在代码中主动调用它。\n包初始化执行的顺序如下图所示：\n\n11.5.2 init()函数执行顺序Go语言包会从main包开始检查其导入的所有包，每个包中又可能导入了其他的包。\nGo编译器由此构建出一个树状的包引用关系，再根据引用顺序决定编译顺序，依次编译这些包的代码。\n在运行时，被最后导入的包会最先初始化并调用其init()函数，如下图所示：\n\n11.6 Golang中使用第三方包我们可以在Go Packages - Go Packages查找看常见的Golang第三方包。\n找到我们需要下载安装的第三方包的地址\n比如解决float精度损失的包decimal\nhttps://github.com/shopspring/decimal\n安装这个包\n第一种方法：go get 包名称 （全局）\ngogo get github.com/shopspring/decimal第二种方法：go mod download （全局）\ngogo mod download依赖包会自动下载到$GOPATH/pkg/mod，多个项目可以共享缓存的mod，注意使用go mod download的时候首先需要在你的项目里面引入第三方包。\n第三种方法：go mod vendor 将依赖复制到当前项目的vendor下 （本项目）\ngogo mod vendor将依赖复制到当前项目的vendor下。\n注意：使用go mod vendor的时候首先需要在你的项目里面引入第三方包。\n看文档使用这个包\n包安装完毕后我们就可以看文档使用这个包了…\n\n模块十二：文件管理12.1 学习目标\n了解文件操作的相关api\n了解基本的文件操作：打开关闭文件、读取文件、写入文件、拷贝文件、重命名文件、创建目录、删除目录或文件等\n\n12.2 相关API\n根据提供的文件名创建新的文件，返回一个文件对象，默认权限是0666\ngofunc Create(name string) (file *File, err Error)\n根据文件描述符创建相应的文件，返回一个文件对象\ngofunc NewFile(fd uintptr, name string) *File\n只读方式打开一个名称为name的文件\ngofunc Open(name string) (file *File, err Error)\n打开名称为name的文件，flag是打开的方式，只读、读写等，perm是权限\ngofunc OpenFile(name string, flag int, perm uint32) (file *File, err Error)\n写入byte类型的信息到文件\ngofunc (file *File) Write(b []byte) (n int, err Error)\n在指定位置开始写入byte类型的信息\ngofunc (file *File) WriteAt(b []byte, off int64) (n int, err Error)\n写入string信息到文件\ngofunc (file *File) WriteString(s string) (ret int, err Error)\n读取数据到b中\ngofunc (file *File) Read(b []byte) (n int, err Error)\n从off开始读取数据到b中\ngofunc (file *File) ReadAt(b []byte, off int64) (n int, err Error)\n删除文件名为name的文件\n\n\ntxt```go\nfunc Remove(name string) Error\n```12.3 打开和关闭文件12.3.1 os.Open()os.Open()函数能够打开一个文件，返回一个*File和一个err。对得到的文件实例调用close()方法能够关闭文件（一定不要忘记关闭文件）。\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;os&quot;\n)\n\nfunc main() &#123;\n    // 只读方式打开当前目录下的main.go文件\n    file, err := os.Open(&quot;./main.go&quot;)\n    if err != nil &#123;\n        fmt.Println(&quot;open file failed!, err:&quot;, err)\n        return\n    &#125;\n    // 关闭文件\n    file.Close()\n&#125;\n12.3.2 os.OpenFile()os.OpenFile()函数能够以指定模式打开文件，从而实现文件写入相关功能。\ngofunc OpenFile(name string, flag int, perm FileMode) (*File, error) &#123; ... &#125;其中： name：要打开的文件名，flag：打开文件的模式。\n模式有以下几种：\n\n\n\n模式\n含义\n\n\n\nos.O_WRONLY\n只写\n\n\nos.O_CREATE\n创建文件\n\n\nos.O_RDONLY\n只读\n\n\nos.O_RDWR\n读写\n\n\nos.O_TRUNC\n清空\n\n\nos.O_APPEND\n追加\n\n\n12.4 读取文件12.4.1 file.Read()和file.ReadAt()文件读取可以用file.Read()和file.ReadAt()，读到文件末尾会返回io.EOF的错误。\nRead 方法定义如下：\ngofunc (f *File) Read(b []byte) (n int, err error)它接收一个字节切片，返回读取的字节数和可能的具体错误，读到文件末尾时会返回0和io.EOF。\n举个例子：\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;io&quot;\n    &quot;os&quot;\n)\n\nfunc main() &#123;\n    // 只读方式打开当前目录下的 main.go 文件\n    file, err := os.Open(&quot;./main.go&quot;)\n    if err != nil &#123;\n        fmt.Println(&quot;open file failed!, err:&quot;, err)\n        return\n    &#125;\n    defer file.Close()\n    // 使用 Read 方法读取数据，注意一次只会读取 128 个字节\n    var tmp = make([]byte, 128)\n    n, err := file.Read(tmp)\n    if err == io.EOF &#123;\n        fmt.Println(&quot;文件读完了&quot;)\n        return\n    &#125;\n    if err != nil &#123;\n        fmt.Println(&quot;read file failed, err:&quot;, err)\n        return\n    &#125;\n    fmt.Printf(&quot;读取了%d 字节数据\\n&quot;, n)\n    fmt.Println(string(tmp[:n]))\n&#125;\nfile.ReadAt()的用法与此类似。\n12.4.1.1 循环读取使用for循环读取文件中的所有数据。\n举个例子：\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;io&quot;\n    &quot;os&quot;\n)\n\nfunc main() &#123;\n    // 只读方式打开当前目录下的 main.go 文件\n    file, err := os.Open(&quot;./main.go&quot;)\n    if err != nil &#123;\n        fmt.Println(&quot;open file failed!, err:&quot;, err)\n        return\n    &#125;\n    defer file.Close()\n    // 循环读取文件\n    var content []byte\n    var tmp = make([]byte, 128)\n    for &#123;\n        n, err := file.Read(tmp)\n        if err == io.EOF &#123;\n            fmt.Println(&quot;文件读完了&quot;)\n            break\n        &#125;\n        if err != nil &#123;\n            fmt.Println(&quot;read file failed, err:&quot;, err)\n            return\n        &#125;\n        content = append(content, tmp[:n]...)\n    &#125;\n    fmt.Println(string(content))\n&#125;\n12.4.2 bufio.NewReader()bufio包实现了带缓冲区的读写，是对文件读写的封装\nbufio.NewReader()缓冲读数据\ngopackage main\n\nimport (\n    &quot;bufio&quot;\n    &quot;fmt&quot;\n    &quot;io&quot;\n    &quot;os&quot;\n)\n\n// bufio 按行读取示例\nfunc main() &#123;\n    file, err := os.Open(&quot;C:/test.txt&quot;)\n    if err != nil &#123;\n        fmt.Println(&quot;open file failed, err:&quot;, err)\n        return\n    &#125;\n    defer file.Close()\n    reader := bufio.NewReader(file)\n    for &#123;\n        line, err := reader.ReadString(&#39;\\n&#39;) // 注意是字符\n        if err == io.EOF &#123;\n            if len(line) != 0 &#123;\n                fmt.Println(line)\n            &#125;\n            fmt.Println(&quot;文件读完了&quot;)\n            break\n        &#125;\n        if err != nil &#123;\n            fmt.Println(&quot;read file failed, err:&quot;, err)\n            return\n        &#125;\n        fmt.Print(line)\n    &#125;\n&#125;\n12.4.3 ioutil.ReadFile()（Go 1.16已废弃io&#x2F;ioutil包）修改（Go 1.16及以上版本）：ioutil.ReadFile() -&gt; os.ReadFile()\nio/ioutil包的ReadFile()方法能够读取完整的文件，只需要将文件名作为参数传入。\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;io/ioutil&quot;\n)\n\n// ioutil.ReadFile 读取整个文件\nfunc main() &#123;\n    content, err := ioutil.ReadFile(&quot;./main.go&quot;)\n    if err != nil &#123;\n        fmt.Println(&quot;read file failed, err:&quot;, err)\n        return\n    &#125;\n    fmt.Println(string(content))\n&#125;\n12.5 写入文件12.5.1 file.Write()和file.WriteString()gopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;os&quot;\n)\n\nfunc main() &#123;\n    file, err := os.OpenFile(&quot;C:/test.txt&quot;, os.O_CREATE|os.O_RDWR, 0666)\n    if err != nil &#123;\n        fmt.Println(&quot;open file failed, err:&quot;, err)\n        return\n    &#125;\n    defer file.Close()\n    str := &quot;你好 golang&quot;\n    file.Write([]byte(str))        // 写入字节切片数据\n    file.WriteString(&quot;直接写入的字符串数据&quot;) // 直接写入字符串数据\n&#125;\n12.5.2 bufio.NewWriter()bufio.NewWriter()缓冲写数据\ngopackage main\n\nimport (\n    &quot;bufio&quot;\n    &quot;fmt&quot;\n    &quot;os&quot;\n)\n\nfunc main() &#123;\n    file, err := os.OpenFile(&quot;C:/test.txt&quot;, os.O_CREATE|os.O_TRUNC|os.O_WRONLY, 0666)\n    if err != nil &#123;\n        fmt.Println(&quot;open file failed, err:&quot;, err)\n        return\n    &#125;\n    defer file.Close()\n    writer := bufio.NewWriter(file)\n    for i := 0; i &lt; 10; i++ &#123;\n        writer.WriteString(&quot;你好 golang\\r\\n&quot;) // 将数据先写入缓存\n    &#125;\n    writer.Flush() // 将缓存中的内容写入文件（注意）\n&#125;\n12.5.3 ioutil.WriteFile()（Go 1.16已废弃io&#x2F;ioutil包）修改（Go 1.16及以上版本）：ioutil.WriteFile() -&gt; os.WriteFile()\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;io/ioutil&quot;\n)\n\nfunc main() &#123;\n    str := &quot;hello golang&quot;\n    err := ioutil.WriteFile(&quot;C:/test.txt&quot;, []byte(str), 0666)\n    if err != nil &#123;\n        fmt.Println(&quot;write file failed, err:&quot;, err)\n        return\n    &#125;\n&#125;\n12.6 拷贝文件12.6.1 第一种拷贝文件方法：ioutil包进行拷贝修改（Go 1.16及以上版本）：ioutil -&gt; os\ngopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;io/ioutil&quot;\n)\n\n// 自己编写一个函数，接收两个文件路径 srcFileName dstFileName\nfunc CopyFile(dstFileName string, srcFileName string) (err error) &#123;\n    input, err := ioutil.ReadFile(srcFileName)\n    if err != nil &#123;\n        fmt.Println(err)\n        return err\n    &#125;\n    err = ioutil.WriteFile(dstFileName, input, 0644)\n    if err != nil &#123;\n        fmt.Println(&quot;Error creating&quot;, dstFileName)\n        fmt.Println(err)\n        return err\n    &#125;\n    return nil\n&#125;\nfunc main() &#123;\n    srcFile := &quot;c:/test1.zip&quot;\n    dstFile := &quot;D:/test1.zip&quot;\n    err := CopyFile(dstFile, srcFile)\n    if err == nil &#123;\n        fmt.Printf(&quot;拷贝完成\\n&quot;)\n    &#125; else &#123;\n        fmt.Printf(&quot;拷贝错误 err=%v\\n&quot;, err)\n    &#125;\n&#125;\n12.6.2 第二种拷贝文件方法：文件流的方式拷贝gopackage main\n\nimport (\n    &quot;fmt&quot;\n    &quot;io&quot;\n    &quot;os&quot;\n)\n\n// 自己编写一个函数，接收两个文件路径 srcFileName dstFileName\nfunc CopyFile(dstFileName string, srcFileName string) (err error) &#123;\n    source, _ := os.Open(srcFileName)\n    destination, _ := os.OpenFile(dstFileName, os.O_CREATE|os.O_WRONLY, 0666)\n    buf := make([]byte, 128)\n    for &#123;\n        n, err := source.Read(buf)\n        if err != nil &amp;&amp; err != io.EOF &#123;\n            return err\n        &#125;\n        if n == 0 &#123;\n            break\n        &#125;\n        if _, err := destination.Write(buf[:n]); err != nil &#123;\n            return err\n        &#125;\n    &#125;\n    return nil\n&#125;\n\nfunc main() &#123;\n    //调用 CopyFile 完成文件拷贝\n    srcFile := &quot;c:/000.avi&quot;\n    dstFile := &quot;D:/000.avi&quot;\n    err := CopyFile(dstFile, srcFile)\n    if err == nil &#123;\n        fmt.Printf(&quot;拷贝完成\\n&quot;)\n    &#125; else &#123;\n        fmt.Printf(&quot;拷贝错误 err=%v\\n&quot;, err)\n    &#125;\n&#125;\n12.7 重命名文件goerr := os.Rename(&quot;C:/test1.txt&quot;, &quot;C:/test2.txt&quot;) // 只能同盘操作\nif err != nil &#123;\n    fmt.Println(err)\n&#125;12.8 创建目录12.8.1 一次创建一个目录goerr := os.Mkdir(&quot;./abc&quot;, 0666)\nif err != nil &#123;\n    fmt.Println(err)\n&#125;12.8.2 一次创建多个目录goerr := os.MkdirAll(&quot;dir1/dir2/dir3&quot;, 0666) //创建多级目录\nif err != nil &#123;\n    fmt.Println(err)\n&#125;12.9 删除目录和文件12.9.1 删除一个目录或者文件goerr := os.Remove(&quot;t.txt&quot;)\nif err != nil &#123;\n    fmt.Println(err)\n&#125;12.9.2 一次删除多个目录或者文件goerr := os.RemoveAll(&quot;aaa&quot;)\nif err != nil &#123;\n    fmt.Println(err)\n&#125;\n后记Go基础还应包括：GoWeb、GoSocket、Go数据库等等，如果有机会，博主再与大家介绍分享。。。\n本篇文档欢迎任何人进行转载，但是必须标注来源！！！\nArticle link： https://tqgoblin.site/post/csdn/Golang基础入门/  Author： Stephen  \n","slug":"csdn/Golang基础入门","date":"2021-09-13T14:07:24.000Z","categories_index":"go语言学习","tags_index":"golang","author_index":"Stephen"},{"id":"1088266219777eea7a5e1871adc5dc26","title":"限流的场景和应用","content":"应用场景1. 防止流量过大导致系统崩溃:当流量突然增大时,限流可以确保系统不会因为过大的流量导致资源耗尽和系统崩溃。\n2. 防止恶意请求:限流可以用来针对一些恶意请求进行限制,避免这些请求对系统造成损害。\n3. 避免线程池耗尽:当请求很多时,后端的线程池可能会耗尽,导致许多请求得不到响应。加上限流可以避免这种情况的发生。\n4. 防止缓存击穿:当缓存失效时,后端系统会接收大量的请求。限流可以确保后端系统不会因为缓存失效而被打垮。\n5. 控制流量并发度:有些系统会根据自身的处理能力来控制并发度,限流可以起到控制流量和并发度的作用。\n常见策略1. 客户端限流:限制客户端向服务器发起请求的频率。可以通过在客户端实现限流算法来实现。\n2. 服务端限流:限制服务器处理请求的频率。可以在服务器上实现限流来达到这个目的。\n3. 应用层限流:在应用层根据业务逻辑对某些请求进行限流。比如限制对重要资源的访问频率。\n4. 网络层限流:在网络设备上进行流量控制,限制到达服务器的请求频率。\n常用技术1. 计数器: 使用一个计数器来统计请求数,如果达到阈值则拒绝请求。这是一种简单的限流方式,可以使用 Java 的 AtomicInteger 来实现计数器。\n2. 滑动窗口: 维持一个固定大小的窗口,统计窗口内的请求数,如果达到阈值则拒绝请求。这种方式比简单的计数器更精确,可以使用 Java 的 LinkedBlockingQueue 来实现滑动窗口。\n3. 漏桶算法: 维持一个桶的大小,如果桶满了则拒绝请求,否则放行请求。漏桶的大小控制了限流速率,可以使用 Java 的 Semaphore 来实现漏桶算法。\n4. 令牌桶算法: 定期生成令牌放入桶中,如果桶中有令牌则允许请求并消耗一个令牌,否则拒绝请求。令牌生成的速率控制了限流速率,可以使用 Java 的 ScheduledExecutorService 和 Semaphore 来实现令牌桶算法。\n5. Guava 的 RateLimiter: Google Guava 中的 RateLimiter 实现了令牌桶算法,可以直接使用。它提供简单易用的限流功能,对并发请求有很好的支持。\n代码实现计数器javapublic boolean limit() &#123;\n    count++;\n    if (count &gt; threshold) &#123;\n        return false;  // 达到限流阈值,拒绝请求\n    &#125;\n    return true;\n&#125;滑动窗口javaDeque&lt;Long&gt; window = new ArrayDeque&lt;&gt;();\nlong now = System.currentTimeMillis(); \nwindow.add(now);\nif (window.size() &gt; windowSize) &#123;\n    long oldest = window.pollFirst();\n    if (now - oldest &lt; interval) &#123;\n        return false;  // 窗口内请求数达到限流阈值,拒绝请求\n    &#125;\n&#125;\nreturn true;  漏桶算法javaSemaphore semaphore = new Semaphore(bucketSize); \nif (semaphore.tryAcquire()) &#123;\n    return true;   // 获取令牌成功,放行请求\n&#125;\nreturn false;  // 桶中无令牌,拒绝请求令牌桶算法javaScheduledExecutorService scheduled = Executors.newScheduledThreadPool(1);\nSemaphore semaphore = new Semaphore(0); \n\nscheduled.scheduleAtFixedRate(() -&gt; &#123;\n    int added = Math.min(bucketSize - semaphore.availablePermits(), addRate);\n    semaphore.release(added);  // 按速率添加令牌\n&#125;, 0, 1, TimeUnit.SECONDS);\n\nif (semaphore.tryAcquire()) &#123;  \n    return true;  // 获取令牌成功,放行请求\n&#125;\nreturn false; // 桶中无令牌,拒绝请求Guava RateLimiterjavaRateLimiter limiter = RateLimiter.create(2);  // 每秒 2 个请求  \nif (limiter.tryAcquire()) &#123;  \n    return true;  // 获取令牌成功,放行请求\n&#125;\nreturn false; // 限流,拒绝请求 优缺点这些限流技术各有优缺点:\n1. 计数器:\n  - 优点:简单易实现\n  - 缺点:不够精确,无法控制速率,窗口大小不可配置\n2. 滑动窗口:\n  - 优点:比计数器更精确,窗口大小可配置\n  - 缺点:实现复杂度高于计数器\n3. 漏桶算法:\n  - 优点:可以限制速率,实现简单\n  - 缺点:不够精确,无法应对突发流量\n4. 令牌桶算法:\n  - 优点:可以限制速率,且能应对突发流量\n  - 缺点:实现较复杂\n5. Guava RateLimiter:\n  - 优点:实现简单,可以限制速率,能应对突发流量\n  - 缺点:速率控制不够灵活,不能实现更复杂的限流策略\n综上,我的推荐是:如果需要一个简单的限流器,使用 Guava RateLimiter。它实现简单,功能足够。如果需要更精确的速率控制或实现更复杂的限流策略,自己实现基于令牌桶算法的限流器。如果对限流精度要求不高,可以使用简单的计数器或滑动窗口。基于应用场景选择合适的限流技术非常重要。没有一种技术适用于所有场景,所以理解每个技术的优缺点很关键。此外,限流还可以从多个 dimensionality 进行\n比如:\n- 对某用户进行限流\n- 对某资源进行限流\n- 基于客户端 IP 进行限流\n- 等等所以在设计一个限流系统时,需要全面考虑到这些因素。\n应用场景1. 计数器:\n  - 场景:需要一个简单的限流器,限流精度要求不高。\n  - 例如:限制某资源的访问次数,简单防止窃取资源。\n  - 框架:简单的应用层限流实现\n2. 滑动窗口:\n   - 场景:需要限制一定窗口内的请求次数,且窗口大小可配置。\n   - 例如:限制一定时间窗口内的登录失败次数,以防暴力破解登录。\n   -框架:Nginx 等代理服务器\n3. 漏桶算法:\n  - 场景:需要固定速率地限制请求,可应对一定程度的突发流量。\n  - 例如:限制接口调用的速率,防止流量骤增导致系统超负荷。\n  - 框架:Hystrix 等容错库\n4. 令牌桶算法:\n  - 场景:需要更精确地控制请求通过的速率,并能应对更高的突发流量。\n  - 例如:限制对重要资源的访问速率,保证资源不会因为突发大流量而超负荷。\n  - 框架:Sentinel、Resilience4j 等流控框架\n5. Guava RateLimiter:\n  - 场景:需要一个简单的速率限制器,不需要很精确的速率控制和复杂的限流策略。\n  - 例如:简单限制 API 调用频率,防止占用过多服务器资源。\n  -框架:Google Guava\n除此之外,限流技术在其他产品和框架中也有应用:\n  - AWS WAF:基于速率限制和 web 防火墙限制恶意流量\n  - Nginx 限流模块:基于速率限制和漏桶算法进行限流\n  - Envoy 路由器:基于速率限制和令牌桶算法进行限流\n  - API 网关:对 API 请求进行限流,一般基于速率限制实现\n  - 数据库代理:基于速率限制和漏桶算法限制数据库连接数和查询频率\n  - 缓存代理:基于速率限制和令牌桶算法限制缓存击穿场景下的请求流量\n  - 等等\n可以看到,这些限流技术可以应用于很多场景,但每个技术都有其最适用的场景。在选择限流技术时,需要考虑:\n  - 需要控制的对象(用户、资源等)\n  - 限流精度要求(速率控制是否精确)\n  - 是否需要应对突发流量\n  - 限流策略的复杂度\n  - 等等\n综合考虑这些因素再选择最适合的限流技术。\n工具类封装代码包里这个工具类实现了计数器限流、滑动窗口限流、漏桶限流和令牌桶限流四种方式。可以根据需要选择相应的限流方法进行流量控制。这样一个集成多种限流技术的工具类可以很好地应用于不同的限流场景,而不需要每次都自己实现限流算法。\n注意：这里的令牌桶和漏桶的限流是线程数层面的限流，无法用在业务层面做数据限速。\nArticle link： https://tqgoblin.site/post/csdn/限流的场景和应用/  Author： Stephen  \n","slug":"csdn/限流的场景和应用","date":"2021-06-25T07:36:34.000Z","categories_index":"Java","tags_index":"Java","author_index":"Stephen"},{"id":"efa52dd3078c52d1c106921316828151","title":"Prometheus监控K8S","content":"一、描述Cadvisor + node-exporter + prometheus + grafana是一套非常流行的Kubernetes监控方案。它们的功能如下:\n- Cadvisor:容器资源监控工具,可以实时监控CPU、内存、存储、网络等容器指标,并暴露Metrics接口。\n- node-exporter:节点级指标导出工具,可以监控节点的CPU、内存、磁盘、网络等指标,并暴露Metrics接口。\n- Prometheus:时间序列数据库和监控报警工具,可以抓取Cadvisor和node-exporter暴露的Metrics接口,存储时序数据,并提供PromQL查询语言进行监控分析和报警。\n- Grafana:图表和Dashboard工具,可以查询Prometheus中的数据,并通过图表的方式直观展示Kubernetes集群的运行指标和状态。\n二、监控流程1. 在Kubernetes集群的每个节点安装Cadvisor和node-exporter,用于采集容器和节点级指标数据。\n2. 部署Prometheus,配置抓取Cadvisor和node-exporter的Metrics接口,存储 containers 和 nodes 的时序数据。\n3. 使用Grafana构建监控仪表盘,选择Prometheus作为数据源,编写PromQL查询语句,展示K8S集群的CPU使用率、内存使用率、网络流量等监控指标。\n4. 根据监控结果,可以设置Prometheus的报警规则,当监控指标超过阈值时发送报警信息。这套方案能够全面监控Kubernetes集群的容器和节点,通过Metrics指标和仪表盘直观反映集群状态,并实现自动报警,非常适合K8S环境下微服务应用的稳定运行。\n具体实现方案如下:\n- Cadvisor:在集群中每个节点作为DaemonSet部署cadvisor,采集容器Metrics。\n- node-exporter:在每个节点也作为DaemonSet运行,采集节点Metrics。\n- Prometheus:部署Prometheus Operator实现,作为Deployment运行,用于抓取Metrics和报警。\n- Grafana:部署Grafana Operator实现,用于仪表盘展示。\n三、Kubernetes监控指标K8S本身的监控指标\n1. CPU利用率:包括节点CPU利用率、Pod CPU利用率、容器CPU利用率等,用于监控CPU资源使用情况。\n2. 内存利用率:包括节点内存利用率、Pod内存利用率、容器内存利用率等,用于监控内存资源使用情况。\n3. 网络流量:节点网络流量、Pod网络流量、容器网络流量,用于监控网络收发包大小和带宽利用率。\n4. 磁盘使用率:节点磁盘使用率,用于监控节点磁盘空间使用情况。\n5. Pod状态:Pod的Running、Waiting、Succeeded、Failed等状态数量,用于监控Pod运行状态。\n6. 节点状态:节点的Ready、NotReady和Unreachable状态数量,用于监控节点运行状态。\n7. 容器重启次数:单个容器或Pod内所有容器的重启次数,用于监控容器稳定性。\n8. API服务指标:Kubernetes API Server的请求LATENCY、请求QPS、错误码数量等,用于监控API Server性能。\n9. 集群组件指标:etcd、kubelet、kube-proxy等组件的运行指标,用于监控组件运行状态。这些都是Kubernetes集群运行状态的关键指标,通过Prometheus等工具可以进行收集和存储,然后在Grafana中设计相应的Dashboard进行可视化展示。当这些指标超出正常范围时,也可以根据阈值设置报警,保证Kubernetes集群和服务的稳定运行。\n例如:\n- CPU利用率超过80%报警\n- 内存利用率超过90%报警\n- 网络流量&#x2F;磁盘空间突增报警\n- Pod&#x2F;节点 NotReady状态超过10%报警 \n- API Server请求LATENCY超过200ms报警\n- etcd节点Down报警等等。\n这些报警规则的设置需要根据集群大小和服务负载进行评估。\n四、使用Prometheus监控k8s部署前准备\n下载prometheus、grafana、node-exporter\nGitHub - redhatxl&#x2F;k8s-prometheus-grafana: k8s monitor of prometheus-grafanak8s monitor of prometheus-grafana. Contribute to redhatxl&#x2F;k8s-prometheus-grafana development by creating an account on GitHub.https://github.com/redhatxl/k8s-prometheus-grafana.git  \n \n 这里我下载下来上传了该附件，可自行下载\nhttps://download.csdn.net/download/qq_40322236/87918488\n拉去镜像：\nbash[root@master ~]# docker pull prom/node-exporter \n[root@master ~]# docker pull prom/prometheus:v2.0.0\n[root@master ~]# docker pull grafana/grafana:6.1.4解压zip得到如下\nbash[root@master prometheus]# ls\nk8s-prometheus-grafana-master  k8s-prometheus-grafana-master.zip\n[root@master prometheus]# cd k8s-prometheus-grafana-master/\n[root@master k8s-prometheus-grafana-master]# ls\ngrafana  node-exporter.yaml  prometheus  README.md\n[root@master k8s-prometheus-grafana-master]# ll\n总用量 8\ndrwxr-xr-x 2 root root  81 1月   7 2019 grafana\n-rw-r--r-- 1 root root 714 6月  13 22:29 node-exporter.yaml\ndrwxr-xr-x 2 root root 106 1月   7 2019 prometheus\n-rw-r--r-- 1 root root 117 6月  13 22:29 README.md先安装node-exporter.yaml\nbash[root@master k8s-prometheus-grafana-master]# kubectl apply -f node-exporter.yaml\ndaemonset.apps/node-exporter created\nservice/node-exporter created\n[root@master k8s-prometheus-grafana-master]# kubectl get pods -A\nNAMESPACE       NAME                                        READY   STATUS    RESTARTS   AGE\nkube-system     coredns-9d85f5447-7nhgv                     1/1     Running   5          22d\nkube-system     coredns-9d85f5447-sfr9j                     1/1     Running   5          22d\nkube-system     etcd-master                                 1/1     Running   6          22d\nkube-system     kube-apiserver-master                       1/1     Running   6          22d\nkube-system     kube-controller-manager-master              1/1     Running   7          22d\nkube-system     kube-flannel-ds-77w47                       1/1     Running   4          22d\nkube-system     kube-flannel-ds-trrcv                       1/1     Running   6          22d\nkube-system     kube-flannel-ds-xcqgs                       1/1     Running   4          22d\nkube-system     kube-proxy-674k4                            1/1     Running   4          22d\nkube-system     kube-proxy-7l9bk                            1/1     Running   6          22d\nkube-system     kube-proxy-kd449                            1/1     Running   4          22d\nkube-system     kube-scheduler-master                       1/1     Running   7          22d\nkube-system     metrics-server-59d984f5b7-2nt8z             2/2     Running   8          22d\nkube-system     node-exporter-5swxx                         1/1     Running   1          3d\nkube-system     node-exporter-pbx6c                         1/1     Running   1          3d\n[root@master k8s-prometheus-grafana-master]# kubectl get daemonset -A\nNAMESPACE     NAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                 AGE\nkube-system   kube-flannel-ds   3         3         3       3            3           &lt;none&gt;                        22d\nkube-system   kube-proxy        3         3         3       3            3           beta.kubernetes.io/os=linux   22d\nkube-system   node-exporter     2         2         2       2            2           &lt;none&gt;                        3d\n[root@master k8s-prometheus-grafana-master]# kubectl get service -A\nNAMESPACE       NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                        AGE\ndefault         kubernetes       ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP                                        22d                              3d\nkube-system     kube-dns         ClusterIP   10.96.0.10       &lt;none&gt;        53/UDP,53/TCP,9153/TCP                         22d\nkube-system     node-exporter    NodePort    10.102.164.149   &lt;none&gt;        9100:31672/TCP                                 3d                                 3d部署prometheus\nbash[root@master k8s-prometheus-grafana-master]# cd prometheus/\n[root@master prometheus]# ls\nconfigmap.yaml  prometheus.deploy.yml  prometheus.svc.yml  rbac-setup.yaml\n[root@master prometheus]# kubectl apply -f rbac-setup.yaml\nclusterrole.rbac.authorization.k8s.io/prometheus configured\nserviceaccount/prometheus configured\nclusterrolebinding.rbac.authorization.k8s.io/prometheus configured\n[root@master prometheus]# kubectl apply -f configmap.yaml\nconfigmap/prometheus-config configured\n[root@master prometheus]# kubectl apply -f prometheus.deploy.yml\ndeployment.apps/prometheus created\n[root@master prometheus]# kubectl apply -f prometheus.svc.yml\nservice/prometheus created部署grafana\nbash[root@master prometheus]# cd ../grafana/\n[root@master grafana]# ls\ngrafana-deploy.yaml  grafana-ing.yaml  grafana-svc.yaml\n[root@master grafana]# kubectl apply -f grafana-deploy.yaml\ndeployment.apps/grafana-core created\n[root@master grafana]# kubectl apply -f grafana-svc.yaml\nservice/grafana created\n[root@master grafana]# kubectl apply -f grafana-ing.yaml\ningress.extensions/grafana created 查看前面安装的pod和service \n\n\n访问http://192.168.159.180:31672/metrics，这是node-exporter采集的数据\n \n 访问prometheus地址http://192.168.159.180:30003/graph\n\n访问grafana的地址http://192.168.159.180:32418/login，默认用户密码都是admin，登录后会默认让你修改密码\n\n \n添加grafana 数据源，选择prometheus \n \n \n \n \n把K8S的Dashboard的模板导入\n\n\n\n导入315模板等待一会,或者可以下载好json文件，然后复制到下面的文本框中，点击load\n\n\n最后就部署完成啦\n\nArticle link： https://tqgoblin.site/post/csdn/Prometheus监控K8S/  Author： Stephen  \n","slug":"csdn/Prometheus监控K8S","date":"2021-06-16T15:48:13.000Z","categories_index":"运维","tags_index":"kubernetes prometheus","author_index":"Stephen"},{"id":"b048cd9598c07b92af477eb0e98be3f6","title":"Kubernetes master集群高可用","content":"实现方案Kubernetes master高可用一般有三种实现方案:\n1. kubeadm 高可用安装使用kubeadm工具安装Kubernetes集群。通过增加master节点数量和指定vip实现master高可用。\n具体步骤如下:\n- 安装一主两备master节点(由kubeadm完成)\n- 配置vip(使用keepalived或haproxy等工具)\n- kubeadm join时指定apiserver-vip参数(kubeadm join … --apiserver-advertise-address apiserver-vip)\n- 各节点apiserver配置service-cluster-ip-range参数,指向vip这种方案简单易行,kubeadm帮助完成大部分复杂配置,但是控制面扩展性较差,可靠性依赖前端vip产品。\n2. etcd集群 + apiserver高可用以etcd集群为后端存储,部署多个apiserver实例,并进行LB实现高可用。\n具体步骤如下:\n- 部署etcd集群(3个或5个节点)\n- 多个master节点分别部署apiserver,指向etcd集群\n- 使用LB(如nginx)反向代理apiserver,实现负载均衡这种方案扩展性好,控制面高可用性高,但是部署和配置比较复杂,依赖额外的LB设备。\n3. 使用CSKU(企业版本Kubernetes)使用企业版本Kubernetes(如GKE),它自带 master高可用实现。通过在GKE控制台选择master节点数,与高可用特性,GKE将自动完成以下配置:\n- 部署etcd集群\n- 部署多个master副本\n- 通过内置LB实现apiserver负载均衡这种方案最简单,完全由企业版本Kubernetes平台实现和运维,但是成本较高。\n这里我将介绍前两种实现\n方式一1. 准备3台机器,配置主机名和hosts\n\n\n\n\n\n\n\n\n\nmaster1\nmaster2\nmaster3\n编辑所有节点的&#x2F;etc&#x2F;hosts,添加三节点映射: \n\n\n\n\n\n\n\n\n\n192.168.0.1 master1192.168.0.2 master2192.168.0.3 master3\n2. 在所有的master节点安装kubeadm,kubelet和kubectl\n\n\n\n\n\n\n\n\n\napt install -y kubeadm kubelet kubectl\n3. 指定vip(如10.0.0.10),在master1上使用keepalived配置,其他master节点配置keepalived backup\nmaster1:\n\n\n\n\n\n\n\n\n\nvrrp_instance VI_1 {    state MASTER    interface ens4    virtual_router_id 50    priority 100    advert_int 1    authentication {        auth_type PASS        auth_pass 1111    }    virtual_ipaddress {        10.0.0.10    }        }\n  其它master节点:\n\n\n\n\n\n\n\n\n\nvrrp_instance VI_1 {    state BACKUP     interface ens4     virtual_router_id 50    priority 50    advert_int 1    authentication {        auth_type PASS        auth_pass 1111     }    virtual_ipaddress {        10.0.0.10    }} \n4. 在所有节点初始化kubeadm配置文件\n\n\n\n\n\n\n\n\n\nkubeadm config print init-defaults &gt; kubeadm.yaml\n修改kubeadm.yaml,指定apiserver地址为vip:\n\n\n\n\n\n\n\n\n\n​apiVersion: kubeadm.k8s.io&#x2F;v1beta2 kind: InitConfigurationlocalAPIEndpoint: advertiseAddress: “10.0.0.10”\n5. 在master1节点执行kubeadm init,使用–config指定kubeadm.yaml文件\n\n\n\n\n\n\n\n\n\nkubeadm init --config&#x3D;kubeadm.yaml\n6. 在其他master节点执行kubeadm join,同样指定apiserver-vip\n\n\n\n\n\n\n\n\n\nkubeadm join 10.0.0.10:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:1234..cdef --apiserver-advertise-address&#x3D;10.0.0.10\n7. 将master节点的kubeconfig文件拷贝到其它节点至此,使用kubeadm工具部署的Kubernetes master高可用集群已完成!客户端只需要访问vip(10.0.0.10)即可,keepalived会自动将请求转发到可用的master节点。\n        至此,使用kubeadm工具部署的Kubernetes master高可用集群已完成!客户端只需要访问vip(10.0.0.10)即可,keepalived会自动将请求转发到可用的master节点。这种方案简单易行,但是扩展性和可靠性略差,依赖第三方软件(如keepalived)实现LB和vip。但对测试和体验Kubernetes HA足以。\n方式二1. 准备3台机器作为etcd集群:\n- etcd1:10.0.0.11- etcd2:10.0.0.12- etcd3:10.0.0.13\n2. 在etcd1上部署etcd\n\n\n\n\n\n\n\n\n\netcd --name etcd1 \\ --listen-client-urls https://10.0.0.11:2379 \\ --advertise-client-urls https://10.0.0.11:2379 \\ --listen-peer-urls https://10.0.0.11:2380 \\ --initial-advertise-peer-urls https://10.0.0.11:2380 \\ --initial-cluster etcd1&#x3D;https://10.0.0.11:2380,etcd2=https://10.0.0.12:2380,etcd3=https://10.0.0.13:2380 \\ --initial-cluster-token etcd-token\n3. 在etcd2和etcd3节点也启动etcd,命令与etcd1相同,只是名称和地址不同\n4. 准备3台master节点:master1,master2,master3\n5.所有master节点安装kube-apiserver、kube-controller-manager和kube-scheduler\n6. 配置kube-apiserver\n- --etcd-servers: 指定etcd集群地址\n- --service-cluster-ip-range: 指定Cluster IP地址段\n- --secure-port: apiserver端口(如6443)\n- 其它参数参考 [kube-apiserver配置文档]( kube-apiserver | Kubernetes)\nmaster1例如:\n\n\n\n\n\n\n\n\n\nkube-apiserver --etcd-servers&#x3D;https://10.0.0.11:2379,https://10.0.0.12:2379,https://10.0.0.13:2379  --service-cluster-ip-range&#x3D;10.0.0.0&#x2F;24 --secure-port&#x3D;6443 ​-bind-address&#x3D;10.0.0.11  …\nmaster2,master3…也类似,修改bind-address为对应节点IP\n7. 配置反向代理(如nginx)实现kube-apiserver高可用负载均衡\n\n\n\n\n\n\n\n\n\nupstream kube-apiserver {     server 10.0.0.11:6443;     server 10.0.0.12:6443;     server 10.0.0.13:6443; } server {    listen 6443;     proxy_pass http://kube-apiserver; }\n8. 其他master组件连接nginx的6443端口在\n所有master节点:\n\n\n\n\n\n\n\n\n\nkube-controller-manager --master&#x3D;http://10.0.0.11:6443 …kube-scheduler --master&#x3D;http://10.0.0.11:6443 …\n9. Node节点kubelet连接nginx的6443端口\n10. kubectl客户端也连接nginx的6443端口至此,实现了高可用的多master控制面,具有高扩展性和高可用性。etcd本身作为Kubernetes的关键存储也具备高可用,nginx实现简单可靠的LB。\n        这种方案设计得较为复杂,但是更加灵活可靠。适用于大型产品级Kubernetes集群,可以根据需要扩展apiserver和etcd节点。也是实际Kubernetes控制面部署的经典模式。\n访问集群方式访问Kubernetes高可用集群,有以下几种方式：\n1. kubectl客户端 connects 到nginx的6443端口,nginx会自动将请求转发到可用的kube-apiserver实例上。所以,您只需要将kubectl指向任一master节点的6443端口即可:\n\n\n\n\n\n\n\n\n\nkubectl --server&#x3D;https://10.0.0.11:6443 get nodes\n2. API Server Endpoint对外暴露的API Server Endpoint也是nginx的6443端口,所以调用Kubernetes API可以像这样:\n\n\n\n\n\n\n\n\n\ncurl https://10.0.0.11:6443/api/v1/nodes \n3. DNS在Kubernetes集群内部,kube-dns会解析kubernetes.default.svc.cluster.local到nginx的6443端口。所以在集群内的Pod也可以通过该DNS名称访问API Server。\n4. Service可以在集群内创建kubernetes服务,将类型设置为NodePort或LoadBalancer,开放6443端口,对外提供访问入口。例如:\n\n\n\n\n\n\n\n\n\nyamlapiVersion: v1kind: Servicemetadata:  name: kubernetesspec:  type: NodePort  ports:  - port: 6443    targetPort: 6443    protocol: TCP  selector:    component: apiserver \n        创建该服务后,外部可以通过Node的ANY IP:31643端口访问(31643是随机分配的端口)。\n        以上就是几种访问Kubernetes高可用集群的方法。总体来说,对内可以直接使用kubernetes服务域名(推荐);对外可以通过NodePort服务或直接通过nginx的6443端口调用API(适用于测试)。\n        在日常使用中,kubectl命令行工具是最常用的方式。只需要在每个节点安装kubectl,并指定正确的apiserver地址即可使用。 \n相关延申keepalived        这里最重要的是VRRP虚拟IP，VRRP虚拟IP(Virtual Router Redundancy Protocol Virtual IP)是keepalived实现高可用的关键概念。\n它是一个浮动的IP地址,可以在keepalived集群中的主节点和备节点之间漂移。客户端并不直接连接主节点或备节点,而是连接这个虚拟IP。\n当主节点出现故障时,VRRP虚拟IP会从主节点漂移到备节点,客户端连接的IP地址不变,实现了业务的故障切换。VRRP虚拟IP具有以下特征:\n1. 它是一个逻辑IP,并不对应于主节点或备节点的物理IP地址。\n2. 它会在keepalived主节点和备节点之间漂移,实现高可用。\n3. 客户端只关注VRRP虚拟IP,不需要关心主节点或备节点的物理IP地址。\n4. VRRP虚拟IP的MAC地址会随着IP漂移而改变,交换机会更新MAC表,无需人工干预。\n5. 一个VRRP实例可以有多个VRRP虚拟IP,所有的虚拟IP会同时漂移。\n6. VRRP虚拟IP所在的子网需要在主节点和备节点的网卡上都有配置。\nVRRP虚拟IP是keepalived实现高可用的基石,当主节点出现故障,它实现了与备节点的无缝切换,为客户端提供了一个逻辑的浮动入口IP,自动实现了故障转移,这也是VRRP协议的初衷。\n综上,VRRP虚拟IP的主要作用是:\n- 为客户端提供一个逻辑入口IP\n- 实现与备节点的故障切换\n- 隐藏集群主节点与备节点的物理IP地址\n- 简化客户端的配置\nngnix方式nginx大家都很熟悉，反向代理服务器，暴露一个端口根据上下文负载均衡所有master节点地址\nkubectlkubectl是k8s的请求客户端命令，kubectl命令内部根据命令带的参数信息封装了个 restful http请求，向kube-apiserver发送请求。接下来列举几个命令的实际http请求的对应请求：\n获取Pod列表:\n\n\n\n\n\n\n\n\n\nGET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;{namespace}&#x2F;pods \n创建Pod:\n\n\n\n\n\n\n\n\n\nPOST &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;{namespace}&#x2F;pods\n获取Service列表:\n\n\n\n\n\n\n\n\n\nGET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;{namespace}&#x2F;services\n创建Service:\n\n\n\n\n\n\n\n\n\nPOST &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;{namespace}&#x2F;services\n获取Deployment列表:\n\n\n\n\n\n\n\n\n\nGET &#x2F;apis&#x2F;apps&#x2F;v1&#x2F;namespaces&#x2F;{namespace}&#x2F;deployments \n创建Deployment:\n\n\n\n\n\n\n\n\n\nPOST &#x2F;apis&#x2F;apps&#x2F;v1&#x2F;namespaces&#x2F;{namespace}&#x2F;deployments\n获取Node列表:\n\n\n\n\n\n\n\n\n\nGET &#x2F;api&#x2F;v1&#x2F;nodes\n获取Namespace列表: \n\n\n\n\n\n\n\n\n\nGET &#x2F;api&#x2F;v1&#x2F;namespaces\n创建Namespace:\n\n\n\n\n\n\n\n\n\nPOST &#x2F;api&#x2F;v1&#x2F;namespaces  \n获取Secret列表: \n\n\n\n\n\n\n\n\n\nGET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;{namespace}&#x2F;secrets \n创建Secret:\n\n\n\n\n\n\n\n\n\nPOST &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;{namespace}&#x2F;secrets    \n获取ConfigMap列表:\n\n\n\n\n\n\n\n\n\nGET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;{namespace}&#x2F;configmaps  \n创建ConfigMap:\n\n\n\n\n\n\n\n\n\nPOST &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;{namespace}&#x2F;configmaps  \n执行kubectl exec:\n\n\n\n\n\n\n\n\n\nPOST &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;{namespace}&#x2F;pods&#x2F;{pod}&#x2F;exec\n执行yaml文件的apply or create命令\n\n\n\n\n\n\n\n\n\ncurl -X POST -H “Content-Type: application&#x2F;json” -H “Authorization: Bearer $TOKEN” -d ‘{“apiVersion”:”v1”, “kind”:”Pod”…}‘  https://10.0.0.1:6443/api/v1/namespaces/default/pods\n这些只是常见API的一部分,kubectl封装的API还有非常多,它通过构建不同的HTTP请求,以RESTful API风格与Kubernetes API Server进行交互,从而实现所有kubectl的功能。\nArticle link： [https://tqgoblin.site/post/csdn/Kubernetes master集群高可用&#x2F;](https://tqgoblin.site/post/csdn/Kubernetes master集群高可用&#x2F;)  Author： Stephen  \n","slug":"csdn/Kubernetes master集群高可用","date":"2021-06-01T09:21:12.000Z","categories_index":"运维","tags_index":"kubernetes 容器","author_index":"Stephen"},{"id":"0fca9df791e8d84a1257cd45f3a83b63","title":"kubernetes环境搭建","content":"目录\nkubernetes 环境搭建\n环境说明\n集群搭建方式\n环境准备\n初始化环境\n安装kubernetes\n安装docker\n 安装kubernetes组件\n准备集群镜像\n集群初始化\n网络插件的安装\nkubernetes部署nginx\n版本说明\n\nkubernetes 环境搭建1.一主多从: 在学习阶段我们只需要一台Master节点和多台Node节点，搭建简单,有单机发生故障风险，适合用于测试环境\n2.多主多从:多台Master节点和多台Node节点，搭建有一些麻烦,安全系数高，适合用于生产环境\n环境说明1.基于云服务器构建kubernetes 环境\n2.本地通过虚拟机构建kubernetes 环境\n安装Centos7.9 准备一个干净的，让后直接克隆三台虚拟机即可\n启动虚拟机过程 ，建议分开的形式启动这三台虚拟机\n虚拟机的搭建可以参考另一个文章：虚拟机上搭建linux环境\n\n集群搭建方式1.kubeadm\nkubeadm是一个工具，提供kubeadm init 和kubeadm join，用于快速部署Kubernetes集群\n下载地址：Kubeadm | Kubernetes\n2.二进制 从官方下载发行版的二进制包，手动部署每个组件，组成Kubernetes集群　　\n下载地址：Releases · kubernetes&#x2F;kubernetes · GitHub\n3.minikube 一个用于快速搭建单节点kubernetes的工具\n下载地址：minikube start | minikube\n环境准备1.一个主多个工作节点: 一台Master节点和多台Node节点，搭建简单,但是有单机故障风险，适合用于测试环境\n2.多主节点多工作节点:多台Master节点和多台Node节点，搭建麻烦,安全性高，适合用于生产环境\n程序员初学练习，采用一主多从即可\nip地址类型操作系统服务配置192.168.159.180masterCentos7.92核CPU 2G内存 20G硬盘192.168.159.181node1Centos7.92核CPU 2G内存 20G硬盘192.168.159.182node2Centos7.92核CPU 2G内存 20G硬盘\n\n使用MobaXterm远程连接到这三台服务器\n初始化环境需要在这三台服务器上执行以下操作：\n1.检查操作系统的版本\n#此方式下安装kubernetes集群要求Centos版本要在7.5或之上\ncat &#x2F;etc&#x2F;redhat-release\n\n2.主机名解析为了方便后面集群节点间的直接调用，在这配置一下主机名解析, 企业中推荐使用内部DNS服务器#主机名成解析编辑三台服务器的&#x2F;etc&#x2F;hosts文件，添加下面内容\n\n\n\n\n\n\n\n\n\nvi &#x2F;etc&#x2F;hosts\n192.168.159.180  master192.168.159.181  node1192.168.159.182  node2\n\n3.时间同步\nkubernetes要求集群中的节点时间必须精确一致, 这里直接使用chronyd服务从网络同步时间。企业中建议配置内部的时间同步服务器\n\n\n\n\n\n\n\n\n\nyum install -y chronysystemctl start chronydsystemctl enable chronyd\n4.禁用iptables和firewalld服务\nkubernetes和docker在运行中会产生大量的iptables规则，为了不让系统规则跟它们混淆，直接关闭系统的规则\n\n\n\n\n\n\n\n\n\n# 1关闭firewalld服务[root@master ~]# systemctl stop firewalld[root@master ~]# systemctl disable firewalld\n# 2关闭iptables服务[root@master ~]# systemctl disabled iptables[root@master ~]# systemctl disable iptables\n5.关闭selinux\nselinux是linux系统下的一个安全服务，如果不关闭它，在安装集群中会产生各种各样的奇葩问题\n\n\n\n\n\n\n\n\n\nvi &#x2F;etc&#x2F;selinux&#x2F;config 文件，修改SELINUX的值为disabled#注意修改完毕之后需要重启 linux服务SELINUX&#x3D;disabled\n \n6.禁用swap分区\nswap分区指的是虚拟内存分区，它的作用是在物理内存使用完之后，将磁盘空间虚拟成内存来使用\n启用swap设备会对系统的性能产生非常负面的影响，因此kubernetes要求每个节点都要禁用swap设备\n但是如果因为某些原因确实不能关闭swap分区，就需要在集群安装过程中通过明确的参数进行配置说明\n\n\n\n\n\n\n\n\n\n#编辑分区配置文件&#x2F;etc&#x2F;fstab,注释掉swap分区一行vi &#x2F;etc&#x2F;fstab\n 7.修改Linux内核参数\n\n\n\n\n\n\n\n\n\n#修改linux的内核参数，添加网桥过滤和地址转发功能vi &#x2F;etc&#x2F;sysctl.d&#x2F;kubernetes.conf文件，添加如下配置:net.bridge.bridge-nf-call-ip6tables &#x3D; 1net.bridge.bridge-nf-call-iptables &#x3D; 1net.ipv4.ip_forward &#x3D; 1#重新加载配置[root@master ~]# sysctl -p#加载网桥过滤模块[root@master ~]# modprobe br_netfilter#查看网桥过滤模块是否加载成功[root@master ~]# lsmod  | grep br_netfilter\n8.配置ipvs功能\n在kubernetes中service有两种代理模型，一种是基于iptables的, 一种是基于ipvs的两者比较的话，ipvs的性能明显要高一些,但是如果要使用它，需要手动载入ipvs模块\nk8s service 会使用 ipvs&#x2F;iptables\n\n\n\n\n\n\n\n\n\n# 1安装ipset和ipvsadm[root@master ~]# yum install ipset ipvsadmin -y\n# 2添加需要加载的模块写入脚本文件[ root@master ~]# cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules#!&#x2F;bin&#x2F;bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4EOF\n# 3为脚本文件添加执行权限[root@master ~]# chmod +x &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules\n# 4执行脚本文件[root@master ~]# sh +x &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules\n# 5查看对应的模块是否加载成功[root@master ~]# lsmod | grep -e ip_vs -e nf_conntrack_ipv4\n9.重启Linux系统\n\n\n\n\n\n\n\n\n\nreboot\n安装kubernetes安装docker\n\n\n\n\n\n\n\n\n# 1切换镜像源[root@master ~]# wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O &#x2F;etc&#x2F;yum.repos.d&#x2F;docker-ce.repo\n如果报错 -bash: wget: 未找到命令则执行： yum -y install wget# 2查看当前镜像源中支持的docker版本[root@master ~]# yum list docker-ce --showduplicates\n# 3安装特定版本的docker-ce#指定–setopt&#x3D;obsoletes&#x3D;0,否则yum会自动安装更高版本[root@master ~]# yum install --setopt&#x3D;obsoletes&#x3D;0 docker-ce-18.06.3.ce-3.el7 -y\n# 4添加一个配置文件 阿里云加速镜像[root@master ~]# mkdir &#x2F;etc&#x2F;docker[ root@master ~]# cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json{“exec-opts”: [“native.cgroupdriver&#x3D;systemd”],“registry-mirrors”: [\n“https://62n2zqcw.mirror.aliyuncs.com“,“https://docker.mirrors.ustc.edu.cn“,“https://hub-mirror.c.163.com“,“https://reg-mirror.qiniu.com“,“https://registry.docker-cn.com“\n]}EOF\n# 5启动docker[root@master ~]# systemctl start docker[root@master ~]# systemctl enable docker\n# 6检查docker状态和版本[root@master ~]# docker --version\n\n安装kubernetes组件\n\n\n\n\n\n\n\n\n#1.由于kubernetes的镜像源在国外，速度比较慢，这里切换成国内的镜像源#2.vi  &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo,添加下面的配置[kubernetes]name&#x3D;Kubernetesbaseurl&#x3D;http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86\\_64enabled&#x3D;1gpgcheck&#x3D;0repo_gpgcheck&#x3D;0gpgkey&#x3D;http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg      http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\n#3.安装kubeadm、kubelet和kubectl[root@master ~]# yum install --setopt&#x3D;obsoletes&#x3D;0 kubeadm-1.17.4-0 kubelet-1.17.4-0 kubectl-1.17.4-0 -y#4.配置kubelet的cgroup#5.vi &#x2F;etc&#x2F;sysconfig&#x2F;kubelet,添加下面的配置KUBELET_CGROUP_ARGS&#x3D;”–cgroup-driver&#x3D;systemd”KUBE_PROXY_MODE&#x3D;”ipvs”\n#6.设置kubelet开机自启[root@master ~]# systemctl enable kubelet  \n \n\n\n准备集群镜像在阿里云仓库中存在该镜像 在更改为k8s 官方的名称,同时需要在所有节点操作\n\n\n\n\n\n\n\n\n\n#在安装k8s集群之前，必须要提前准备好集群需要的镜像，所需镜像可以通过下面命令查看1.kubeadm config images listk8s.gcr.io&#x2F;kube-apiserver:v1.17.17k8s.gcr.io&#x2F;kube-controller-manager:v1.17.17k8s.gcr.io&#x2F;kube-scheduler:v1.17.17k8s.gcr.io&#x2F;kube-proxy:v1.17.17k8s.gcr.io&#x2F;pause:3.1k8s.gcr.io&#x2F;etcd:3.4.3-0k8s.gcr.io&#x2F;coredns:1.6.52.下载镜像#此镜像在k8s的仓库中,由于网络原因,无法连接，下面提供了-种替代方案images&#x3D;(kube-apiserver:v1.17.4kube-controller-manager:v1.17.4kube-scheduler:v1.17.4kube-proxy:v1.17.4pause:3.1etcd:3.4.3-0coredns:1.6.5)\nfor imageName in ${images[@]} ; dodocker pull registry.cn-hangzhou.aliyuncs.com&#x2F;google_containers&#x2F;$imageNamedocker tag registry.cn-hangzhou.aliyuncs.com&#x2F;google_containers&#x2F;$imageName k8s.gcr.io&#x2F;$imageNamedocker rmi registry.cn-hangzhou.aliyuncs.com&#x2F;google_containers&#x2F;$imageNamedone\n执行docker images\n \n集群初始化只需要在master节点上执行即可\n\n\n\n\n\n\n\n\n\nkubeadm init \\--kubernetes-version&#x3D;v1.17.4 \\--pod-network-cidr&#x3D;10.244.0.0&#x2F;16 \\--image-repository registry.aliyuncs.com&#x2F;google_containers \\--service-cidr&#x3D;10.96.0.0&#x2F;12 \\--apiserver-advertise-address&#x3D;192.168.159.180\n注意：–image-repository 需要更改为阿里云加速镜像 --apiserver-advertise-address&#x3D;192.168.159.180  修改为自己的 master的ip。初始化大概需要花费1分钟左右journalctl -xeu kubelet  查看日志\n\n备份，留着在node1,node2节点上执行：\n\n\n\n\n\n\n\n\n\n kubeadm join 192.168.159.180:6443 --token 75z5j1.oxokr4v2gfekxzna \\    --discovery-token-ca-cert-hash sha256:9afa207224044011b459ca5d3e970d7ab7c5a2ce48a6d8d1eb9f82f9f9ffb422\n2.创建必要文件\n\n\n\n\n\n\n\n\n\nmkdir -p $HOME&#x2F;.kube\nsudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config\nsudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config\n3.拷贝备份部分到其他两个节点执行 加入 k8s集群环境 \n\n \n4.执行 kubectl get nodes\n\n网络插件的安装kube-flannel.yml 上传到Master节点任意位置\n\n\n\n\n\n\n\n\n\n-–apiVersion: policy&#x2F;v1beta1kind: PodSecurityPolicymetadata:  name: psp.flannel.unprivileged  annotations:    seccomp.security.alpha.kubernetes.io&#x2F;allowedProfileNames: docker&#x2F;default    seccomp.security.alpha.kubernetes.io&#x2F;defaultProfileName: docker&#x2F;default    apparmor.security.beta.kubernetes.io&#x2F;allowedProfileNames: runtime&#x2F;default    apparmor.security.beta.kubernetes.io&#x2F;defaultProfileName: runtime&#x2F;defaultspec:  privileged: false  volumes:  - configMap  - secret  - emptyDir  - hostPath  allowedHostPaths:  - pathPrefix: “&#x2F;etc&#x2F;cni&#x2F;net.d”  - pathPrefix: “&#x2F;etc&#x2F;kube-flannel”  - pathPrefix: “&#x2F;run&#x2F;flannel”  readOnlyRootFilesystem: false  # Users and groups  runAsUser:    rule: RunAsAny  supplementalGroups:    rule: RunAsAny  fsGroup:    rule: RunAsAny  # Privilege Escalation  allowPrivilegeEscalation: false  defaultAllowPrivilegeEscalation: false  # Capabilities  allowedCapabilities: [‘NET_ADMIN’, ‘NET_RAW’]  defaultAddCapabilities: []  requiredDropCapabilities: []  # Host namespaces  hostPID: false  hostIPC: false  hostNetwork: true  hostPorts:  - min: 0    max: 65535  # SELinux  seLinux:    # SELinux is unused in CaaSP    rule: ‘RunAsAny’-–kind: ClusterRoleapiVersion: rbac.authorization.k8s.io&#x2F;v1metadata:  name: flannelrules:- apiGroups: [‘extensions’]  resources: [‘podsecuritypolicies’]  verbs: [‘use’]  resourceNames: [‘psp.flannel.unprivileged’]- apiGroups:  - “”  resources:  - pods  verbs:  - get- apiGroups:  - “”  resources:  - nodes  verbs:  - list  - watch- apiGroups:  - “”  resources:  - nodes&#x2F;status  verbs:  - patch-–kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io&#x2F;v1metadata:  name: flannelroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: flannelsubjects:- kind: ServiceAccount  name: flannel  namespace: kube-system-–apiVersion: v1kind: ServiceAccountmetadata:  name: flannel  namespace: kube-system-–kind: ConfigMapapiVersion: v1metadata:  name: kube-flannel-cfg  namespace: kube-system  labels:    tier: node    app: flanneldata:  cni-conf.json: |    {      “name”: “cbr0”,      “cniVersion”: “0.3.1”,      “plugins”: [        {          “type”: “flannel”,          “delegate”: {            “hairpinMode”: true,            “isDefaultGateway”: true          }        },        {          “type”: “portmap”,          “capabilities”: {            “portMappings”: true          }        }      ]    }  net-conf.json: |    {      “Network”: “10.244.0.0&#x2F;16”,      “Backend”: {        “Type”: “vxlan”      }    }-–apiVersion: apps&#x2F;v1kind: DaemonSetmetadata:  name: kube-flannel-ds  namespace: kube-system  labels:    tier: node    app: flannelspec:  selector:    matchLabels:      app: flannel  template:    metadata:      labels:        tier: node        app: flannel    spec:      affinity:        nodeAffinity:          requiredDuringSchedulingIgnoredDuringExecution:            nodeSelectorTerms:            - matchExpressions:              - key: kubernetes.io&#x2F;os                operator: In                values:                - linux      hostNetwork: true      priorityClassName: system-node-critical      tolerations:      - operator: Exists        effect: NoSchedule      serviceAccountName: flannel      initContainers:      - name: install-cni-plugin        image: rancher&#x2F;mirrored-flannelcni-flannel-cni-plugin:v1.0.0        command:        - cp        args:        - -f        - &#x2F;flannel        - &#x2F;opt&#x2F;cni&#x2F;bin&#x2F;flannel        volumeMounts:        - name: cni-plugin          mountPath: &#x2F;opt&#x2F;cni&#x2F;bin      - name: install-cni        image: quay.io&#x2F;coreos&#x2F;flannel:v0.15.1        command:        - cp        args:        - -f        - &#x2F;etc&#x2F;kube-flannel&#x2F;cni-conf.json        - &#x2F;etc&#x2F;cni&#x2F;net.d&#x2F;10-flannel.conflist        volumeMounts:        - name: cni          mountPath: &#x2F;etc&#x2F;cni&#x2F;net.d        - name: flannel-cfg          mountPath: &#x2F;etc&#x2F;kube-flannel&#x2F;      containers:      - name: kube-flannel        image: quay.io&#x2F;coreos&#x2F;flannel:v0.15.1        command:        - &#x2F;opt&#x2F;bin&#x2F;flanneld        args:        - –ip-masq        - –kube-subnet-mgr        resources:          requests:            cpu: “100m”            memory: “50Mi”          limits:            cpu: “100m”            memory: “50Mi”        securityContext:          privileged: false          capabilities:            add: [“NET_ADMIN”, “NET_RAW”]        env:        - name: POD_NAME          valueFrom:            fieldRef:              fieldPath: metadata.name        - name: POD_NAMESPACE          valueFrom:            fieldRef:              fieldPath: metadata.namespace        volumeMounts:        - name: run          mountPath: &#x2F;run&#x2F;flannel        - name: flannel-cfg          mountPath: &#x2F;etc&#x2F;kube-flannel&#x2F;      volumes:      - name: run        hostPath:          path: &#x2F;run&#x2F;flannel      - name: cni-plugin        hostPath:          path: &#x2F;opt&#x2F;cni&#x2F;bin      - name: cni        hostPath:          path: &#x2F;etc&#x2F;cni&#x2F;net.d      - name: flannel-cfg        configMap:          name: kube-flannel-cfg\n#在master节点执行：\n\n\n\n\n\n\n\n\n\n[root@master ~]# kubectl apply -f kube-flannel.yml\n \n耐心等待30-60s 查看状态 是否为 ready状态\n\n\n\n\n\n\n\n\n\nkubectl get nodes\n\n如果还是都是为 notready状态(我没有出现过这种情况，只要等待下图init边测running，上图就ready了)\n\n如果status没有变running则需要修改 每台节点的：vi &#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;kubeadm-flags.env # 删除–network-plugin&#x3D;cni\n#重新启动\n每台节点都执行一下下面的命令\n\n\n\n\n\n\n\n\n\nsystemctl daemon-reload\nsystemctl restart kubelet\n 如上图所示，k8s就搭建成功了！接下来以nignx部署试下k8s是否搭建成功。\nkubernetes部署nginx在Master节点上执行\n\n\n\n\n\n\n\n\n\nkubectl create deployment nginx --image&#x3D;nginx:1.14-alpine\nkubectl expose deployment nginx --port&#x3D;80 --type&#x3D;NodePort\nkubectl get pods,service\n\ndocker ps 查看到nginx运行在181节点，使用181地址加上端口号访问一下\n\n\n 可以在master节点查询到pod然后查看pod信息找到nginx落到哪个节点\n\n\n\n\n\n\n\n\n\nkubectl get pods -–查看正在运行的pod\nkubectl describe pod nginx-6867cdf567-kg88k -–查看正在运行的pod\n \n环境搭建就介绍到这！！\n版本说明这次部署的k8s版本是1.17.4，这个版本容器默认还是支持docker的。k8s 从 1.20 版本开始移除了对 Docker 的默认支持,转而默认使用 containerd 作为容器运行时。\n具体的版本变化是:\n• 在 1.20 版本中,Docker 被移除为默认的容器运行时,containerd 取而代之成为默认的容器运行时。\n• 从 1.20 版本开始,如果要继续使用 Docker 作为容器运行时,需要在 kubelet 启动参数中指定 --container-runtime&#x3D;docker 。\n• 1.22 版本将会移除 Docker 支持。从那个版本开始,只支持 containerd 作为容器运行时。\n• 如果集群中有节点运行的是 1.20 以下的 Kubernetes 版本,在升级这些节点到 1.20 或更高版本时,需要使用 --container-runtime&#x3D;docker 参数保持使用 Docker ,然后在以后升级到支持 containerd 的版本时再移除该参数。\n所以简单来说,如果要使用 Kubernetes 1.20 及以上版本,建议使用 containerd 作为默认的容器运行时。要继续使用 Docker,需要在 1.20 版本中手动指定,然后在 1.22 版本升级时需要切换到containerd。\n这一变化的原因是,containerd 比 Docker 更加轻量级、安全和可移植。community希望通过这一切换来简化 Kubernetes 容器解决方案,并减少依赖 dodcker.service 这样的系统服务。\nArticle link： https://tqgoblin.site/post/csdn/kubernetes环境搭建/  Author： Stephen  \n","slug":"csdn/kubernetes环境搭建","date":"2021-05-20T10:16:24.000Z","categories_index":"运维","tags_index":"运维 docker kubernetes","author_index":"Stephen"},{"id":"a48e6dda0c21e40880cba7e763278b04","title":"Docker","content":"简介与概述1.Docker 是一个开源的应用容器引擎，基于 Go 语言 并遵从 Apache2.0 协议开源。\nDocker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。\n2.  Docker的主要目标是‘build ，ship and run any app，anywhere’，也就是说通过对应用程序组件的封装，分发，部署，运行等生命周期的管理。使用户的app（可以是一个web应用程序或者数据库应用等）及其运行环境能够做到‘一次封装，到处运行’。\n3.Linux容器技术的出现解决了这个问题。而docker就是基于他的基础上发展过来的。将应用运行到docker容器上面，而docker容器在任何操作系统上都是一致的，这就是实现跨平台跨服务器。只需要一次配置好环境，换到别的机子上就可以一键部署好，大大简化了操作。\n\n容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app）,更重要的是容器性能开销极低。\n\n5.Docker 从 17.03 版本之后分为 CE（Community Edition: 社区版） 和 EE（Enterprise Edition: 企业版），我们用社区版就可以了。\n\nDocker实际上就是一个虚拟化轻量级linux服务器，可以解决我们在开发环境中运行配置问题。\n为什么需要使用dockerDocker：虚拟化容器技术\nDocker主要解决我们开发环境配置迁移的问题。\n1.我们现在开发了一个javaweb项目，需要依赖很多环境配置 比如：Tomcat、JDK环境、Nginx、Redis环境等。\n2.本地需要安装这些环境Tomcat、JDK环境、Nginx、Redis环境等，在打war包给运维部署在linux服务器，运维人员也需要在linux服务器上安装Tomcat、JDK环境、Nginx、Redis环境。\n3.但是有时候可能会发生这些问题：我在本地运行环境没有问题，但是打包到Linux服务器运行总是遇到很多错误，大多数由于一些版本冲突影响。\n4.所以在这时候我们就可以使用docker部署和安装软件就非常方便，直接将该springboot项目制作成一个镜像文件，镜像文件中包含jdk版本 tomcat版本信息  直接部署linux即可，减少依赖冲突概率。\n看看linux安装mysql\nlinux下在线安装mysql(完整版)_linux服务器在线安装mysql5.7_zhangsan分享之家的博客-CSDN博客 在不同的linux内核中安装\nMysql很容易发生版本冲突的问题。\n在对比docker安装mysql\ndocker pull mysql:5.7docker create --name mysql3308 -e MYSQL_ROOT_PASSWORD=root -p 3308:3306 mysql:5.7\n\nDocker最终解决了运行环境配置中的问题。—-镜像文件底层封装好了\nSpringboot 核心思想—-\n使用docker的好处\n简化配置 安装创建非常的方便\n代码流水线（Code Pipeline）管理 传统项目部署可能需要经过很多环节，\n\n容易产生版本的依赖冲突问题，Docker给应用提供了一个从开发到上线均一致的环境，让代码的流水线变得简单不少\n\nDevops 开发与运维一体化减少沟通的成本 （docker或者是k8s实现）\n虚拟技术 快速部署\n弹性扩容\n\n应用场景1.Web 应用的自动化打包和发布。\n2.自动化测试和持续集成、发布。\n3.在服务型环境中部署和调整数据库或其他的后台应用。\n4.从头编译或者扩展现有的 OpenShift 或 Cloud Foundry 平台来搭建自己的 PaaS 环境。\n容器与虚拟机区别什么是虚拟机：在一台物理机器上，利用虚拟化技术，虚拟出来多个操作系统，每个操作系统之间是隔离的。\n\n从下到上理解上图：\n最下面的一层就是物理机，可以是服务器，设置是一台个人电脑；\n电脑上需要安装操作系统，比如我们安装了win10的操作系统；\n再往上就是虚拟机软件了，比如我们常用的VirtualBox、VMWare，它们的作用是模拟计算机硬件；\n继续向上，就是虚拟机模拟出来的操作系统了；\n在虚拟的操作系统中，安装所需的软件、组件等。比如我们需要在虚拟操作系统中安装JDK、Tomcat等；\n最后就是具体的应用了，例如部署到Tomcat中。\nDocker ：Docker是开源的应用容器引擎\n\n 依然从下往上看：\n最下面两层，概念同上。\n往上，可以看做Docker容器的管理器。\n依赖和应用都被打包成了Docker镜像。例如，JDK、Tomcat、应用都被打包在了一起，运行在Docker容器里，容器和容器间是隔离的。\nDocker和虚拟机的区别\n1.从两者的架构图上看，虚拟机是在硬件级别进行虚拟化，模拟硬件搭建操作系统；而Docker是在操作系统的层面虚拟化，复用操作系统，运行Docker容器。\n2.Docker的速度很快，秒级，而虚拟机的速度通常要按分钟计算。\n3.Docker所用的资源更少，性能更高。同样一个物理机器，Docker运行的镜像数量远多于虚拟机的数量。\n4.虚拟机实现了操作系统之间的隔离，Docker是进程之间的隔离，虚拟机隔离级别更高、安全性方面也更强。\n5.虚拟机和Docker各有优势，不存在谁替代掉谁的问题，很多企业都采用物理机上做虚拟机，虚拟机中跑Docker的方式。\n特性容器虚拟机启动速度秒级分钟级别硬盘使用一般为MB一般GB性能接近原生弱于系统支持量单机支持上千个容器一般几十个隔离性完全隔离完全隔离\n\nDocker官网Docker Docs: How to build, share, and run applications\nhttps://www.docker.com/\nDocker安装Docker 要求 CentOS7 系统的内核版本在 3.10以上 ，查看本页面的前提条件来验证你的CentOS 版本是否支持 Docker 。\n1、通过 uname -r 命令查看你当前的内核版本\nuname -r\n\n\n使用 root 权限登录 Centos。确保 yum 包更新到最新。\n\nyum -y update该过程大概需要维持10分钟左右\n\n\n卸载旧版本(如果安装过旧版本的话)\n\nyum remove docker docker-common docker-selinux docker-engine\n\n\n安装需要的软件包， yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动依赖的\n\nyum install -y yum-utils device-mapper-persistent-data lvm2\n\n\n设置yum源\n\nyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n\n\n可以查看所有仓库中所有docker版本，并选择特定版本安装\n\nyum list docker-ce --showduplicates | sort -r\n\n\n安装docker\n\nsudo yum install -y docker-ce #由于repo中默认只开启stable仓库，故这里安装的是最新稳定版18.03.1\n\n\n启动并加入开机启动\n\nsystemctl start dockersystemctl enable docker\n\n\n验证安装是否成功(有client和service两部分表示docker安装启动都成功了)\n\ndocker version\n\nDocker快速入门Docker核心名词镜像文件\n容器\n仓库\n镜像:简单理解为就是一个安装包，里面包含容器所需要运行的的基础文件和配置信息\n，比如：redis镜像、mysql镜像等。\n镜像的来源方式：\n\n自己做镜像  比如（自己开发微服务项目）\n拉取别人制作好的镜像， 例如 nginx、mysql、redis等。\n\n容器: 容器就是镜像运行的实例，容器状态分为：初创建、运行、停止、暂停、删除， 一个镜像可以创建多个不同的容器。\n每个镜像文件都有自己独立ip信息—轻量级的linux服务器 虚拟化\n比如：镜像就是类  容器就是实例对象\n仓库: 仓库可以简单理解为，专门存储镜像文件仓库， 类似于 谷歌手机市场，统一在\n谷歌手机市场下载开发者的安装包。\nDocker 公开仓库地址： Docker hub\nDocker\nDocker官方仓库：Docker -—访问比较慢\n宿主机：当前win7操作系统\n\n\n需要制作镜像文件（springboot项目）——类似于开发者开发安装应用程序打包\n需要将我们制作好的镜像文件提交到docker仓库中—–开发者将自己的app应用程序发布安卓手机助手中。\n本地需要拉去我们docker仓库中下载镜像文件，在交给我们容器运行—用户从app市场中下载安装包运行。\n\n1.在需要制作镜像文件，将该镜像文件发布到docker仓库\ndocker仓库 dockerhub -—谷歌安卓手机市场   国内加速镜像\n阿里云、网易、科大（）—-360、小米、华为。\n\n从docker仓库下载镜像文件—–用户从手机市场中，下载软件。\ndocker运行镜像文件—-容器—独立ip访问信息—–端口号码映射\n\nDocker下载镜像原理Docker  pull 从远程docker 官方仓库下载 镜像，到本地，在使用容器运行该镜像。\n注意的是：docker官方镜像仓库地址部署在国外，下载镜像可能比较慢，建议配置国内加速镜像\n\nDocker加载镜像配置Docker -–在国外访问可能比较慢\n国内从 DockerHub 拉取镜像有时会遇到困难，此时可以配置镜像加速器。Docker 官方和国内很多云服务商都提供了国内加速器服务，例如：\n科大镜像：https://docker.mirrors.ustc.edu.cn/\n网易：https://hub-mirror.c.163.com/\n阿里云：https:&#x2F;&#x2F;&lt;你的ID&gt;.mirror.aliyuncs.com\n七牛云加速器：https://reg-mirror.qiniu.com\n当配置某一个加速器地址之后，若发现拉取不到镜像，请切换到另一个加速器地址。国内各大云服务商均提供了 Docker 镜像加速服务，建议根据运行 Docker 的云平台选择对应的镜像加速服务。\n阿里云加速镜像配置我的加速镜像：https://66mzqrih.mirror.aliyuncs.com \n阿里云镜像获取地址：https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors，登陆后，左侧菜单选中镜像加速器就可以看到你的专属地址了：\n\nsudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'{\"registry-mirrors\": [\"https://66mzqrih.mirror.aliyuncs.com\"]}EOFsudo systemctl daemon-reloadsudo systemctl restart docker\n\n\n如何查看加速镜像安装成功输入：docker info\n\nDocker常用命令docker --help  帮助命令\ndocker –versiondocker -version\ndocker images查看本地images 镜像缓存\ndocker images 查看本地镜像文件\ndocker rmi -f kibana:5.6.9 -–删除镜像文件\nREPOSITORY 存储库名称\nTag  镜像的标签  不写版本号码 默认下载最新latest镜像\nIMAGE ID  镜像id\nCREATED 创建时间\nSIZE 大小\ndocker images -a\ndocker images -q -–只显示镜像的id\ndocker images --digests -–显示镜像的摘要信息\ndocker images --no-trunc -–显示完整镜像信息\ndocker rmi tomcat（镜像文件名称）\ndocker searchdocker search mysql\nDocker\n\n docker search -s 30 mysql  列出点赞数超过30以上。\nlatest 表示为最新的镜像文件 mysql8.0版本\ndocker pulllatest -—-tag 最新版本的镜像文件\ndocker pull nginx:latest --默认的情况下 下载最新版本的镜像  可以通过\nhttps://hub.docker.com/\\_/nginx\\?tab=tags\\&amp;page=1\\&amp;ordering=last\\_updated\n\n容器管理查看容器信息\nDocker  ps  获取到容器id\ndocker inspect  1e07cc5cc78d\n运行容器\ndocker run\ndocker run -i（保持容器一直运行）-t（给容器一个伪终端）-d(后台运行，不直接进入容器) --name&#x3D;tomcat9.2（给启动容器起名字）-p 8080:8080(宿主:docker容器)tomcat:9.2(启动的容器)  【参数】(加入容器初始化命令)\n#通过 -it 启动的容器有两个特点 一创建就进入容器 exit退出容器 容器就会停止运行  -–交互式容器\n#通过 -id 创建的容器 docker exec -it tomcat9.2（–name起的名称）进入容器 exit退出容器 容器不会停止运行   -–守护式容器\ndocker ps 查看正在运行的容器\ndocker ps -a 查看运行和已经运行关闭大的容器\ndocker stop tomcat8  关闭容器\ndocker start tomcat8 启动容器\ndocker rm tomcat8 删除容器\ndocker inspect tomcat8 查看容器信息\ndocker exec 参数  进入容器\ndocker run 运行原理\ndocker run mayikt\n\n简单描述：首先会先从本地获取获取mayikt镜像文件，如果本地没有该镜像文件则会去\n阿里云仓库查找该镜像文件，如果阿里云仓库也没有该镜像文件，则会报错找不到\n镜像文件。\n获取到镜像文件之后直接运行。\n详细描述：\n1.docker在本机缓存中  mayikt镜像文件，如果本地存在该镜像文件\n，则以该镜像文件作为模板在容器中运行。\n2.如果本地缓存中，没有mayikt镜像文件 则会从dockerhub 或者加速镜像中\n查找，如果查找不到的话，则返回错误找不到该镜像。\n\n如果能够查找到该镜像，则以该镜像作为模板运行。\n\n\n每个容器都有自己独立的网络 ip信息 运行成功 就是一个轻量级linux操作系统\n启动容器\ndocker start 容器id\n停止容器\ndocker stop  容器id\n删除容器\ndocker rm  容器id\n进入容器中\n# 首先使用下面的命令，查看容器ID（CONTAINER ID）：\ndocker ps -a\n# 然后用下面的命令进入容器，就可以使用bash命令浏览容器里的文件：\ndocker exec -it [CONTAINER ID] bash\n# 有的镜像没有bash命令，可以用对应的shell，比如sh\ndocker exec -it [CONTAINER ID] sh\nDocker 镜像原理镜像是什么\n基于docker安装tomcat服务器 是否需要配置jdk环境变量呢？\ndocker安装tomcat:8 --jdk8 配置环境变量\ndocker安装tomcat:9 --jdk9 配置环境变量\n如何封装配置环境依赖的呢？\nDockerfile—文件\nTomcat 100mb\n1.依赖于我们JDK 200mb\n2.Linux服务器centos 200mb\n镜像是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境的开发软件，它包含运行某个软件所需的所有内容，包括代码、运行时、库、环境变量和配置文件。\n镜像文件的组成通过 Union fs\n运行我们tomcat镜像文件\ntomcat镜像文件\n\n依赖于我们JDK\nLinux服务器\n\n为什么运行tomcat镜像文件，不需要配置jdk环境变量。\n\ntomcat镜像文件包含jdk依赖镜像 tomcat8—–jdk8镜像文件\n底层dockerfile —–描述配置jdk环境\n\n镜像加载的原理\nLinux文件系统由bootfs和rootfs两部分组成\n bootfs：包含bootloader（引导加载程序）和 kernel（内核）\n rootfs： root文件系统，包含的就是典型 Linux 系统中的&#x2F;dev，&#x2F;proc，&#x2F;bin，&#x2F;etc等标准目录和文件\n不同的linux发行版，bootfs基本一样，而rootfs不同，如ubuntu，centos等\nDocker镜像底层实际上是有多个不同的联合文件系统组成的\n最底层：bootfs，并使用宿主机的bootfs-复用\n第二层：root文件系统rootfs,称为base image\nUnion fs\n然后再往上可以叠加其他的镜像文件\n统一文件系统（Union File System）技术能够将不同的层整合成一个文件系统，为这些层提供了一个统一的视角，隐藏多层的存在，我们看来只是存在一个文件系统。\n所以当我们安装的tomcat镜像大小是600多MB 是因为里面还包含了jdk和centos的镜像\n而centos镜像复用了宿主机的bootfs 下载的只有rootfs 所以小很多\n\n所以tomcat&gt;jdk（父镜像）-&gt;centos&gt; 所以整个向外暴露就是600MB\n镜像只读 当从一个镜像启动容器时， 所以docker会在镜像上面加载一个可读可写的文件系统作为容器运行。\nDocker\n  \n\nDocker Commit主要作用：根据当前容器制作为镜像文件\n流程：\n\n从docker hub中下载一个tomcat8镜像文件；\n运行tomcat8镜像文件 在tomcatwebapps 目录中新增 mayikt文件夹 index.html\n将当前容器内容根据模板制作为镜像文件\n\ndocker commit提交容器副本使之成为一个新的镜像\n命令：docker commit -m&#x3D;“提交的描述信息” -a&#x3D;“作者” 容器ID 要创建的目标镜像名:[标签名]\n\n安装一个tomcat8\n\ndocker run -p 8081:8080   tomcat:8\n\ndocker exec -it 3a06b4c779a8   bash\ncd webapps\nmkdir mayikt\ntouch index.html\necho “mayikt” &gt;&gt;index.html\n\ndocker commit -m&#x3D;“提交的描述信息” -a&#x3D;“作者” 容器ID 要创建的目标镜像名:[标签名]\n1.根据当前容器作为模板制作为镜像文件\ndocker commit -m&#x3D;”mayikt tomcat” -a&#x3D;”mayikt”  3a06b4c779a8 mayikt-tomcat:1.0\n2.在以当前自己制作的镜像文件运行\ndocker run -p 8088:8080   mayikt-tomcat:1.0\n\nDocker数据卷基本的概念\n数据卷就是宿主机上的一个文件或目录\n当容器目录和数据卷（宿主机）目录绑定，双方修改会立即同步操作\n一个数据卷可以被多个容器同时挂载\n数据卷作用：容器数据的持久化 外部机器和容器间接通信 容器之间数据交换\n使用 -v命令。\n数据卷添加的方式\n容器内与宿主机实现数据的共享\n数据卷–添加两种方式\n\n直接命令形式添加  docker run -it -v 宿主机绝对路径目录:容器内目录 镜像文件名称\nDockerfile方式添加\n\n安装Nginx实现负载均衡\n挂载nginx html文件\nDocker\n\n创建挂载目录\n\nmkdir -p &#x2F;data&#x2F;nginx&#x2F;{conf,conf.d,html,logs}\n\n\n启动docker容器\n\ndocker run --name nginx81 -d -p 81:80 -v &#x2F;data&#x2F;nginx&#x2F;html:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html nginx\n-v &#x2F;data&#x2F;nginx&#x2F;html 虚拟机目录 --挂载 容器目录 &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html\n上传一个 html 放入到  &#x2F;data&#x2F;nginx&#x2F;html\ndocker run --name nginx81 -d -p 81:80 -v &#x2F;data&#x2F;nginx&#x2F;html:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html nginx\n-v &#x2F;data&#x2F;nginx&#x2F;html: linux虚拟机目录\n&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html 容器中html目录\n\nnginx .conf文件和日志文件\ndocker run --name nginx81 -d -p 81:80 -v &#x2F;data&#x2F;nginx&#x2F;html:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html \\\n-v &#x2F;data&#x2F;nginx&#x2F;conf&#x2F;nginx.conf:&#x2F;etc&#x2F;nginx&#x2F;nginx.conf \\\n-v &#x2F;data&#x2F;nginx&#x2F;logs:&#x2F;var&#x2F;log&#x2F;nginx  nginx\n\\反斜杠  表示换行\n&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html\n&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;conf\n&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;log\nDocker实战部署软件\n安装Tomcat服务器\ndocker run -p 8081:8080   tomcat:8\n-p 8081 :8080    容器外部（linux虚拟机访问端口8081）:8080（容器端口号）\ndocker ps  获取tomcat正在运行的容器id 进入到中\ndocker exec -it 1210e05f1a59 bash\ndocker run -p 8081:8080   tomcat:8\n-p 8081:8080  8081(linux虚拟机访问的端口号):8080(容器内部中端口号)\ndocker run -p 8081:8080 -d   tomcat:8 后台启动 -–每次运行都会创建一个新的容器\ndocker run --name mayikt-tomcat  -p 8081:8080 -d   tomcat:8\n--name: 指定容器名称\n-p:指定容器端口号\n-d:指定容器后台运行\ndocker run --name mayikt-tomcat    tomcat\ndocker run --name mayikt-tomcat1  -p 8081:8080  tomcat\n8081（容器外部或者linux虚拟机访问的端口号 宿主机）\n8080 容器内部的端口号\ndocker run --name mayikt-tomcat2022  -p 8081:8080 -d   tomcat:8\n-d 后台启动\n前台启动与后台启动的区别\n前台启动会打印启动日志信息\n后台启动不会打印启动日志信息\n安装Nginx实现静态服务\nDocker run 运行容器\n--name nginx-test：容器名称。\n-p 8080:80  端口进行映射，将本地 8080 端口映射到容器内部的 80 端口。\n-d nginx： 设置容器在在后台一直运行。\ndocker  ps -– 正在运行的容器\ndocker ps -a 显示所有的容器 包括为运行的容器\ndocker ps 容器id\ndocker run --name nginx-mayikt -p 8080:80 nginx   默认前台启动\ndocker run --name nginx-mayikt -p 8080:80 -d nginx   后台启动方式\n前台与后台启动区别：\n前台启动：会展示容器启动的日志信息—–\n后台启动：不会展示启动日志信息\n8080:80 8080 虚拟机本地端口 -–浏览器访问 80 容器内部端口\n Elk+kafka—\n systemctl stop firewalld \n\n安装MySQL5.7\n1.查询mysql版本\ndocker search mysql\n\n2.下载MySQL5.7版本\ndocker pull mysql:5.7 (这里选择的是第一个mysql镜像， :5.7选择的5.7版本)\n\n3.等待下载完成、创建MySQL容器\ndocker create --name mysql3308 -e MYSQL_ROOT_PASSWORD=root -p 3308:3306 mysql:5.7\n\n 创建容器名称为mysql3308，密码为root\n\n启动容器\n\ndocker start mysql3308\n\n进入到容器\n\ndocker exec -it mysql3308 bash\n\nmysql连接\n\nmysql -uroot –p\nDocker运行底层原理\n\n首先启动docker systemctl start docker\nDocker是一个CS架构的系统，docker守护进程运行在主机上，让后通过socket连接\n\n从客户端访问，守护进程从客户端接收命令管理运行在主机上的容器。\nps aux | grep ‘docker’\n\n网站“bs”CS\n数据卷volumes-from\n容器间传递共享数据volumes-from\n启动容器报错了如何解决？先改为前台启动如果没有问题的情况下，在修改为后台启动。\n容器与容器局域网DockerFile 解析一个镜像文件到底是如何创建？\n\ndockerfile 描述出镜像文件需要的一些依赖配置和环境变量 执行命令\n\n将我们dockerfile 文件打包成一个镜像文件\n\n直接使用我们的容器运行到该镜像文件。\n\n需要手动编写一个dockerfile文件\n\n将该dockerfile  docker build 自定义成一个镜像文件\n\ndocker run 运行容器\n\n\nCentos镜像文件docker run -it centos \n\nsig-cloud-instance-images&#x2F;Dockerfile at b2d195220e1c5b181427c3172829c23ab9cd27eb · CentOS&#x2F;sig-cloud-instance-images · GitHub\nDockerFile编写规范A.#描述注释\nB.指令必须要大写，后面至少需要带至少一个参数;\nC.指令是按照从上到下，顺序执行；\nDockerFile指令\nFROM 指定父镜像:  基于哪个镜像image构建  指定基础镜像，必须为第一个命令\nMAINTAINER :维护者\nRUN: 容器创建的时候执行一段命令   构建镜像时执行的命令\nADD: 将本地文件添加到容器中，tar类型文件会自动解压(网络压缩资源不会被解压)，可以访问网络资源，类似wget\nCOPY:功能类似ADD，但是是不会自动解压文件，也不能访问网络资源\nCMD:构建容器后调用，也就是在容器启动时才进行调用。 .sh执行文件\nENV: 设置环境变量\nEXPOSE: 指定于外界交互的端口\nVOLUME  用于指定持久化目录\nWORKDIR 设置进入容器时的路径 默认访问的目录\n\nTomcat—–jdk环境\nTomcat docker File：\ntomcat&#x2F;Dockerfile at 385e8403a38fab7097d4c3fed2484caba7dfd099 · docker-library&#x2F;tomcat · GitHub\nhttps://github.com/docker-library/redis/blob/231905d0841f52ee4f3a5b8b42d62cd6d14a1a93/6.2/Dock\n进入tomcat 容器—-&#x2F;data\nredis容器&#x2F;data\n&#x2F;\nDockerFile案例Base镜像(scratch) docker hub中的镜像都是通过base镜像中安装和配置需要的软件构建的。\n构建自己centos镜像\ndocker run -it centos\n\n需求定制修改centos根目录；\n实现支持vim插件；\n\nyum -y install vim \n\n需要自己制作一个dockerfile文件\n继承docker hub中的centos\n在docker hubcentos 上加入以下两个功能\n\nA.进入容器中 默认访问目录&#x2F;usr\nB.实现支持vim插件\n需要将该dockerfile文件打包成一个镜像文件 交给我们容器执行\nhttps://github.com/CentOS/sig-cloud-instance-images/blob/b2d195220e1c5b181427c3172829c23ab9cd27eb/docker/Dockerfile\n定制CentOS镜像\nFROM centosMAINTAINER mayikt-yushengjunENV MYPATH /usrWORKDIR $MYPATHRUN yum -y install vimEXPOSE 80CMD /bin/bash\n\nDockerfile→使用docker 将该Dockerfile实现打包成镜像文件→\n容器运行该镜像文件。\ndocker build—将该Dockerfile实现打包成镜像文件\n将该dockerfile文件上传到linux服务器中\n使用docker build -f Dockerfile -t mycs:1 .\nDockerfile -—–配置文件\nmycs—-打包镜像文件名称\n1 tag 版本号码\n\ndocker run -it mycs:1 \n将springboot项目打包部署\n\n基于docker原生方式 部署我们的springboot项目\n\nDockerfile\n2.dockercompose—– 容器编排技术\nspringboot项目—-变成镜像文件—容器运行\n\n将我们springboot项目—打成一个jar包\n\n2.定义dockerfile文件—–描述出springboot项目 配置依赖和环境变量\nJDK\n注意：springboot内置嵌入我们的tomcat服务器 所以不需要额外的tomcat容器来\n运行。\n原生方式运行我们的jar包\nJava- jar指令\n\n需要先将我们外部jar，拷贝到容器中\n容器运行成功执行java -jar\n\n2.将该dockerfile文件打包成镜像文件-\n\n将springboot项目打包；\n制作dockerfile文件；\n继承我们的jdk环境\n将我们本地的jar包拷贝到容器中\nJava -jar\n将dockerfile文件打包成镜像文件；\n运行该镜像文件即可；\n\n将springboot项目打包\nmvn clean package \n制作dockerfile文件\n# 基础镜像使用javaFROM java:8# 作者MAINTAINER www.mayikt.com# VOLUME 指定了临时文件目录为/tmp。# 其效果是在主机/var/lib/docker 目录下创建了一个临时文件，并链接到容器的/tmpVOLUME /tmp# 将jar包添加到容器中并更名为mayikt.jarADD mayikt-thymeleaf-1.0-SNAPSHOT.jar mayikt.jar# 运行jar包RUN bash -c 'touch /mayikt.jar'ENTRYPOINT [\"java\",\"-Djava.security.egd=file:/dev/./urandom\",\"-jar\",\"/mayikt.jar\"]#暴露8080端口EXPOSE 8080\n\n打包成镜像文件\ndocker build -f Dockerfile -t mayikt-member:1 .\ndocker build -f Dockerfile -t mayikt-member:1 .\n启动容器\ndocker run -p 8070:8080   mayikt-member:1\n访问测试\n\nDocker ComposeSit pre prd环境\n为什么需要使用Docker ComposeDocker Compose  容器编排技术\n容器编排技术\n1.现在我们有一个springboot项目，需要依赖Redis、mysql5.7、nginx。\n如果使用docker原生部署的话，则需要安装Redis、mysql5、nginx容器，在才可以启动我们springboot项目，这样的话部署项目的流程非常复杂，所以需要引入我们的\nDocker compose实现容器编排技术。\n\n基本的概念Docker-Compose项目是Docker官方的开源项目，负责实现对Docker容器集群的快速编排。\nDocker-Compose将所管理的容器分为三层，分别是工程（project），服务（service）以及容器（container）。\n开发一个springboot项目—大工程\n\n依赖mysql\n依赖redis\n依赖zk\n\n等。\n需要在docker-compose.yml 配置项目工程依赖环境配置\nDocker-Compose运行目录下的所有文件（docker-compose.yml，extends文件或环境变量文件等）组成一个工程，若无特殊指定工程名即为当前目录名。一个工程当中可包含多个服务，每个服务中定义了容器运行的镜像，参数，依赖。一个服务当中可包括多个容器实例，Docker-Compose并没有解决负载均衡的问题，因此需要借助其它工具实现服务发现及负载均衡。\nDocker-Compose的工程配置文件默认为docker-compose.yml，可通过环境变量COMPOSE_FILE或-f参数自定义配置文件，其定义了多个有依赖关系的服务及每个服务运行的容器。\nCompose 中有两个重要的概念：\n服务 (service) ：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。\n项目 (project) ：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。\n一个项目可以由多个服务（容器）关联而成，Compose 面向项目进行管理，通过子命令对项目中的一组容器进行便捷地生命周期管理。\nCompose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。\nDocker-Compose分成三层\n1.项目层 springboot项目依赖于我们的mysql redis、nginx等 一个项目是由多个容器组成的。\n2.服务层  运行一个镜像的实例 -–\nCompose环境安装（离线安装）\n访问docker compose  github 官网\n\n\n\ndocker-compose-Linux-x86_64 上传到服务器中，然后执行如下命令将其移动到&#x2F;usr&#x2F;local&#x2F;bin&#x2F;目录中 并且更名为docker-compose\n\nmv docker-compose-Linux-x86_64 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose\n\n执行如下命令：添加可执行的权限\n\nsudo chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose\n\n验证docker-compose\n\ndocker-compose -v\n\nCompose常用命令docker-compose -h                           # 查看帮助\ndocker-compose up                           # 创建并运行所有容器\ndocker-compose up -d                        # 创建并后台运行所有容器\ndocker-compose -f docker-compose.yml up -d  # 指定模板\ndocker-compose down                         # 停止并删除容器、网络、卷、镜像。\ndocker-compose logs       # 查看容器输出日志\ndocker-compose pull       # 拉取依赖镜像\ndokcer-compose config     # 检查配置\ndokcer-compose config -q  # 检查配置，有问题才有输出\ndocker-compose restart   # 重启服务\ndocker-compose start     # 启动服务\ndocker-compose stop      # 停止服务\nCompose入门案例流程：\n\n需要定义一个docker-compose.yml文件—-工程\n需要在docker-compose文件配置依赖服务\ndocker-compose up 执行该文件\n创建一个docker-compose.yml；\n定制docker-compose 内容；\n运行 docker-compose up ；\n\nversion: '3.0'services:tomcat: ##服务名称image: tomcat:8 #镜像文件名称ports:- 8080:8080\n\nCompose模板文件version: '3.0'services:tomcat80: ##服务名称#container_name: tomcat8080 指定容器名称image: tomcat:8 #镜像文件名称run imagesports: ###端口号的映射-p- 8080:8080volumes: ## 数据源 宿主机与容器数据共享-v- /usr/tomcat/webapps:/usr/local/tomcat/webappsnetworks: ###定义网络的桥- mayikttomcat81: ##服务名称#container_name: tomcat8080 指定容器名称image: tomcat:8 #镜像文件名称ports: ###端口号的映射- 8081:8080volumes: ## 数据源 宿主机与容器数据共享- /usr/tomcat/webapps:/usr/local/tomcat/webappsnetworks:- mayiktnetworks: ## 定义服务的桥mayikt:\n\nCompose常用命令\ndocker-compose ps****  列出项目中所有的容器****\ndocker-compose stop**** 停止****docker-compose\ndocker-compose logs****  查看容器中日志信息****\ndocker-compose pull****  拉取服务依赖的镜像****\nCompose常用配置Image 镜像名称；\nBuild 根据docker file 打包 成镜像；\nContext  指定docker file文件位置；\nCommond 使用command可以覆盖容器启动后默认执行的命令；\nContainer_name 容器名称；\ndepends_on 指定依赖那个服务；\nPorts 映射的端口号；\nextra_hosts 会在&#x2F;etc&#x2F;hosts文件中添加一些记录；\nVolumes 持久化目录；\nvolumes_from 从另外一个容器挂在数据卷；\nDns 设置dns\nCompose部署springboot项目定义Compose文件\nversion: \"3.0\"services:mysql: # mysql服务image: mysql:5.7command: --default-authentication-plugin=mysql_native_password #解决外部无法访问ports:- \"3306:3306\" #容器端口映射到宿主机的端口environment:MYSQL_ROOT_PASSWORD: 'root'MYSQL_ALLOW_EMPTY_PASSWORD: 'no'MYSQL_DATABASE: 'mayikt'MYSQL_USER: 'mayikt'MYSQL_PASSWORD: 'mayikt'networks:- mayikt_webmayikt-web: #自己单独的springboot项目hostname: mayiktbuild: ./ #需要构建的Dockerfile文件ports:- \"38000:8080\" #容器端口映射到宿主机的端口depends_on: #web服务依赖mysql服务，要等mysql服务先启动- mysqlnetworks:- mayikt_webnetworks: ## 定义服务的桥mayikt_web:\n\nSpring项目配置\nspring:profiles:active: prddatasource:url: jdbc:mysql://mysql:3306/mayikt?useUnicode=true&amp;characterEncoding=UTF-8&amp;serverTimezone=UTCusername: rootpassword: rootdriver-class-name: com.mysql.jdbc.Driverserver:###端口号port: 8080servlet:##设置springboot项目访问路径context-path: /mayikt\n\n演示效果\nhttp://192.168.163.129:38000/mayikt/insertUser?userName=mayikt&amp;userAge=22\nDocker可视化工具使用****Portainer\nPortainer是一款Docker可视化管理工具，允许我们在网页中方便的查看和管理Docker容器。\n要使用Portainer很简单，运行下面两条命令即可。这些命令会创建一个Portainer专用的卷，然后在8000和9000端口创建容器并运行。\n\n启动：\ndocker run -d -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer\n\n\nCentOS Docker 安装docker rm $(docker ps -aq)\ndocker stop $(docker ps -q) &amp; docker rm $(docker ps -aq) -–删除所有的容器\ndocker rmi $(docker images -q)\ndocker network ls\nLinux关闭防火墙命令systemctl stop firewalld\n注：此文章内容源于蚂蚁课堂余胜军老师的课程笔记\nArticle link： https://tqgoblin.site/post/csdn/Docker/  Author： Stephen  \n","slug":"csdn/Docker","date":"2021-03-26T08:52:57.000Z","categories_index":"运维","tags_index":"docker 容器","author_index":"Stephen"},{"id":"8da5616698516c85cde26f7cf0077841","title":"分布式日志采集（elk+kafka）","content":"目的        分布式系统的日志，每个服务器节点web服务都会产生各自的日志文件，如果想要整合或者排查日志，就需要到每个节点下逐一查看日志文件这样会比较麻烦。所以需要一个方案将日志采集放到一个位置进行存储和查询。 这里就可以使用elk+kafka的方式解决。\n       elk(elasticsearch、logstash、kibana)的首字母缩写。elasticsearch用来存储采集的日志数据，logstash负责采集日志，kibana作为es的可视化分析查询工具。\n原理eIk 采集日志的原理:1.需要在每个服务器上安装 logstash\n2.logstash需要配置固定读取某个日志文件\n3.logstash将我们的日志文件格式化为json 的格式输出到es 中\n4.开发者使用kibana连接到es中查询存储日志内容\n\nelk+kafka 原理:1.springboot 项目会基于 aop 的方式拦截系统中日志\n日志(错误日志)错误日志:异常通知\n请求与响应日志信息—前置或者环绕通知。2.将该日志投递到我们 kafka 中注意该过程一定要是异步的形式\n3.Logstash 数据源—kafka 订阅 kafka 的主题获取日志消息内容\n4.在将日志消息内容输出到es 中存放\n5.开发者使用Kibana 连接到 ElasticSeach 查询存储日志内容\n\nelk+kafka的环境搭建elaticsearch和kibana的安装使用的docker，也可以自己下载安装包进行安装\nelaticsearch安装elasticsearch1.下载 ES 镜像问题docker pull elasticsearch2.运行ESdocker run -it --name elasticsearch -d -p 9200:9200 -p 9300:9300 -p  5601:5601 elasticsearch\n3. 测试运行结果http://192.168.163.129:9200/\n\n如果出现以下报错就是创建的es容器有冲突，需要删除已有的es容器或者镜像重命名\n\nlogstashLogstash 是一个开源数据收集引擎，具有实时管道功能。Logstash 可以动态地将来自不同数据源的数据统一起来，并将数据标准化到你所选择的目的地。\nLogstash 输入数据源:、本地文件、Kafka、Redis、mysql\nLogstash 输出数据源:Es、Mongdb、Redis、Mysql\n安装步骤：\n1.上传 logstash-6.4.3.tar.gz 到服务中\n2.tar -zxvf logstash-6.4.3.tar.gz\n3.cd logstash-6.4.3\n4.bin&#x2F;logstash-plugin install logstash-input-kafka\n5.bin&#x2F;logstash-plugin install logstash-output-elasticsearch\n配置文件mylog.conf：\n\n 启动：\n\nkibanadocker run -it -d -e ELASTICSEARCH URL&#x3D;http://127.0.0.1:9200 --name kibana --network&#x3D;container:elasticsearch kibana测试运行结果http://192.168.163.129:5601/app/kibana#\nkafka这里我们用window版本的，如果是在Linux环境。可以自行了解安装\nkafka依赖zookeeper，所以需要先装zk:\n\n\n 演示代码\n演示使用springboot框架，aop前置通知采集请求信息入kafka\n\nkafka的spring-boot配置信息：\n\n\n  浏览器访问：127.0.0.1:8881&#x2F;项目名称&#x2F;getMeberl,aop前置通知会把请求信息封装投递给kafka，logstash从kafka里自动拿到数据存入到es中，使用kibana查看存储的数据，如图：\n\n 复制出来放到格式化工具里\n\n为什么要使用es存储日志   es是一种搜索服务器，底层是基于_Lucene，支持倒排索引，做搜索的效率是特别高的。基于业务需要还是可以考虑选择_Mongdb、Redis、Mysql。后面会更新一篇flume_采集入hdfs做大数据存储、分析、统计_\n为什么elk要结合kafka1.如果单纯的使用 elk 的话，服务器节点扩容 需要每个服务器上安装我们Logstash步骤比较冗余\n2.Logstash 读取本地日志文件，可能会对本地的磁盘 io 性能会有一定影响。\n优化        图中可以看出请求触发aop，前置通知中往kafka里send消息，如果kafka出现异常、网络抖动都会影响主流程的一个请求响应效率和是否出异常的问题。所以需要把数据放到内存队列中，然后开一个线程，在另一个线程里循环从内存队列里取数据入kafka这样不会影响请求的正常执行业务和响应。\nArticle link： https://tqgoblin.site/post/csdn/分布式日志采集（elk+kafka）/  Author： Stephen  \n","slug":"csdn/分布式日志采集（elk+kafka）","date":"2021-03-24T15:15:22.000Z","categories_index":"mq","tags_index":"分布式 elk","author_index":"Stephen"},{"id":"4e6d25d934b34d1d1306ad13dba25eb2","title":"MYSQL-性能优化篇","content":"为什么要进行数据库优化？1.避免网站页面出现访问错误\n        由于数据库连接timeout产生页面5xx错误\n        由于慢查询造成页面无法加载\n        由于阻塞造成数据无法提交\n2.增加数据库的稳定性\n        很多数据库问题都是由于低效的查询引起的\n3.优化用户体验\n        流畅页面的访问速度\n        良好的网站功能体验\nmysql数据库优化可以从哪几个方面进行数据库的优化？如下图所示：\n\n1.SQL及索引优化\n        根据需求写出良好的SQL，并创建有效的索引，实现某一种需求可以多种写法，这时候我们就要选择一种效率最高的写法。这个时候就要了解sql优化\n2.数据库表结构优化\n        根据数据库的范式，设计表结构，表结构设计的好直接关系到写SQL语句。\n3.系统配置优化\n        大多数运行在Linux机器上，如tcp连接数的限制、打开文件数的限制、安全性的限制，因此我们要对这些配置进行相应的优化。\n4.硬件配置优化\n        选择适合数据库服务的cpu，更快的IO，更高的内存；cpu并不是越多越好，某些数据库版本有最大的限制，ＩＯ操作并不是减少阻塞。\n注：通过上图可以看出，该金字塔中，优化的成本从下而上逐渐增高，而优化的效果会逐渐降低。\nSQL及索引优化mysql安装与卸载（linux在线安装与卸载）数据库版本选择1、查看数据库的版本\nsqlselect @@version;\n准备数据网址：****https://dev.mysql.com/doc/sakila/en/sakila-installation.html\n\n\n\nsakila-db.zip压缩包所包含的文件如下解释\n\n加载数据\n步骤如下图所示\n\n \n表结构关系\n注：该表结构关系是用工具生成的。\n如何发现有问题的SQLMySQL慢查日志的开启方式和存储格式\n检查慢查日志是否开启：sqlshow variables like &#39;slow_query_log&#39;\nsqlshow variables like &#39;slow_query_log&#39;  \n\n//查看是否开启慢查询日志\n\nset global slow_query_log_file=&#39; /usr/share/mysql/sql_log/mysql-slow.log&#39;\n\n//慢查询日志的位置\n\nset global log_queries_not_using_indexes=on;\n\n//开启慢查询日志\n\nset global long_query_time=1;  \n\n//大于1秒钟的数据记录到慢日志中，如果设置为默认0，则会有大量的信息存储在磁盘中，磁盘很容易满掉查看所有日志的变量信息sqlshow variables like &#39;%log%&#39;sqlmysql&gt; show variables like &#39;%log%&#39;;\n\n+-----------------------------------------+------------------------------------+\n\n| Variable_name                           | Value                              |\n\n+-----------------------------------------+------------------------------------+\n\n| back_log                                | 80                                 |\n\n| binlog_cache_size                       | 32768                              |\n\n| binlog_checksum                         | CRC32                              |\n\n| binlog_direct_non_transactional_updates | OFF                                |\n\n| binlog_error_action                     | IGNORE_ERROR                       |\n\n| binlog_format                           | STATEMENT                          |\n\n| binlog_gtid_simple_recovery             | OFF                                |\n\n| binlog_max_flush_queue_time             | 0                                  |\n\n| binlog_order_commits                    | ON                                 |\n\n| binlog_row_image                        | FULL                               |\n\n| binlog_rows_query_log_events            | OFF                                |\n\n| binlog_stmt_cache_size                  | 32768                              |\n\n| binlogging_impossible_mode              | IGNORE_ERROR                       |\n\n| expire_logs_days                        | 0                                  |\n\n| general_log                             | OFF                                |\n\n| general_log_file                        | /var/lib/mysql/mysql-host.log      |\n\n| innodb_api_enable_binlog                | OFF                                |\n\n| innodb_flush_log_at_timeout             | 1                                  |\n\n| innodb_flush_log_at_trx_commit          | 1                                  |\n\n| innodb_locks_unsafe_for_binlog          | OFF                                |\n\n| innodb_log_buffer_size                  | 8388608                            |\n\n| innodb_log_compressed_pages             | ON                                 |\n\n| innodb_log_file_size                    | 50331648                           |\n\n| innodb_log_files_in_group               | 2                                  |\n\n| innodb_log_group_home_dir               | ./                                 |\n\n| innodb_mirrored_log_groups              | 1                                  |\n\n| innodb_online_alter_log_max_size        | 134217728                          |\n\n| innodb_undo_logs                        | 128                                |\n\n| log_bin                                 | OFF                                |\n\n| log_bin_basename                        |                                    |\n\n| log_bin_index                           |                                    |\n\n| log_bin_trust_function_creators         | OFF                                |\n\n| log_bin_use_v1_row_events               | OFF                                |\n\n| log_error                               | /var/log/mysqld.log                |\n\n| log_output                              | FILE                               |\n\n| log_queries_not_using_indexes           | ON                                 |\n\n| log_slave_updates                       | OFF                                |\n\n| log_slow_admin_statements               | OFF                                |\n\n| log_slow_slave_statements               | OFF                                |\n\n| log_throttle_queries_not_using_indexes  | 0                                  |\n\n| log_warnings                            | 1                                  |\n\n| max_binlog_cache_size                   | 18446744073709547520               |\n\n| max_binlog_size                         | 1073741824                         |\n\n| max_binlog_stmt_cache_size              | 18446744073709547520               |\n\n| max_relay_log_size                      | 0                                  |\n\n| relay_log                               |                                    |\n\n| relay_log_basename                      |                                    |\n\n| relay_log_index                         |                                    |\n\n| relay_log_info_file                     | relay-log.info                     |\n\n| relay_log_info_repository               | FILE                               |\n\n| relay_log_purge                         | ON                                 |\n\n| relay_log_recovery                      | OFF                                |\n\n| relay_log_space_limit                   | 0                                  |\n\n| simplified_binlog_gtid_recovery         | OFF                                |\n\n| slow_query_log                          | OFF                                |\n\n| slow_query_log_file                     | /var/lib/mysql/mysql-host-slow.log |\n\n| sql_log_bin                             | ON                                 |\n\n| sql_log_off                             | OFF                                |\n\n| sync_binlog                             | 0                                  |\n\n| sync_relay_log                          | 10000                              |\n\n| sync_relay_log_info                     | 10000                              |\n\n+-----------------------------------------+------------------------------------+\n\n61 rows in set (0.01 sec)开启慢查日志：\nsqlshow variables like &#39;slow_query_log&#39;  \n//查看是否开启慢查询日志\nset global slow_query_log_file=&#39; /var/lib/mysql/mysql-host-slow.log &#39;\n//慢查询日志的位置\nset global log_queries_not_using_indexes=on;\n//开启慢查询日志\nset global long_query_time=1;  \n//大于1秒钟的数据记录到慢日志中，如果设置为默认0，则会有大量的信息存储在磁盘中，磁盘很容易满掉验证慢查询日志是否开启：\n在mysql操作中，\nsqlshow databases;\nuse sakila;\nselect * from store;\nselect * from staff;监听日志文件，看是否写入\ntail -50f &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql-host-slow.log\n\nMySQL慢查日志的存储格式如下图所示：\n\n说明：\n1、# Time: 180526  1:06:54 -——&gt;查询的执行时间\n2、# User@Host: root[root] @ localhost []  Id:     4 -——&gt;执行sql的主机信息\n3、# Query_time: 0.000401  Lock_time: 0.000105 Rows_sent: 2  Rows_examined: 2——-&gt;SQL的执行信息：\nQuery_time：SQL的查询时间\nLock_time：锁定时间\nRows_sent：所发送的行数\nRows_examined：锁扫描的行数\n4、SET timestamp&#x3D;1527268014; -——&gt;SQL执行时间\n5、select * from staff; -——&gt;SQL的执行内容\nMySQL慢查日志分析工具（mysqldumpslow）****介绍        如何进行查看慢查询日志，如果开启了慢查询日志，就会生成很多的数据，然后我们就可以通过对日志的分析，生成分析报表，然后通过报表进行优化。\n用法接下来我们查看一下这个工具的用法：\n注意：在mysql数据库所在的服务器上，而不是在mysql&gt;命令行中\n该工具如何使用：\n\n\n\n\n\n\n\n\n\nmysqldumpslow****-h\n\n查看verbose信息\n\n\n\n\n\n\n\n\n\nMysqldumpslow -v\n\n查看慢查询日志的前10个，mysqldumpslow 分析的结果如下\n\n\n\n\n\n\n\n\n\nmysqldumpslow -t 10 &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql-slow.log\n\n如上图两条就是分析的结果，每条结果都显示是执行时间，锁定时间，发送的行数，扫描的行数\n这个工具是最常用的工具，通过安装mysql进行附带安装，但是该工具统计的结果比较少，对我们的优化锁表现的数据还是比较少。\nMySQL慢查日志分析工具(pt-query-digest)介绍及作用        作为一名优秀的mysql dba也需要有掌握几个好用的mysql管理工具，所以我也一直在整理和查找一些能够便于管理mysql的利器。以后的一段时间内，将会花一大部分的精力去搜索这些工具。\n        性能的管理一直都是摆在第一位的，dba的很多工作管理层都看不到也没有办法衡量价值，但是如果一个系统慢的跟蜗牛一样，dba通过监控调优把系统从崩溃边 缘重新拉回到高铁时代。这种价值和触动应该是巨大的。（很多企业的领导认为系统跑不动了就需要换更快的CPU、更大的内存、更快的存储，而且这还不是少 数，所以DBA的价值也一直体现不出来，薪水自然也就不会很高）\n        mysql 的日志是跟踪mysql性能瓶颈的最快和最直接的方式了，系统性能出现瓶颈的时候，首先要打开慢查询日志，进行跟踪；这段时间关于慢查询日志的管理和查看 已经整理过两篇文章了，不经意间又发现了一个查看慢查询日志的工具：mk-query-digest，这个工具网上号称mysql dba必须掌握的十大工具之首。\n安装pt-query-digest工具快速安装（注：必须先要安装wget）\n\n\n\n\n\n\n\n\nwget https://www.percona.com/downloads/percona-toolkit/2.2.16/RPM/percona-toolkit-2.2.16-1.noarch.rpm &amp;&amp; yum localinstall -y  percona-toolkit-2.2.16-1.noarch.rpm\n检查是否安装完成： 命令行中输入：pt-summary\n显示如下图所示：说明安装成功！输入【[root@node03 mysql]# pt-query-digest --help】 \n\n\n工具使用简介：\n\n\n\n\n\n\n\n\npt-summary –help\nwget http://percona.com/get/pt-summary\n查看服务器信息\n\n\n\n\n\n\n\n\n\n命令：pt-summary\n查看磁盘开销使用信息\n\n\n\n\n\n\n\n\n\n命令：pt-diskstats\n查看mysql数据库信息\n\n\n\n\n\n\n\n\n\n命令：pt-mysql-summary --user&#x3D;root --password&#x3D;123456\n\n分析慢查询日志\n\n\n\n\n\n\n\n\n\n命令：pt-query-digest &#x2F;data&#x2F;mysql&#x2F;data&#x2F;db-3-12-slow.lo\n查找mysql的从库和同步状态\n\n\n\n\n\n\n\n\n\n命令：pt-slave-find --host&#x3D;localhost --user&#x3D;root --password&#x3D;123456\n查看mysql的死锁信息\n\n\n\n\n\n\n\n\n\npt-deadlock-logger --user&#x3D;root --password&#x3D;123456 localhost\n从慢查询日志中分析索引使用情况\n\n\n\n\n\n\n\n\n\npt-index-usage slow_20131009.log\n查找数据库表中重复的索引\n\n\n\n\n\n\n\n\n\npt-duplicate-key-checker --host&#x3D;localhost --user&#x3D;root --password&#x3D;123456\n查看mysql表和文件的当前活动IO开销\n\n\n\n\n\n\n\n\n\npt-ioprofile\n查看不同mysql配置文件的差异\n\n\n\n\n\n\n\n\n\npt-config-diff &#x2F;etc&#x2F;my.cnf &#x2F;etc&#x2F;my_master.cnf\npt-find查找mysql表和执行命令，示例如下\n查找数据库里大于2G的表：\n\n\n\n\n\n\n\n\n\npt-find --user&#x3D;root --password&#x3D;123456 --tablesize +2G\n查找10天前创建，MyISAM引擎的表：\n\n\n\n\n\n\n\n\n\npt-find --user&#x3D;root --password&#x3D;123456 --ctime +10 --engine MyISAM\n查看表和索引大小并排序\n\n\n\n\n\n\n\n\n\npt-find --user&#x3D;root --password&#x3D;123456 --printf “%T\\t%D.%N\\n” | sort -rn\npt-kill 杀掉符合标准的mysql进程\n显示查询时间大于60秒的查询\n\n\n\n\n\n\n\n\n\npt-kill --user&#x3D;root --password&#x3D;123456 --busy-time 60 --print\nkill掉大于60秒的查询\n\n\n\n\n\n\n\n\n\n pt-kill --user&#x3D;root --password&#x3D;123456 --busy-time 60 --kill\n查看mysql授权\n\n\n\n\n\n\n\n\n\n1、pt-show-grants --user&#x3D;root --password&#x3D;123456\n2、pt-show-grants --user&#x3D;root --password&#x3D;123456 --separate –revoke\n验证数据库复制的完整性\n\n\n\n\n\n\n\n\n\npt-table-checksum --user&#x3D;root --password&#x3D;123456\n附录：\n\n如何通过慢查日志发现有问题的SQL****查询次数多且每次查询占用时间长的sql        通常为pt-query-digest分析的前几个查询；该工具可以很清楚的看出每个SQL执行的次数及百分比等信息，执行的次数多，占比比较大的SQL\nIO大的sql        注意pt-query-digest分析中的Rows examine项。扫描的行数越多，IO越大。\n未命中的索引的SQL        注意pt-query-digest分析中的Rows examine 和Rows Send的对比。说明该SQL的索引命中率不高，对于这种SQL，我们要重点进行关注。\n通过explain查询分析SQL的执行计划****使用explain查询SQL的执行计划SQL的执行计划侧面反映出了SQL的执行效率，具体执行方式如下所示：\n在执行的SQL前面加上explain关键词即可；\n\n每个字段的说明：1）id列数字越大越先执行，如果说数字一样大，那么就从上往下依次执行，id列为null的就表是这是一个结果集，不需要使用它来进行查询。\n2）select_type列常见的有：\nA：simple：表示不需要union操作或者不包含子查询的简单select查询。有连接查询时，外层的查询为simple，且只有一个\nB：primary：一个需要union操作或者含有子查询的select，位于最外层的单位查询的select_type即为primary。且只有一个\nC：union：union连接的两个select查询，第一个查询是dervied派生表，除了第一个表外，第二个以后的表select_type都是union\nD：dependent union：与union一样，出现在union 或union all语句中，但是这个查询要受到外部查询的影响\nE：union result：包含union的结果集，在union和union all语句中,因为它不需要参与查询，所以id字段为null\nF：subquery：除了from子句中包含的子查询外，其他地方出现的子查询都可能是subquery\nG：dependent subquery：与dependent union类似，表示这个subquery的查询要受到外部表查询的影响\nH：derived：from字句中出现的子查询，也叫做派生表，其他数据库中可能叫做内联视图或嵌套select\n3）table\n显示的查询表名，如果查询使用了别名，那么这里显示的是别名，如果不涉及对数据表的操作，那么这显示为null，如果显示为尖括号括起来的&lt;derived N&gt;就表示这个是临时表，后边的N就是执行计划中的id，表示结果来自于这个查询产生。如果是尖括号括起来的&lt;union M,N&gt;，与&lt;derived N&gt;类似，也是一个临时表，表示这个结果来自于union查询的id为M,N的结果集。\n4）type\n依次从好到差：system，const，eq_ref，ref，fulltext，ref_or_null，unique_subquery，index_subquery，range，index_merge，index，ALL，除了all之外，其他的type都可以使用到索引，除了index_merge之外，其他的type只可以用到一个索引\nA：system：表中只有一行数据或者是空表，且只能用于myisam和memory表。如果是Innodb引擎表，type列在这个情况通常都是all或者index\nB：const：使用唯一索引或者主键，返回记录一定是1行记录的等值where条件时，通常type是const。其他数据库也叫做唯一索引扫描\nC：eq_ref：出现在要连接过个表的查询计划中，驱动表只返回一行数据，且这行数据是第二个表的主键或者唯一索引，且必须为not null，唯一索引和主键是多列时，只有所有的列都用作比较时才会出现eq_ref\nD：ref：不像eq_ref那样要求连接顺序，也没有主键和唯一索引的要求，只要使用相等条件检索时就可能出现，常见与辅助索引的等值查找。或者多列主键、唯一索引中，使用第一个列之外的列作为等值查找也会出现，总之，返回数据不唯一的等值查找就可能出现。\nE：fulltext：全文索引检索，要注意，全文索引的优先级很高，若全文索引和普通索引同时存在时，mysql不管代价，优先选择使用全文索引\nF：ref_or_null：与ref方法类似，只是增加了null值的比较。实际用的不多。\nG：unique_subquery：用于where中的in形式子查询，子查询返回不重复值唯一值\nH：index_subquery：用于in形式子查询使用到了辅助索引或者in常数列表，子查询可能返回重复值，可以使用索引将子查询去重。\nI：range：索引范围扫描，常见于使用&gt;,&lt;,is null,between ,in ,like等运算符的查询中。\nJ：index_merge：表示查询使用了两个以上的索引，最后取交集或者并集，常见and ，or的条件使用了不同的索引，官方排序这个在ref_or_null之后，但是实际上由于要读取所个索引，性能可能大部分时间都不如range\nK：index：索引全表扫描，把索引从头到尾扫一遍，常见于使用索引列就可以处理不需要读取数据文件的查询、可以使用索引排序或者分组的查询。\nL：all：这个就是全表扫描数据文件，然后再在server层进行过滤返回符合要求的记录。\n5）possible_keys\n查询可能使用到的索引都会在这里列出来\n6）key\n查询真正使用到的索引，select_type为index_merge时，这里可能出现两个以上的索引，其他的select_type这里只会出现一个。\n7）key_len\n用于处理查询的索引长度，如果是单列索引，那就整个索引长度算进去，如果是多列索引，那么查询不一定都能使用到所有的列，具体使用到了多少个列的索引，这里就会计算进去，没有使用到的列，这里不会计算进去。留意下这个列的值，算一下你的多列索引总长度就知道有没有使用到所有的列了。要注意，mysql的ICP特性使用到的索引不会计入其中。另外，key_len只计算where条件用到的索引长度，而排序和分组就算用到了索引，也不会计算到key_len中。\n8）ref\n如果是使用的常数等值查询，这里会显示const，如果是连接查询，被驱动表的执行计划这里会显示驱动表的关联字段，如果是条件使用了表达式或者函数，或者条件列发生了内部隐式转换，这里可能显示为func\n9）rows\n这里是执行计划中估算的扫描行数，不是精确值\n10）extra\n这个列可以显示的信息非常多，有几十种，常用的有\nA：distinct：在select部分使用了distinc关键字\nB：no tables used：不带from字句的查询或者From dual查询\nC：使用not in()形式子查询或not exists运算符的连接查询，这种叫做反连接。即，一般连接查询是先查询内表，再查询外表，反连接就是先查询外表，再查询内表。\nD：using filesort：排序时无法使用到索引时，就会出现这个。常见于order by和group by语句中\nE：using index：查询时不需要回表查询，直接通过索引就可以获取查询的数据。\nF：using join buffer（block nested loop），using join buffer（batched key accss）：5.6.x之后的版本优化关联查询的BNL，BKA特性。主要是减少内表的循环数量以及比较顺序地扫描查询。\nG：using sort_union，using_union，using intersect，using sort_intersection：\nusing intersect：表示使用and的各个索引的条件时，该信息表示是从处理结果获取交集\nusing union：表示使用or连接各个使用索引的条件时，该信息表示从处理结果获取并集\nusing sort_union和using sort_intersection：与前面两个对应的类似，只是他们是出现在用and和or查询信息量大时，先查询主键，然后进行排序合并后，才能读取记录并返回。\nH：using temporary：表示使用了临时表存储中间结果。临时表可以是内存临时表和磁盘临时表，执行计划中看不出来，需要查看status变量，used_tmp_table，used_tmp_disk_table才能看出来。\nI：using where：表示存储引擎返回的记录并不是所有的都满足查询条件，需要在server层进行过滤。查询条件中分为限制条件和检查条件，5.6之前，存储引擎只能根据限制条件扫描数据并返回，然后server层根据检查条件进行过滤再返回真正符合查询的数据。5.6.x之后支持ICP特性，可以把检查条件也下推到存储引擎层，不符合检查条件和限制条件的数据，直接不读取，这样就大大减少了存储引擎扫描的记录数量。extra列显示using index condition\nJ：firstmatch(tb_name)：5.6.x开始引入的优化子查询的新特性之一，常见于where字句含有in()类型的子查询。如果内表的数据量比较大，就可能出现这个\nK：loosescan(m..n)：5.6.x之后引入的优化子查询的新特性之一，在in()类型的子查询中，子查询返回的可能有重复记录时，就可能出现这个\n除了这些之外，还有很多查询数据字典库，执行计划过程中就发现不可能存在结果的一些提示信息\n11）filtered\n使用explain extended时会出现这个列，5.7之后的版本默认就有这个字段，不需要使用explain extended了。这个字段表示存储引擎返回的数据在server层过滤后，剩下多少满足查询的记录数量的比例，注意是百分比，不是具体记录数。\n附图：\n\n\n具体慢查询的优化案例函数Max()的优化****用途：查询最后支付时间-优化max（）函数\n语句：\nsqlselect max(payment_date) from payment;\n执行计划：\nsqlexplain select max(payment_date) from payment;\n\n可以看到显示的执行计划，并不是很高效，可以拖慢服务器的效率，如何优化了？\n创建索引\nsqlcreate index inx_paydate on payment(payment_date);\n\n索引是顺序操作的，不需要扫描表，执行效率就会比较恒定，\n函数Count()的优化****需求：在一条SQL中同时查处2006年和2007年电影的数量\n错误的方式：\n语句：\nsqlselect count(release_year=&#39;2006&#39; or release_year=&#39;2007&#39;) from film;\n2006和2007年分别是多少，判断不出来\nsql select count(*) from film where release_year=&#39;2006&#39; or release_year=&#39;2007&#39;;\n正确的编写方式：\nsqlselect count(release_year=&#39;2006&#39; or null) as &#39;06films&#39;,count(release_year=&#39;2007&#39; or null) as &#39;07films&#39; from film;\n区别：count（*）和count（id）\n创建表并插入语句\nsql create table t(id int);\n\n insert into t values(1),(2),(null);\nsqlCount（*）：select count(*)from t;\nsqlCount（id）：select count(id)from t;\n说明：\nCount（id）是不包含null的值\nCount（*）是包含null的值\n子查询的优化        子查询是我们在开发过程中经常使用的一种方式，在通常情况下，需要把子查询优化为join查询但在优化是需要注意关联键是否有一对多的关系，要注意重复数据。\n查看我们所创建的t表\nsqlshow create table t;\n接下来我们创建一个t1表\nsqlcreate table t1(tid int);并插入一条数据\n\n我们要进行一个子查询，需求：查询t表中id在t1表中tid的所有数据；\nsqlselect * from t where t.id in (select t1.tid from t1);\n接下来我们用join的操作来进行操作\nsqlselect id from t join t1 on t.id =t1.tid;\n 通过上面结果来看，查询的结果是一致的，我们就将子查询的方式优化为join操作。\n接下来，我们在t1表中再插入一条数据\nsqlinsert into t1 values (1);\n\nselect * from t1;\n在这种情况下，如果我们使用子查询方式进行查询，返回的结果就是如下图所示：\n\n如果使用join方式进行查找，如下图所示：\n\n在这种情况下出现了一对多的关系，会出现数据的重复，我们为了方式数据重复，不得不使用distinct关键词进行去重操作\nsqlselect distinct id from t join t1 on t.id =t1.tid;\n注意：这个一对多的关系是我们开发过程中遇到的一个坑，出现数据重复，需要大家注意一下。\n例子：查询sandra出演的所有影片：\nsqlexplain select title,release_year,length\n\n from film\n\n where film_id in (\n\n select film_id from film_actor where actor_id in (\n\n select actor_id from actor where first_name=&#39;sandra&#39;));group by的优化最好使用同一表中的列，\n需求：每个演员所参演影片的数量-（影片表和演员表） \nsqlexplain select actor.first_name,actor.last_name,count(*)\n\nfrom sakila.film_actor\n\ninner join sakila.actor using(actor_id)\n\ngroup by film_actor.actor_id;\n优化后的SQL：\nsqlexplain select actor.first_name,actor.last_name,c.cnt\n\nfrom sakila.actor inner join (\n\nselect actor_id,count(*) as cnt from sakila.film_actor group by actor_id\n\n)as c using(actor_id);\n\n说明：从上面的执行计划来看，这种优化后的方式没有使用临时文件和文件排序的方式了，取而代之的是使用了索引。查询效率老高了。\n这个时候我们表中的数据比较大，会大量的占用IO操作，优化了sql执行的效率，节省了服务器的资源，因此我们就需要优化。\n注意：\n1、mysql 中using关键词的作用：也就是说要使用using,那么表a和表b必须要有相同的列。\n2、在用Join进行多表联合查询时，我们通常使用On来建立两个表的关系。其实还有一个更方便的关键字，那就是Using。\n3、如果两个表的关联字段名是一样的，就可以使用Using来建立关系，简洁明了。\nLimit查询的优化Limit常用于分页处理，时长会伴随order by从句使用，因此大多时候回使用Filesorts这样会造成大量的IO问题。\n例子：\n需求：查询影片id和描述信息，并根据主题进行排序，取出从序号50条开始的5条数据。\nsqlselect film_id,description from sakila.film order by title limit 50,5;执行的结果：\n\n在查看一下它的执行计划：\n 对于这种操作，我们该用什么样的优化方式了？\n优化步骤1：\n使用有索引的列或主键进行order by操作，因为大家知道，innodb是按照主键的逻辑顺序进行排序的。可以避免很多的IO操作。\nsqlselect film_id,description from sakila.film order by film_id limit 50,5; 查看一下执行计划\n\n 那如果我们获取从500行开始的5条记录，执行计划又是什么样的了？\nsqlexplain select film_id,description from sakila.film order by film_id limit 500,5\\G\n\n随着我们翻页越往后，IO操作会越来越大的，如果一个表有几千万行数据，翻页越后面，会越来越慢，因此我们要进一步的来优化。\n优化步骤2、记录上次返回的主键， 在下次查询时使用主键过滤。（说明：避免了数据量大时扫描过多的记录）\n上次limit是50,5的操作，因此我们在这次优化过程需要使用上次的索引记录值，\nsqlselect film_id,description from sakila.film  where film_id &gt;55 and film_id&lt;=60 order by film_id limit 1,5;查看执行计划：\n\n\n\n 结论：扫描行数不变，执行计划是很固定，效率也是很固定的\n注意事项：\n主键要顺序排序并连续的，如果主键中间空缺了某一列，或者某几列，会出现列出数据不足5行的数据；如果不连续的情况，建立一个附加的列index_id列，保证这一列数据要自增的，并添加索引即可。\n索引的优化1、什么是索引？****\n索引的作用相当于图书的目录，可以根据目录中的页码快速找到所需的内容。\n数据库使用索引以找到特定值，然后顺指针找到包含该值的行。在表中建立索引，然后在索引中找到符合查询条件的索引值，最后通过保存在索引中的ROWID（相当于页码）快速找到表中对应的记录。索引的建立是表中比较有指向性的字段，相当于目录，比如说行政区域代码，同一个地域的行政区域代码都是相同的，那么给这一列加上索引，避免让它重复扫描，从而达到优化的目的！\n2、如何创建索引\n在执行CREATE TABLE语句时可以创建索引，也可以单独用CREATE INDEX或ALTER TABLE来为表增加索引。\n1、ALTER TABLE\nALTER TABLE用来创建普通索引、UNIQUE索引或PRIMARY KEY索引。\nsqlALTER TABLE table_name ADD INDEX index_name (column_list)\n\nALTER TABLE table_name ADD UNIQUE (column_list)\n\nALTER TABLE table_name ADD PRIMARY KEY (column_list)说明：其中table_name是要增加索引的表名，column_list指出对哪些列进行索引，多列时各列之间用逗号分隔。索引名index_name可选，缺省时，MySQL将根据第一个索引列赋一个名称。另外，ALTER TABLE允许在单个语句中更改多个表，因此可以在同时创建多个索引。\n2、CREATE INDEX\nCREATE INDEX可对表增加普通索引或UNIQUE索引。\nsqlCREATE INDEX index_name ON table_name (column_list)\n\nCREATE UNIQUE INDEX index_name ON table_name (column_list)说明：table_name、index_name和column_list具有与ALTER TABLE语句中相同的含义，索引名不可选。另外，不能用CREATE INDEX语句创建PRIMARY KEY索引。\n3、索引类型\n在创建索引时，可以规定索引能否包含重复值。如果不包含，则索引应该创建为PRIMARY KEY或UNIQUE索引。对于单列惟一性索引，这保证单列不包含重复的值。对于多列惟一性索引，保证多个值的组合不重复。\nPRIMARY KEY索引和UNIQUE索引非常类似。\n事实上，PRIMARY KEY索引仅是一个具有名称PRIMARY的UNIQUE索引。这表示一个表只能包含一个PRIMARY KEY，因为一个表中不可能具有两个同名的索引。\n下面的SQL语句对students表在sid上添加PRIMARY KEY索引。\nsqlALTER TABLE students ADD PRIMARY KEY (sid)4、删除索引\n可利用ALTER TABLE或DROP INDEX语句来删除索引。类似于CREATE INDEX语句，DROP INDEX可以在ALTER TABLE内部作为一条语句处理，语法如下。  \n \nsqlDROP INDEX index_name ON talbe_name\n\nALTER TABLE table_name DROP INDEX index_name\n\nALTER TABLE table_name DROP PRIMARY KEY其中，前两条语句是等价的，删除掉table_name中的索引index_name。\n第3条语句只在删除PRIMARY KEY索引时使用，因为一个表只可能有一个PRIMARY KEY索引，因此不需要指定索引名。如果没有创建PRIMARY KEY索引，但表具有一个或多个UNIQUE索引，则MySQL将删除第一个UNIQUE索引。\n如果从表中删除了某列，则索引会受到影响。对于多列组合的索引，如果删除其中的某列，则该列也会从索引中删除。如果删除组成索引的所有列，则整个索引将被删除。\n   5、查看索引\nsqlshow index from tblname;\n\nshow keys from tblname;   6、什么情况下，使用索引了？\n1、表的主关键字\n2、自动建立唯一索引\n3、表的字段唯一约束\n4、直接条件查询的字段（在SQL中用于条件约束的字段）\n5、查询中与其它表关联的字段\n6、查询中排序的字段（排序的字段如果通过索引去访问那将大大提高排序速度）\n7、查询中统计或分组统计的字段\n8、表记录太少（如果一个表只有5条记录，采用索引去访问记录的话，那首先需访问索引表，再通过索引表访问数据表，一般索引表与数据表不在同一个数据块）\n9、经常插入、删除、修改的表（对一些经常处理的业务表应在查询允许的情况下尽量减少索引）\n10、数据重复且分布平均的表字段（假如一个表有10万行记录，有一个字段A只有T和F两种值，且每个值的分布概率大约为50%，那么对这种表A字段建索引一般不会提高数据库的查询速度。）\n11、经常和主字段一块查询但主字段索引值比较多的表字段\n12、对千万级MySQL数据库建立索引的事项及提高性能的手段\n3、如何选择合适的列建立索引****\n1、在where从句，group by从句，order by从句，on从句中的列添加索引\n2、索引字段越小越好（因为数据库数据存储单位是以“页”为单位的，数据存储的越多，IO也会越大）\n3、离散度大的列放到联合索引的前面\n例子：\nsqlselect * from payment where staff_id =2 and customer_id =584;注意:\n是index（staff_id，customer_id）好，还是index（customer_id，staff_id）好\n那我们怎么进行验证离散度好了？\nA、我们先查看一下表结构\nsqldesc payment;\nB、分别查看这两个字段中不同的id的数量，数量越多，则表明离散程度越大：因此可以通过下图看出：customer_id 离散程度大。\n\n结论：由于customer_id 离散程度大，使用index（customer_id，staff_id）好\nC、mysql联合索引\n①命名规则 ：表名_字段名\n1、需要加索引的字段，要在where条件中\n2、数据量少的字段不需要加索引\n3、如果where条件中是OR关系，加索引不起作用\n4、符合最左原则\n②什么是联合索引\n\n两个或更多个列上的索引被称作联合索引，又被称为是复合索引。\n利用索引中的附加列，您可以缩小搜索的范围，但使用一个具有两列的索引 不同于使用两个单独的索引。复合索引的结构与电话簿类似，人名由姓和名构成，电话簿首先按姓氏对进行排序，然后按名字对有相同姓氏的人进行排序。如果您知 道姓，电话簿将非常有用；如果您知道姓和名，电话簿则更为有用，但如果您只知道名不姓，电话簿将没有用处。\n\n所以说创建复合索引时，应该仔细考虑列的顺序。对索引中的所有列执行搜索或仅对前几列执行搜索时，复合索引非常有用；仅对后面的任意列执行搜索时，复合索引则没有用处。\n4、索引优化SQL的方法\n1、索引的维护及优化（重复及冗余索引）\n增加索引会有利于查询效率，但会降低insert，update，delete的效率，但实际上往往不是这样的，过多的索引会不但会影响使用效率，同时会影响查询效率，这是由于数据库进行查询分析时，首先要选择使用哪一个索引进行查询，如果索引过多，分析过程就会越慢，这样同样的减少查询的效率，因此我们要知道如何增加，有时候要知道维护和删除不需要的索引\n2、如何找到重复和冗余的索引\n重复索引：\n重复索引是指相同的列以相同的顺序建立的同类型的索引，如下表中的 primary key和ID列上的索引就是重复索引\nsqlcreate table test(\n\nid int not null primary key,\n\nname varchar(10) not null,\n\ntitle varchar(50) not null,\n\nunique(id)\n\n)engine=innodb;冗余索引：\n冗余索引是指多个索引的前缀列相同，或是在联合索引中包含了主键的索引，下面这个例子中key（name，id）就是一个冗余索引。\nsqlcreate table test(\n\nid int not null primary key,\n\nname varchar(10) not null,\n\ntitle varchar(50) not null,\n\nkey(name,id)\n\n)engine=innodb;说明：对于innodb来说，每一个索引后面，实际上都会包含主键，这时候我们建立的联合索引，又人为的把主键包含进去，那么这个时候就是一个冗余索引。\n3、如何查找重复索引\n工具：使用pt-duplicate-key-checker工具检查重复及冗余索引\n\n\n\n\n\n\n\n\n\npt-duplicate-key-checker -uroot -padmin -h 127.0.0.1\n\n4、索引维护的方法\n由于业务变更，某些索引是后续不需要使用的，就要进行删除。\n在mysql中，目前只能通过慢查询日志配合pt-index-usage工具来进行索引使用情况的分析；\n\n\n\n\n\n\n\n\n\npt-index-usage -uroot -padmin &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql-host-slow.lo\n 附：https://www.percona.com/downloads/\n5、注意事项****\n设计好MySql的索引可以让你的数据库飞起来，大大的提高数据库效率。设计MySql索引的时候有一下几点注意：\n1，创建索引\n对于查询占主要的应用来说，索引显得尤为重要。很多时候性能问题很简单的就是因为我们忘了添加索引而造成的，或者说没有添加更为有效的索引导致。如果不加索引的话，那么查找任何哪怕只是一条特定的数据都会进行一次全表扫描，如果一张表的数据量很大而符合条件的结果又很少，那么不加索引会引起致命的性能下降。\n但是也不是什么情况都非得建索引不可，比如性别可能就只有两个值，建索引不仅没什么优势，还会影响到更新速度，这被称为过度索引。\n2，复合索引\n比如有一条语句是这样的：select * from users where area&#x3D;’beijing’ and age&#x3D;22;\n如果我们是在area和age上分别创建单个索引的话，由于mysql查询每次只能使用一个索引，所以虽然这样已经相对不做索引时全表扫描提高了很多效率，但是如果在area、age两列上创建复合索引的话将带来更高的效率。如果我们创建了(area, age,salary)的复合索引，那么其实相当于创建了(area,age,salary)、(area,age)、(area)三个索引，这被称为最佳左前缀特性。\n因此我们在创建复合索引时应该将最常用作限制条件的列放在最左边，依次递减。\n3，索引不会包含有NULL值的列\n只要列中包含有NULL值都将不会被包含在索引中，复合索引中只要有一列含有NULL值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为NULL。\n4，使用短索引\n对字符串列进行索引，如果可能应该指定一个前缀长度。例如，如果有一个CHAR(255)的 列，如果在前10 个或20 个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I&#x2F;O操作。\n5，排序的索引问题\nmysql查询只使用一个索引，因此如果where子句中已经使用了索引的话，那么order by中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引。\n6，like语句操作\n一般情况下不鼓励使用like操作，如果非使用不可，如何使用也是一个问题。like “%aaa%” 不会使用索引而like “aaa%”可以使用索引。\n7，不要在列上进行运算\nselect * from users where\nYEAR(adddate)\n8，不使用NOT IN操作\nNOT IN操作都不会使用索引将进行全表扫描。NOT IN可以NOT EXISTS代替\nArticle link： https://tqgoblin.site/post/csdn/MYSQL-性能优化篇/  Author： Stephen  \n","slug":"csdn/MYSQL-性能优化篇","date":"2021-03-20T13:00:38.000Z","categories_index":"数据库","tags_index":"数据库 mysql","author_index":"Stephen"},{"id":"845ca224a2ba9eeb409ae729b9b9c1dd","title":"HBase详解","content":"第 1 章：****HBase 简介1.1 HBase 定义HBase 是一种分布式、可扩展、支持海量数据存储的 NoSQL 数据库。\n1.2 HBase 数据模型逻辑上， HBase 的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。但从 HBase 的底层物理存储结构（ K-V ）来看， HBase 更像是一个 multi-dimensional map。\n1.2.1 HBase 逻辑结构\n\n1.2.2 HBase 物理存储结构\n\n1.2.3 数据模型\n1 ） Name Space\n命名空间，类似于关系型数据库的 DatabBase 概念，每个命名空间下有多个表。 HBase有两个自带的命名空间，分别是 hbase 和 default ， hbase 中存放的是 HBase 内置的表， default 表是用户默认使用的命名空间。\n2 ） Region\n类似于关系型数据库的表概念。不同的是， HBase 定义表时只需要声明 列族 即可，不需要声明具体的列。这意味着，往 HBase 写入数据时，字段可以 动态 、 按需 指定。因此， 和关系型数据库相比，HBase 能够轻松应对字段变更的场景。\n3 ） Row\nHBase 表中的每行数据都由一个 RowKey 和多个 Column （列） 组成，数据是按照RowKey 的 字典顺序存储 的，并且查询数据时只能根据 RowKey 进行检索，所以 RowKey的设计十分重要。\n4 ） Column\nHBase 中的每个列都由 Column Family( 列族 ) 和 Column Qualifier （列限定符） 进行限定，例如 info ： name ， info ： age 。建表时，只需指明列族，而列限定符无需预先定义。\n5 ） Time Stamp\n用于标识数据的不同版本（version），每条数据写入时，如果不指定时间戳，系统会自动为其加上该字段，其值为写入 HBase 的时间。\n6 ） Cell\n由 {rowkey, column Family ： column Qualifier, time Stamp} 唯一确定的单元。 cell 中的数据是没有类型的，全部是字节码形式存贮。\n1.3 HBase 基本架构\n架构角色：\n1 ） Region Server\nRegion Server 为 Region 的管理者，其实现类为 HRegionServer ，主要作用如下：\n①对于数据的操作： get, put, delete ；\n②对于 Region 的操作： splitRegion 、 compactRegion 。\n2 ） Master\nMaster 是所有 Region Server 的管理者，其实现类为 HMaster ，主要作用如下：\n①对于表的操作： create, delete, alter\n②对于 RegionServer 的操作：分配 regions 到每个 RegionServer ，监控每个 RegionServer的状态，负载均衡和故障转移。\n3 ） Zookeeper\nHBase 通过 Zookeeper 来做 Master 的高可用、 RegionServer 的监控、元数据的入口以及集群配置的维护等工作。\n4 ） HDFS\nHDFS 为 HBase 提供最终的底层数据存储服务，同时为 HBase 提供高可用的支持。\n第 2 章：****HBase 快速入门2.1 HBase 安装部署2.1.1 Zookeeper 正常部署\n首先保证 Zookeeper 集群的正常部署，并启动之：\n\n\n\n\n\n\n\n\n\n[root@master zookeeper]# bin&#x2F;zkServer.sh start\n[root@node1 zookeeper]# bin&#x2F;zkServer.sh start\n[root@node2 zookeeper]# bin&#x2F;zkServer.sh start\n2.1.2 Hadoop 正常部署\nHadoop 集群的正常部署并启动：\n\n\n\n\n\n\n\n\n\n[root@master hadoop]# sbin&#x2F;start-all.sh\n2.1.3 HBase 的解压\n解压 Hbase 到指定目录：\n\n\n\n\n\n\n\n\n\n[root@master package]# tar -zxvf hbase-1.4.6-bin.tar.gz -C &#x2F;usr&#x2F;local&#x2F;soft&#x2F;\n2.1.4 HBase 的配置文件\n修改 HBase 对应的配置文件。\n1 ） hbase-env.sh 修改内容：\n\n\n\n\n\n\n\n\n\nexport JAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;soft&#x2F;jdk1.8.0_171\nexport HBASE_MANAGES_ZK&#x3D;false\n2 ） hbase-site.xml 修改内容：\n\n\n\n\n\n\n\n\n\n&lt;configuration&gt;\n        &lt;property&gt;\n                &lt;name&gt;hbase.rootdir&lt;&#x2F;name&gt;\n                &lt;value&gt;hdfs:&#x2F;&#x2F;master:9000&#x2F;HBase&lt;&#x2F;value&gt;\n        &lt;&#x2F;property&gt;\n        &lt;property&gt;\n                &lt;name&gt;hbase.cluster.distributed&lt;&#x2F;name&gt;\n                &lt;value&gt;true&lt;&#x2F;value&gt;\n        &lt;&#x2F;property&gt;\n        &lt;!– 0.98 后的新变动，之前版本没有 .port, 默认端口为 60000 –&gt;\n        &lt;property&gt;\n                &lt;name&gt;hbase.master.port&lt;&#x2F;name&gt;\n                &lt;value&gt;16000&lt;&#x2F;value&gt;\n        &lt;&#x2F;property&gt;\n        &lt;property&gt;\n                &lt;name&gt;hbase.zookeeper.quorum&lt;&#x2F;name&gt;\n                &lt;value&gt;master,node1,node2&lt;&#x2F;value&gt;\n        &lt;&#x2F;property&gt;\n        &lt;property&gt;\n                &lt;name&gt;hbase.zookeeper.property.dataDir&lt;&#x2F;name&gt;\n                &lt;value&gt;&#x2F;usr&#x2F;local&#x2F;soft&#x2F;hbase&#x2F;zkData&lt;&#x2F;value&gt;\n        &lt;&#x2F;property&gt;\n&lt;&#x2F;configuration&gt;\n3 ） regionservers ：\n\n\n\n\n\n\n\n\n\nmaster\nnode1\nnode2\n4 ）软连接 hadoop 配置文件到 HBase ：\n\n\n\n\n\n\n\n\n\n[root@master soft]# ln -s &#x2F;usr&#x2F;local&#x2F;soft&#x2F;hadoop-2.7.6&#x2F;etc&#x2F;hadoop&#x2F;core-site.xml &#x2F;usr&#x2F;local&#x2F;soft&#x2F;hbase&#x2F;conf&#x2F;core-site.xml\n[root@master soft]# ln -s &#x2F;usr&#x2F;local&#x2F;soft&#x2F;hadoop-2.7.6&#x2F;etc&#x2F;hadoop&#x2F;hdfs-site.xml &#x2F;usr&#x2F;local&#x2F;soft&#x2F;hbase&#x2F;conf&#x2F;hdfssite.xml\n2.1.5 HBase 远程发送到其他集群\n\n\n\n\n\n\n\n\n\n[root@master soft]# scp -r hbase node1:`pwd`\n[root@master soft]# scp -r hbase node2:`pwd`\n2.1.6 HBase 服务的启动\n1 ．启动方式\n\n\n\n\n\n\n\n\n\n[root@master hbase]# bin&#x2F;hbase-daemon.sh start master\n[root@master hbase]# bin&#x2F;hbase-daemon.sh start regionserver\n提示： 如果集群之间的节点时间不同步，会导致 regionserver 无法启动，抛出 ClockOutOfSyncException 异常。\n修复提示：\n①百度， linux 时间同步\n②属性： hbase.master.maxclockskew 设置更大的值\n\n\n\n\n\n\n\n\n\n&lt;property&gt;\n        &lt;name&gt;hbase.master.maxclockskew&lt;&#x2F;name&gt;\n        &lt;value&gt;180000&lt;&#x2F;value&gt;\n        &lt;description&gt;Time difference of regionserver from master&lt;&#x2F;description&gt;\n&lt;&#x2F;property&gt;\n2 ．启动方式 2\n\n\n\n\n\n\n\n\n\n[root@master hbase]# bin&#x2F;start-hbase.sh\n对应的停止服务：\n\n\n\n\n\n\n\n\n\n[root@master hbase]# bin&#x2F;stop-hbase.sh\n2.1.7 查看 HBase 页面\n启动成功后，可以通过“host:port”的方式来访问 HBase 管理页面，例如：\n\n\n\n\n\n\n\n\n\nhttp://master:16010\n2.2 HBase Shell 操作2.2.1 基本操作\n1 ．进入 HBase 客户端命令行\n\n\n\n\n\n\n\n\n\n[root@master hbase]# bin&#x2F;hbase shell\n2 ．查看帮助命令\n\n\n\n\n\n\n\n\n\nhbase(main):001:0&gt; help\n3 ．查看当前数据库中有哪些表\n\n\n\n\n\n\n\n\n\nhbase(main):002:0&gt; list\n2.2.2 表的操作\n1 ．创建表\n\n\n\n\n\n\n\n\n\nhbase(main):002:0&gt; create ‘student’,’info’\n2 ．插入数据到表\n\n\n\n\n\n\n\n\n\nhbase(main):003:0&gt; put ‘student’,’1001’,’info:sex’,’male’\nhbase(main):004:0&gt; put ‘student’,’1001’,’info:age’,’18’\nhbase(main):005:0&gt; put ‘student’,’1002’,’info:name’,’Janna’\nhbase(main):006:0&gt; put ‘student’,’1002’,’info:sex’,’female’\nhbase(main):007:0&gt; put ‘student’,’1002’,’info:age’,’20’\n3 ．扫描查看表数据\n\n\n\n\n\n\n\n\n\nhbase(main):008:0&gt; scan ‘student’\nhbase(main):009:0&gt; scan ‘student’,{STARTROW &#x3D;&gt; ‘1001’, STOPROW &#x3D;&gt; ‘1001’}\nhbase(main):010:0&gt; scan ‘student’,{STARTROW &#x3D;&gt; ‘1001’}\n4 ．查看表结构\n\n\n\n\n\n\n\n\n\nhbase(main):011:0&gt; describe ‘student’\n5 ．更新指定字段的数据\n\n\n\n\n\n\n\n\n\nhbase(main):012:0&gt; put ‘student’,’1001’,’info:name’,’Nick’\nhbase(main):013:0&gt; put ‘student’,’1001’,’info:age’,’100’\n6 ．查看“指定行”或“指定列族 : 列”的数据\n\n\n\n\n\n\n\n\n\nhbase(main):014:0&gt; get ‘student’,’1001’\nhbase(main):015:0&gt; get ‘student’,’1001’,’info:name’\n7 ．统计表数据行数\n\n\n\n\n\n\n\n\n\nhbase(main):021:0&gt; count ‘student’\n8 ．删除数据\n删除某 rowkey 的全部数据：\n\n\n\n\n\n\n\n\n\nhbase(main):016:0&gt; deleteall ‘student’,’1001’\n删除某 rowkey 的某一列数据：\n\n\n\n\n\n\n\n\n\nhbase(main):017:0&gt; delete ‘student’,’1002’,’info:sex’\n9 ．清空表数据\n\n\n\n\n\n\n\n\n\nhbase(main):018:0&gt; truncate ‘student’\n提示：清空表的操作顺序为先 disable ，然后再 truncate 。\n10 ．删除表\n首先需要先让该表为 disable 状态：\n\n\n\n\n\n\n\n\n\nhbase(main):019:0&gt; disable ‘student’\n然后才能 drop 这个表：\n\n\n\n\n\n\n\n\n\nhbase(main):020:0&gt; drop ‘student’\n提示：如果直接 drop 表，会报错： ERROR: Table student is enabled. Disable it first.\n11 ．变更表信息\n将 info 列族中的数据存放 3 个版本：\n\n\n\n\n\n\n\n\n\nhbase(main):022:0&gt; alter ‘student’,{NAME&#x3D;&gt;’info’,VERSIONS&#x3D;&gt;3}\nhbase(main):022:0&gt; get ‘student’,’1001’,{COLUMN&#x3D;&gt;’info:name’,VERSIONS&#x3D;&gt;3}\n第 3 章 HBase 进阶3.1 架构原理\n1 ） StoreFile\n保存实际数据的物理文件， StoreFile 以 HFile 的形式存储在 HDFS 上。每个 Store 会有一个或多个 StoreFile （ HFile ），数据在每个 StoreFile 中都是有序的。\n2 ） MemStore\n写缓存，由于 HFile 中的数据要求是有序的，所以数据是先存储在 MemStore 中，排好序后，等到达刷写时机才会刷写到 HFile ，每次刷写都会形成一个新的 HFile 。\n3 ） WAL\n由于数据要经 MemStore 排序后才能刷写到 HFile ，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做 Write-Ahead logfile 的文件中，然后再写入MemStore 中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。\n3.2 写流程\n写流程：\n1 ） Client 先访问 zookeeper ，获取 hbase:meta 表位于哪个 Region Server 。\n2 ）访问对应的 Region Server ， 获取 hbase:meta 表 ， 根据读请求的namespace:table&#x2F;rowkey，查询出目标数据位于哪个 Region Server 中的哪个 Region 中。并将该 table 的 region 信息以及 meta 表的位置信息缓存在客户端的 meta cache ，方便下次访问。\n3 ）与目标 Region Server 进行通讯；\n4 ）将数据顺序写入（追加）到 WAL ；\n5 ）将数据写入对应的 MemStore ，数据会在 MemStore 进行排序；\n6 ）向客户端发送 ack ；\n7 ）等达到 MemStore 的刷写时机后，将数据刷写到 HFile 。\n3.3 MemStore FlushMemStore 刷写时机：\n\n当某个 memstroe 的大小达到了 hbase.hregion.memstore.flush.size （默认值 128M ） ，其所在 region 的所有 memstore 都会刷写 。\n\n当 memstore 的大小达到了：\nhbase.hregion.memstore.flush.size （默认值 128M **） **\nhbase.hregion.memstore.block.multiplier （默认值 4 ） 时，会阻止继续 往该 memstore 写数据。\n\n当 region server 中 memstore 的总大小达到： java_heapsize\n\n*hbase.regionserver.global.memstore.size （默认值 0.4 ）\n*hbase.regionserver.global.memstore.size.lower.limit （默认值 0.95 ）\nregion 会按照其所有 memstore 的大小顺序（由大到小）依次进行刷写。直到 region server 中所有 memstore 的总大小减小到上述值以下。\n当 region server 中 memstore 的总大小达到 java_heapsize*hbase.regionserver.global.memstore.size （默认值 0.4 ）时，会阻止继续往所有的 memstore 写数据。\n3. 到达自动刷写的时间，也会触发 memstore flush 。自动刷新的时间间隔由该属性进行配置 hbase.regionserver.optionalcacheflushinterval （默认 1 小时）。\n\n当 WAL 文件的数量超过 hbase.regionserver.max.logs ， region 会按照时间顺序依次进行刷写，直到 WAL 文件数量减小到 hbase.regionserver.max.log 以下（ 该属性名已经废 弃，现无需手动设置，最大值为 32 ）。\n\n3.4 读流程\n读流程\n1 ） Client 先访问 zookeeper ，获取 hbase:meta 表位于哪个 Region Server 。\n2 ）访问对应的 Region Server ， 获 取 hbase:meta 表 ， 根据读请求的namespace:table&#x2F;rowkey，查询出目标数据位于哪个 Region Server 中的哪个 Region 中。并将该 table 的 region 信息以及 meta 表的位置信息缓存在客户端的 meta cache ，方便下次访问。\n3 ）与目标 Region Server 进行通讯；\n4 ）分别在 Block Cache （读缓存）， MemStore 和 Store File （ HFile ）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put&#x2F;Delete）。\n5 ） 将从文件中查询到的数据块（ Block ， HFile 数据存储单元，默认大小为 64KB ）缓存到 Block Cache 。\n6 ）将合并后的最终结果返回给客户端。\n3.5 StoreFile Compaction由于 memstore 每次刷写都会生成一个新的 HFile ，且同一个字段的不同版本（timestamp） 和不同类型（Put&#x2F;Delete）有可能会分布在不同的 HFile 中，因此查询时需要遍历所有的HFile。为了减少 HFile 的个数，以及清理掉过期和删除的数据，会进行 StoreFile Compaction 。Compaction 分为两种，分别是 Minor Compaction 和 Major Compaction 。 Minor Compaction 会将临近的若干个较小的 HFile 合并成一个较大的 HFile ，但 不会清理过期和 删除的数据 。Major Compaction 会将一个 Store 下的所有的 HFile 合并成一个大 HFile ，并且 会清 理掉过期和删除的数据 。\n3.6 Region Split\n默认情况下，每个 Table 起初只有一个 Region ，随着数据的不断写入， Region 会自动进行拆分。刚拆分时，两个子 Region 都位于当前的 Region Server ，但处于负载均衡的考虑， HMaster 有可能会将某个 Region 转移给其他的 Region Server 。\nRegion Split 时机：\n\n当 1 个 region 中的某个 Store 下所有 StoreFile 的总大小超 hbase.hregion.max.filesize ，该 Region 就会进行拆分（0.94 版本之前）。\n\n2. 当 1 个 region 中的某个 Store 下所有 StoreFile 的总大小超过 Min(R^2*“hbase.hregion.memstore.flush.size”,hbase.hregion.max.filesize”)，该 Region 就会进行拆分，其中 R 为当前 Region Server 中属于该 Table 的个数（0.94 版本之后）。\n\n第 4 章 HBase API4.1 环境准备新建项目后在 pom.xml 中添加依赖：\n\n\n\n\n\n\n\n\n\n&lt;!– https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper –&gt;\n&lt;dependency&gt;\n        &lt;groupId&gt;org.apache.zookeeper&lt;&#x2F;groupId&gt;\n        &lt;artifactId&gt;zookeeper&lt;&#x2F;artifactId&gt;\n        &lt;version&gt;3.4.1&lt;&#x2F;version&gt;\n&lt;&#x2F;dependency&gt;\n&lt;!–hbase–&gt;\n&lt;dependency&gt;\n        &lt;groupId&gt;org.apache.hbase&lt;&#x2F;groupId&gt;\n        &lt;artifactId&gt;hbase-client&lt;&#x2F;artifactId&gt;\n        &lt;version&gt;1.4.6&lt;&#x2F;version&gt;\n&lt;&#x2F;dependency&gt;\n4.2 HBaseAPI4.2.1 获取 Configuration 对象\n\n\n\n\n\n\n\n\n\npublic static Configuration conf;\nstatic{\n&#x2F;&#x2F; 使用 HBaseConfiguration 的单例方法实例化\nconf &#x3D; HBaseConfiguration.create();\nconf.set(“hbase.zookeeper.quorum”, “192.168.152.100”);\nconf.set(“hbase.zookeeper.property.clientPort”, “2181”);\n}\n4.2.2 判断表是否存在\n\n\n\n\n\n\n\n\n\npublic static boolean isTableExist(String tableName) throws\nMasterNotRunningException,\nZooKeeperConnectionException, IOException{\n&#x2F;&#x2F; 在 HBase 中管理、访问表需要先创建 HBaseAdmin 对象\n&#x2F;&#x2F;Connection connection &#x3D; ConnectionFactory.createConnection(conf);\n&#x2F;&#x2F;HBaseAdmin admin &#x3D; (HBaseAdmin) connection.getAdmin();\nHBaseAdmin admin &#x3D; new HBaseAdmin(conf);\nreturn admin.tableExists(tableName);\n}\n4.2.3 创建表\n\n\n\n\n\n\n\n\n\npublic static void createTable(String tableName, String…\ncolumnFamily) throws\nMasterNotRunningException, ZooKeeperConnectionException,\nIOException{\nHBaseAdmin admin &#x3D; new HBaseAdmin(conf);\n&#x2F;&#x2F; 判断表是否存在\nif(isTableExist(tableName)){\nSystem.out.println(“ 表 “ + tableName + “ 已存在 “);\n&#x2F;&#x2F;System.exit(0);\n}else{\n&#x2F;&#x2F; 创建表属性对象 , 表名需要转字节\nHTableDescriptor descriptor &#x3D;\nnew\nHTableDescriptor(TableName.valueOf(tableName));\n&#x2F;&#x2F; 创建多个列族\nfor(String cf : columnFamily){\ndescriptor.addFamily(new HColumnDescriptor(cf));\n}\n&#x2F;&#x2F; 根据对表的配置，创建表\nadmin.createTable(descriptor);\nSystem.out.println(“ 表 “ + tableName + “ 创建成功！ “);\n}\n}\n4.2.4 删除表\n\n\n\n\n\n\n\n\n\npublic static void dropTable(String\ntableName) throws\nMasterNotRunningException,\nZooKeeperConnectionException, IOException{\nHBaseAdmin admin &#x3D; new HBaseAdmin(conf);\nif(isTableExist(tableName)){\nadmin.disableTable(tableName);\nadmin.deleteTable(tableName);\nSystem.out.println(“ 表 “ + tableName + “ 删除成功！ “);\n}else{\nSystem.out.println(“ 表 “ + tableName + “ 不存在！ “);\n}\n}\n4.2.5 向表中插入数据\n\n\n\n\n\n\n\n\n\npublic static void addRowData(String tableName, String rowKey,\nString columnFamily, String\ncolumn, String value) throws IOException{\n&#x2F;&#x2F; 创建 HTable 对象\nHTable hTable &#x3D; new HTable(conf, tableName);\n&#x2F;&#x2F; 向表中插入数据\nPut put &#x3D; new Put(Bytes.toBytes(rowKey));\n&#x2F;&#x2F; 向 Put 对象中组装数据\nput.add(Bytes.toBytes(columnFamily),\nBytes.toBytes(column),\nBytes.toBytes(value));\nhTable.put(put);\nhTable.close();\nSystem.out.println(“ 插入数据成功 “);\n}\n4.2.6 删除多行数据\n\n\n\n\n\n\n\n\n\npublic static void deleteMultiRow(String tableName, String…\nrows)\nthrows IOException{\nHTable hTable &#x3D; new HTable(conf, tableName);\nList&lt;Delete&gt; deleteList &#x3D; new ArrayList&lt;Delete&gt;();\nfor(String row : rows){\nDelete delete &#x3D; new Delete(Bytes.toBytes(row));\ndeleteList.add(delete);\n}\nhTable.delete(deleteList);\nhTable.close();\n}\n4.2.7 获取所有数据\n\n\n\n\n\n\n\n\n\npublic static void getAllRows(String tableName) throws\nIOException{\nHTable hTable &#x3D; new HTable(conf, tableName);\n&#x2F;&#x2F; 得到用于扫描 region 的对象\nScan scan &#x3D; new Scan();\n&#x2F;&#x2F; 使用 HTable 得到 resultcanner 实现类的对象\nResultScanner resultScanner &#x3D; hTable.getScanner(scan);\nfor(Result result : resultScanner){\nCell[] cells &#x3D; result.rawCells();\nfor(Cell cell : cells){\n&#x2F;&#x2F; 得到 rowkey\nSystem.out.println(“\n行 键\n:”\n\n\n\nBytes.toString(CellUtil.cloneRow(cell)));\n&#x2F;&#x2F; 得到列族\nSystem.out.println(“\n列 族\n“ +\nBytes.toString(CellUtil.cloneFamily(cell)));\nSystem.out.println(“\n列\n:” +\nBytes.toString(CellUtil.cloneQualifier(cell)));\nSystem.out.println(“\n值\n:” +\nBytes.toString(CellUtil.cloneValue(cell)));\n}\n}\n}\n4.2.8 获取某一行数据\n\n\n\n\n\n\n\n\n\npublic static void getRow(String tableName, String rowKey)\nthrows\nIOException{\nHTable table &#x3D; new HTable(conf, tableName);\nGet get &#x3D; new Get(Bytes.toBytes(rowKey));\n&#x2F;&#x2F;get.setMaxVersions(); 显示所有版本\n&#x2F;&#x2F;get.setTimeStamp(); 显示指定时间戳的版本\nResult result &#x3D; table.get(get);\nfor(Cell cell : result.rawCells()){\nSystem.out.println(“\n行 键\n:” +\nBytes.toString(result.getRow()));\nSystem.out.println(“\n列 族\n“ +\nBytes.toString(CellUtil.cloneFamily(cell)));\nSystem.out.println(“\n列\n:” +\nBytes.toString(CellUtil.cloneQualifier(cell)));\nSystem.out.println(“\n值\n:” +\nBytes.toString(CellUtil.cloneValue(cell)));\nSystem.out.println(“ 时间戳 :” + cell.getTimestamp());\n}\n}\n4.2.9 获取某一行指定“列族 : 列”的数据\n\n\n\n\n\n\n\n\n\npublic static void getRowQualifier(String tableName, String\nrowKey,\nString family, String\nqualifier) throws IOException{\nHTable table &#x3D; new HTable(conf, tableName);\nGet get &#x3D; new Get(Bytes.toBytes(rowKey));\nget.addColumn(Bytes.toBytes(family),\nBytes.toBytes(qualifier));\nResult result &#x3D; table.get(get);\nfor(Cell cell : result.rawCells()){\nSystem.out.println(“\n行 键\n:” +\nBytes.toString(result.getRow()));\nSystem.out.println(“\n列 族\n“ +\nBytes.toString(CellUtil.cloneFamily(cell)));\nSystem.out.println(“\n列\n:” +\nBytes.toString(CellUtil.cloneQualifier(cell)));\nSystem.out.println(“\n值\n:” +\nBytes.toString(CellUtil.cloneValue(cell)));\n}\n}\n第 5 章 Hbase 过滤器5.1 Hbase 过滤器简介HBase 的基本 API，包括增、删、改、查等。 增、删都是相对简单的操作，与传统的 RDBMS 相比，这里的查询操作略显苍白，只能根据特性的行键进行查询（Get）或者根据行键的范围来查询（Scan）。 HBase 不仅提供了这些简单的 查询，而且提供了更加高级的过滤器（Filter）来查询。\n5.2 过滤器的两类参数过滤器可以根据列族、列、版本等更多的条件来对数据进行过滤， 基于HBase 本身提供的三维有序（行键，列，版本有序），这些过滤器可以高效地完成查询过滤的任务，带有过滤器条件的 RPC 查询请求会把过滤器分发到各个RegionServer（这是一个服务端过滤器），这样也可以降低网络传输的压力。 使用过滤器至少需要两类参数： 一类是抽象的操作符，另一类是比较器\n5.3 操作符HBase 提供了枚举类型的变量来表示这些抽象的操作符：\nLESS ： 小于\nLESS_OR_EQUAL ： 小于等于\nEQUAL ： 等于\nNOT_EQUAL：不等于\nGREATER_OR_EQUAL ： 大于等于\nGREATER： 大于\nNO_OP ; 不比较\n5.4 比较器比较器作为过滤器的核心组成之一，用于处理具体的比较逻辑，例如字节级的比较，字符串级的比较等。\n（ 1 ） RegexStringComparator ：支持正则表达式的值比较\n\n\n\n\n\n\n\n\n\nScan scan &#x3D; new Scan();\nRegexStringComparator comp &#x3D; new RegexStringComparator( “文科 * ” );\n&#x2F;&#x2F; 以 文科 开头的字符串\nSingleColumnValueFilter filter &#x3D; new\nSingleColumnValueFilter(Bytes.toBytes(“info”),\nBytes.toBytes(“clazz”), CompareOp.EQUAL, comp);\nscan.setFilter(filter);\n（2） SubStringComparator：用于监测一个子串是否存在于值中，并且不区分大小写。\n\n\n\n\n\n\n\n\n\nScan scan &#x3D; new Scan();\nSubstringComparator comp &#x3D; new SubstringComparator(“1129”);\n&#x2F;&#x2F; 查找包含 1129 的字符串\nSingleColumnValueFilter filter &#x3D; new\nSingleColumnValueFilter(Bytes.toBytes(“info”),\nBytes.toBytes(“clazz”), CompareOp.EQUAL, comp);\nscan.setFilter(filter);\n（3） BinaryComparator：二进制比较器，用于按字典顺序比较 Byte 数据值。\n\n\n\n\n\n\n\n\n\nScan scan &#x3D; new Scan();\nBinaryComparator comp &#x3D; new\nBinaryComparator(Bytes.toBytes(“xmei”));\n&#x2F;&#x2F; ValueFilter filter &#x3D; new ValueFilter(CompareOp.EQUAL, comp);\nscan.setFilter(filter);\n（4） BinaryPrefixComparator：前缀二进制比较器。与二进制比较器不同的是，只比较前缀是否相同。\n\n\n\n\n\n\n\n\n\nScan scan &#x3D; new Scan();\nBinaryPrefixComparator comp &#x3D; new\nBinaryPrefixComparator(Bytes.toBytes(“yting”)); &#x2F;&#x2F;\nSingleColumnValueFilter filter &#x3D; new\nSingleColumnValueFilter(Bytes.toBytes(“family”),\nBytes.toBytes(“qualifier”), CompareOp.EQUAL, comp);\nscan.setFilter(filter);\n5.5 过滤器列值过滤器：效率较低，需要做全表扫描 SingleColumnValueFilter：用于测试值的情况（相等，不等，范围 、、、）\n列簇过滤器： FamilyFilter：用于过滤列族（通常在 Scan 过程中通过设定某些列族来实现该功能，而不是直接使用该过滤器）。\n列名过滤器： QualifierFilter：用于列名（Qualifier）过滤。\n行键过滤器：效率较高，行键前缀过滤效率较高\nRowFilter：行键过滤器，一般来讲，执行 Scan 使用 startRow&#x2F;stopRow 方式比较好，而 RowFilter 过滤器也可以完成对某一行的过滤。\nBloom Filter 布隆过滤器\n（1）Bloom Filter 简介\n        Bloom Filter（布隆过滤器）是 1970 年由布隆提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。 在计算机科学中，我们常常会碰到时间换空间或者空间换时间的情况，即为了达到某一个方面的最优而牺牲另一个方面。\n        Bloom Filter 在时间空间这两个因素之外又引入了另一个因素：错误率。在使用 Bloom Filter 判断一个元素是否属于某个集合时，会有一定的错误率。也就是说，有可能把不属于这个集合的元素误认为属于这个集合（False Positive），但不会把属于这个集合的元素误认为不属于这个集合（False Negative）。在增加了错误率这个因素之后，Bloom Filter 通过允许少量的错误来节省大量的存储空间。它的用法其实是很容易理解的，我们拿个 HBase 中应用的例子来说下，我们已经知道 rowKey 存放在 HFile 中，那么为了从一系列的 HFile 中查询某个 rowkey，我们就可以通过 Bloom Filter 快速判断 rowkey 是否在这个 HFile 中，从而过滤掉大部分的 HFile，减少需要扫描的 Block。\n（2） Bloom Filter 工作原理\n        BloomFilter 对于 HBase 的随机读性能至关重要，对于 get 操作以及部分scan 操作可以剔除掉不会用到的 HFile 文件，减少实际 IO 次数，提高随机读性能。在此简单地介绍一下 Bloom Filter 的工作原理，Bloom Filter 使用位数组来实现过滤，初始状态下位数组每一位都为 0，如下图所示：\n\n        假如此时有一个集合 S &#x3D; {x1, x2, … xn}，Bloom Filter 使用 k 个独立的hash 函数，分别将集合中的每一个元素映射到｛1,…,m｝的范围。对于任何一个元素，被映射到的数字作为对应的位数组的索引，该位会被置为 1。比如元素 x1 被 hash 函数映射到数字 8，那么位数组的第 8 位就会被置为 1。下图中集合 S只有两个元素 x 和 y，分别被 3 个 hash 函数进行映射，映射到的位置分别为（\n0，3，6）和（4，7，10），对应的位会被置为 1:\n\n        现在假如要判断另一个元素是否是在此集合中，只需要被这 3 个 hash 函数 进行映射，查看对应的位置是否有 0 存在，如果有的话，表示此元素肯定不存在 于这个集合，否则有可能存在。下图所示就表示 z 肯定不在集合｛x，y｝中：\n\n从上面的内容我们可以得知，Bloom Filter 有两个很重要的参数\n哈希函数个数\n位数组的大小\n（3） Bloom Filter 在 HBase 中的应用\n        HFile 中和 Bloom Filter 相关的 Block， Scanned Block Section（扫描 HFile 时被读取）：Bloom Block\n        Load-on-open-section（regionServer 启动时加载到内存）：BloomFilter Meta Block、Bloom Index Block\n        Bloom Block：Bloom 数据块，存储 Bloom 的位数组\n        Bloom Index Block：Bloom 数据块的索引\n        BloomFilter Meta Block：从 HFile 角度看 bloom 数据块的一些元数据信息，大小个数等等。         HBase 中每个 HFile 都有对应的位数组，KeyValue 在写入 HFile时会先经过几个 hash 函数的映射，映射后将对应的数组位改为 1，get 请求进来之后再进行 hash 映射，如果在对应数组位上存在 0，说明该 get 请求查询的数据不在该 HFile 中。\n        HFile 中的 Bloom Block 中存储的就是上面说得位数组，当 HFile 很大时，Data Block 就会很多，同时 KeyValue 也会很多，需要映射入位数组的 rowKey也会很多，所以为了保证准确率，位数组就会相应越大，那 Bloom Block 也会越大，为了解决这个问题就出现了 Bloom Index Block，一个 HFile 中有多个 Bloom Block（位数组），根据 rowKey 拆分，一部分连续的 Key 使用一个位数组。这样查询 rowKey 就要先经过 Bloom Index Block（在内存中）定位到 Bloom Block，再把Bloom Block 加载到内存，进行过滤。\n5.6 示列代码写代码之前需要将 hbase 中的配置文件 hbase-site.xml 文件放在 idea 的 resource 文件夹\n下面\n代码实现\njavapackage shujia;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.hbase.Cell;\nimport org.apache.hadoop.hbase.CellUtil;\nimport org.apache.hadoop.hbase.HBaseConfiguration;\nimport org.apache.hadoop.hbase.TableName;\nimport org.apache.hadoop.hbase.client.*;\nimport org.apache.hadoop.hbase.filter.*;\nimport org.apache.hadoop.hbase.util.Bytes;\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Test;\nimport java.io.IOException;\npublic class Demo4Filter &#123;\nConnection conn;\nAdmin admin;\nTableName studentTN;\nTable student;\npublic void printRSWithFilter(Filter filter) throws\nIOException &#123;\nScan scan = new Scan();\nscan.setFilter(filter);\nResultScanner scanner = student.getScanner(scan);\nfor (Result rs : scanner) &#123;\nString id = Bytes.toString(rs.getRow());\nString name =\nBytes.toString(rs.getValue(&quot;info&quot;.getBytes(),\n&quot;name&quot;.getBytes()));\nString age =\nBytes.toString(rs.getValue(&quot;info&quot;.getBytes(),\n&quot;age&quot;.getBytes()));\nString gender =\nBytes.toString(rs.getValue(&quot;info&quot;.getBytes(),\n&quot;gender&quot;.getBytes()));\nString clazz =\nBytes.toString(rs.getValue(&quot;info&quot;.getBytes(),\n&quot;clazz&quot;.getBytes()));\nSystem.out.println(id + &quot;,&quot; + name + &quot;,&quot; + age + &quot;,&quot; +\ngender + &quot;,&quot; + clazz);\n&#125;\n&#125;\n@Before\npublic void createConn() throws IOException &#123;\n// 1、创建一个配置文件\nConfiguration conf = HBaseConfiguration.create();\n// 配置 ZK 的地址，通过 ZK 可以找到 HBase\nconf.set(&quot;hbase.zookeeper.quorum&quot;,\n&quot;master:2181,node1:2181,node2:2181&quot;);\n// 2、创建连接\nconn = ConnectionFactory.createConnection(conf);\n// 3、创建 Admin 对象\nadmin = conn.getAdmin();\nstudentTN = TableName.valueOf(&quot;student&quot;);\nstudent = conn.getTable(studentTN);\n&#125;\n@Test\n/**\n* age &gt; 23 的学生\n* ValueFilter:作用在每一个 cell 上，符合要求的 cell 不会被过滤\n* 结果不符合正常的思维逻辑\n*/\npublic void ValueFilter1() throws IOException &#123;\n// 创建二进制比较器\nBinaryComparator binaryComparator = new\nBinaryComparator(&quot;23&quot;.getBytes());\n// 列值过滤器\nValueFilter valueFilter = new\nValueFilter(CompareFilter.CompareOp.GREATER, binaryComparator);\nprintRSWithFilter(valueFilter);\n&#125;\n@Test\n/**\n* SingleColumnValueFilter 单列值过滤器\n* 可以指定一个列进行过滤\n* 该过滤器会将符合过滤条件的列对应的 cell 所在的整行数据进行返回\n* 如果某条数据的列不符合条件，则会将整条数据进行过滤\n* 如果数据中不存在指定的列，则默认会直接返回\n*\n* age &gt; 23 的学生\n*/\npublic void SingleColumnValueFilter1() throws IOException &#123;\nSingleColumnValueFilter singleColumnValueFilter = new\nSingleColumnValueFilter(\n&quot;info&quot;.getBytes(),\n&quot;age&quot;.getBytes(),\nCompareFilter.CompareOp.GREATER,\n&quot;23&quot;.getBytes()\n);\nprintRSWithFilter(singleColumnValueFilter);\n&#125;\n@Test\n/**\n* 过滤文科班的学生\n* SingleColumnValueExcludeFilter 单列值排除过滤器\n* 同单列值过滤器最大的区别在于最后的返回结果包不包含比较的列\n*/\npublic void SingleColumnValueExcludeFilter2() throws\nIOException &#123;\nBinaryPrefixComparator binaryPrefixComparator = new\nBinaryPrefixComparator(&quot;文科&quot;.getBytes());\nSingleColumnValueExcludeFilter\nsingleColumnValueExcludeFilter = new\nSingleColumnValueExcludeFilter(\n&quot;info&quot;.getBytes(),\n&quot;clazz&quot;.getBytes(),\nCompareFilter.CompareOp.EQUAL,\nbinaryPrefixComparator\n);\nprintRSWithFilter(singleColumnValueExcludeFilter);\n&#125;\n@Test\n/**\n* 过滤出 rowkey（id）以 150010088 开头的学生\n*/\npublic void RowFilter1() throws IOException &#123;\nBinaryPrefixComparator binaryPrefixComparator = new\nBinaryPrefixComparator(&quot;150010088&quot;.getBytes());\nRowFilter rowFilter = new\nRowFilter(CompareFilter.CompareOp.EQUAL,\nbinaryPrefixComparator);\nprintRSWithFilter(rowFilter);\n&#125;\n@Test\n/**\n* 过滤出 rowkey（id）以 150010088 开头的学生\n* 使用 PrefixFilter:rowkey 前缀过滤器\n* 相当于 BinaryPrefixComparator+RowFilter\n*/\npublic void PrefixFilter1() throws IOException &#123;\nPrefixFilter prefixFilter = new\nPrefixFilter(&quot;150010088&quot;.getBytes());\nprintRSWithFilter(prefixFilter);\n&#125;\n@Test\n/**\n* 通过正则表达式： [A-Za-z0-9]&#123;1&#125;f[0-9]+ 过滤出符合条件的列簇下的\n所有 cell\n*/\npublic void RegexFamilyFilter() throws IOException &#123;专注于大数据！\nRegexStringComparator regexStringComparator = new\nRegexStringComparator(&quot;[A-Za-z0-9]&#123;1&#125;f[0-9]+&quot;);\nFamilyFilter familyFilter = new\nFamilyFilter(CompareFilter.CompareOp.EQUAL,\nregexStringComparator);\nScan scan = new Scan();\nscan.setFilter(familyFilter);\nResultScanner scanner = student.getScanner(scan);\nfor (Result rs : scanner) &#123;\nfor (Cell cell : rs.listCells()) &#123;\nString cf =\nBytes.toString(CellUtil.cloneFamily(cell));\nString q =\nBytes.toString(CellUtil.cloneQualifier(cell));\nString value =\nBytes.toString(CellUtil.cloneValue(cell));\nSystem.out.println(cf + &quot;:&quot; + q + &quot; &quot; + value);\n&#125;\n&#125;\n&#125;\n@Test\n/**\n* 过滤出列名包含 q 的所有的 cell\n*\n*/\npublic void SubStringQualifierFilter() throws IOException &#123;\nSubstringComparator comparator = new\nSubstringComparator(&quot;q&quot;);\nQualifierFilter qualifierFilter = new\nQualifierFilter(CompareFilter.CompareOp.EQUAL, comparator);\nScan scan = new Scan();\nscan.setFilter(qualifierFilter);\nResultScanner scanner = student.getScanner(scan);\nfor (Result rs : scanner) &#123;\nfor (Cell cell : rs.listCells()) &#123;\nString cf =\nBytes.toString(CellUtil.cloneFamily(cell));\nString q =\nBytes.toString(CellUtil.cloneQualifier(cell));\nString value =\nBytes.toString(CellUtil.cloneValue(cell));\nSystem.out.println(cf + &quot;:&quot; + q + &quot; &quot; + value);\n&#125;\n&#125;\n&#125;\n@Test\n/**\n* 分页过滤器：PageFilter\n* 获取第四页的数据，每页 10 条\n*\n* 实际上需要遍历该页前面所有的数据，性能非常低\n*/\npublic void PageFilter1() throws IOException &#123;\nint page = 4;\nint pageSize = 10;\n// 首先先获取第 4 页的第一条数据的 rk\nint page_first = (page - 1) * pageSize + 1;\nPageFilter pageFilter1 = new PageFilter(page_first);\nScan scan = new Scan();\nscan.setFilter(pageFilter1);\n//\nscan.setLimit(40); // PageFilter 就相当于 setLimit\nString rowkey = null;\nResultScanner scanner = student.getScanner(scan);\nfor (Result rs : scanner) &#123;\nrowkey = Bytes.toString(rs.getRow());\n&#125;\nScan scan1 = new Scan();\nscan1.withStartRow(rowkey.getBytes());\nPageFilter pageFilter2 = new PageFilter(pageSize);\nscan1.setFilter(pageFilter2);\nResultScanner scanner2 = student.getScanner(scan1);\nfor (Result rs : scanner2) &#123;\nString id = Bytes.toString(rs.getRow());\nString name =\nBytes.toString(rs.getValue(&quot;info&quot;.getBytes(),\n&quot;name&quot;.getBytes()));\nString age =\nBytes.toString(rs.getValue(&quot;info&quot;.getBytes(),\n&quot;age&quot;.getBytes()));\nString gender =\nBytes.toString(rs.getValue(&quot;info&quot;.getBytes(),\n&quot;gender&quot;.getBytes()));\nString clazz =\nBytes.toString(rs.getValue(&quot;info&quot;.getBytes(),\n&quot;clazz&quot;.getBytes()));\nSystem.out.println(id + &quot;,&quot; + name + &quot;,&quot; + age + &quot;,&quot; +\ngender + &quot;,&quot; + clazz);\n&#125;\n&#125;\n@Test\n/**\n* 通过合理地设计 Rowkey 来实现分页的功能\n* 获取第五页的数据 每页还是 10 条\n*\n*/\npublic void PageWithRowkey() throws IOException &#123;\nint page = 5;\nint pageSize = 10;\nint baseId = 1500100000;\nint current_page_first_rk = baseId + (page - 1) * pageSize\n+ 1;\nScan scan = new Scan();\nscan.withStartRow((current_page_first_rk +\n&quot;&quot;).getBytes());\nscan.setLimit(pageSize);\nResultScanner scanner = student.getScanner(scan);\nfor (Result rs : scanner) &#123;\nString id = Bytes.toString(rs.getRow());\nString name =\nBytes.toString(rs.getValue(&quot;info&quot;.getBytes(),\n&quot;name&quot;.getBytes()));\nString age =\nBytes.toString(rs.getValue(&quot;info&quot;.getBytes(),\n&quot;age&quot;.getBytes()));\nString gender =\nBytes.toString(rs.getValue(&quot;info&quot;.getBytes(),\n&quot;gender&quot;.getBytes()));\nString clazz =\nBytes.toString(rs.getValue(&quot;info&quot;.getBytes(),\n&quot;clazz&quot;.getBytes()));\nSystem.out.println(id + &quot;,&quot; + name + &quot;,&quot; + age + &quot;,&quot; +\ngender + &quot;,&quot; + clazz);\n&#125;\n&#125;\n@Test\n/**\n* 过滤 gender 为男，age&gt;23，理科班的学生\n* 过条件过滤需要使用 FilterList\n*/\npublic void MultipleFilter() throws IOException &#123;\nSingleColumnValueFilter filter1 = new\nSingleColumnValueFilter(\n&quot;info&quot;.getBytes(),\n&quot;gender&quot;.getBytes(),\nCompareFilter.CompareOp.EQUAL,\n&quot;男&quot;.getBytes()\n);\nSingleColumnValueFilter filter2 = new\nSingleColumnValueFilter(\n&quot;info&quot;.getBytes(),\n&quot;age&quot;.getBytes(),\nCompareFilter.CompareOp.GREATER,\n&quot;23&quot;.getBytes()\n);\nSingleColumnValueFilter filter3 = new\nSingleColumnValueFilter(\n&quot;info&quot;.getBytes(),\n&quot;clazz&quot;.getBytes(),\nCompareFilter.CompareOp.EQUAL,\nnew BinaryPrefixComparator(&quot;理科&quot;.getBytes())\n);\nFilterList filterList = new FilterList();\nfilterList.addFilter(filter1);\nfilterList.addFilter(filter2);\nfilterList.addFilter(filter3);\nprintRSWithFilter(filterList);\n&#125;\n@After\npublic void close() throws IOException &#123;\nadmin.close();\nconn.close();\n&#125;\n&#125;第 6 章 Phoenix6.1 Phoenix 简介Hbase 适合存储大量的对关系运算要求低的 NOSQL 数据，受 Hbase 设计上的限制不能直接使用原生的API 执行在关系数据库中普遍使用的条件判断和聚合等操作。 Hbase 很优秀，一些团队寻求在Hbase 之上提供一种更面向普通开发人员的操作方式， Apache Phoenix 即是。\nPhoenix 基于 Hbase 给面向业务的开发人员提供了以标准 SQL 的方式对 Hbase 进行查询操作，并支持标准 SQL 中大部分特性 : 条件运算 , 分组，分页，等高级查询语法。\n6.2 Phoenix 搭建Phoenix 4.15 HBase 1.4.6 hadoop 2.7.6\n1 、关闭 hbase 集群，在 master 中执行\n\n\n\n\n\n\n\n\n\nstop-hbase.sh\n2 、上传解压配置环境变量\n\n\n\n\n\n\n\n\n\ntar -xvf apache-phoenix-4.15.0-HBase-1.4-bin.tar.gz -C &#x2F;usr&#x2F;local&#x2F;soft&#x2F;\nmv apache-phoenix-4.15.0-HBase-1.4-bin phoenix-4.15.0\n3 、将 phoenix-4.15.0-HBase-1.4-server.jar 复制到所有节点的 hbase lib 目录下\n\n\n\n\n\n\n\n\n\nscp &#x2F;usr&#x2F;local&#x2F;soft&#x2F;phoenix-4.15.0&#x2F;phoenix-4.15.0-HBase-1.4-server.jar master:&#x2F;usr&#x2F;local&#x2F;soft&#x2F;hbase-1.4.6&#x2F;lib&#x2F;\nscp &#x2F;usr&#x2F;local&#x2F;soft&#x2F;phoenix-4.15.0&#x2F;phoenix-4.15.0-HBase-1.4-server.jar node1:&#x2F;usr&#x2F;local&#x2F;soft&#x2F;hbase-1.4.6&#x2F;lib&#x2F;\nscp &#x2F;usr&#x2F;local&#x2F;soft&#x2F;phoenix-4.15.0&#x2F;phoenix-4.15.0-HBase-1.4-server.jar node2:&#x2F;usr&#x2F;local&#x2F;soft&#x2F;hbase-1.4.6&#x2F;lib&#x2F;\n4 、启动 hbase ， 在 master 中执行\n\n\n\n\n\n\n\n\n\nstart-hbase.sh\n5 、配置环境变量\n\n\n\n\n\n\n\n\n\nvim &#x2F;etc&#x2F;profile\nexport PHOENIX_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;soft&#x2F;hbase-1.4.6export PHOENIX_CLASSPATH&#x3D;$PHOENIX_HOMEexport PATH&#x3D;$PATH:$PHOENIX_HOME&#x2F;bin\n6.3 Phoenix 使用1. 连接 sqlline\n\n\n\n\n\n\n\n\n\nsqlline.py master,node1,node2\n# 出现\n163&#x2F;163 (100%) Done\nDone\nsqlline version 1.5.0\n0: jdbc:phoenix:master,node1,node2&gt;\n2 、常用命令\n# 1 、创建表\n\n\n\n\n\n\n\n\n\nCREATE TABLE IF NOT EXISTS STUDENT (\nid VARCHAR NOT NULL PRIMARY KEY,\nname VARCHAR,\nage BIGINT,\ngender VARCHAR ,\nclazz VARCHAR\n);\n# 2 、显示所有表\n\n\n\n\n\n\n\n\n\n!table\n# 3 、插入数据\n\n\n\n\n\n\n\n\n\nupsert into STUDENT values(‘1500100004’,’ 葛德曜 ‘,24,’ 男 ‘,’ 理科三班 ‘);\nupsert into STUDENT values(‘1500100005’,’ 宣谷芹 ‘,24,’ 男 ‘,’ 理科六班 ‘);\nupsert into STUDENT values(‘1500100006’,’ 羿彦昌 ‘,24,’ 女 ‘,’ 理科三班 ‘);\n# 4 、查询数据 , 支持大部分 sql 语法，\n\n\n\n\n\n\n\n\n\nselect * from STUDENT ;\nselect * from STUDENT where age&#x3D;24;\nselect gender ,count(*) from STUDENT group by gender;\nselect * from student order by gender;\n# 5 、删除数据\n\n\n\n\n\n\n\n\n\ndelete from STUDENT where id&#x3D;’1500100004’;\n# 6 、删除表\n\n\n\n\n\n\n\n\n\ndrop table STUDENT;\n# 7 、退出命令行\n\n\n\n\n\n\n\n\n\n!quit\n更多语法参照官网\nhttps://phoenix.apache.org/language/index.html#upsert\\_select\n3 、 phoenix 表映射\n默认情况下，直接在 hbase 中创建的表，通过 phoenix 是查看不到的\n如果需要在 phoenix 中操作直接在 hbase 中创建的表，则需要在 phoenix 中进行表的映射。映射方式有两种：视图映射和表映射\n视图映射\nPhoenix 创建的视图是只读的，所以只能用来做查询，无法通过视图对源数据进行修改等操作\n\n\n\n\n\n\n\n\n\n# hbase shell 进入 hbase 命令行\nhbase shell\n# 创建 hbase 表\ncreate ‘test’,’name’,’company’\n# 插入数据\nput ‘test’,’001’,’name:firstname’,’zhangsan1’\nput ‘test’,’001’,’name:lastname’,’zhangsan2’\nput ‘test’,’001’,’company:name’,’ 数加 ‘\nput ‘test’,’001’,’company:address’,’ 合肥 ‘\n# 在 phoenix 创建视图， primary key 对应到 hbase 中的 rowkey\ncreate view “test”(\nempid varchar primary key,\n“name”.”firstname” varchar,\n“name”.”lastname” varchar,\n“company”.”name” varchar,\n“company”.”address” varchar\n);\nCREATE view “student” (\nid VARCHAR NOT NULL PRIMARY KEY,\n“info”.”name” VARCHAR,\n“info”.”age” VARCHAR,\n“info”.”gender” VARCHAR ,\n“info”.”clazz” VARCHAR\n) column_encoded_bytes&#x3D;0;\n# 在 phoenix 查询数据，表名通过双引号引起来\nselect * from “test”;\n# 删除视图\ndrop view “test”;\n表映射\n使用 Apache Phoenix 创建对 HBase 的表映射，有两类：\n1 ）当 HBase 中已经存在表时，可以以类似创建视图的方式创建关联表，只需要将 create view 改为 create table 即可。\n2 ）当 HBase 中不存在表时，可以直接使用 create table 指令创建需要的表，并且在创建指令中可以根据需要对 HBase 表结构进行显示的说明。\n第 1 ）种情况下，如在之前的基础上已经存在了 test 表，则表映射的语句如下：\n\n\n\n\n\n\n\n\n\ncreate table “test” (\nempid varchar primary key,\n“name”.”firstname” varchar,\n“name”.”lastname”varchar,\n“company”.”name” varchar,\n“company”.”address” varchar\n)column_encoded_bytes&#x3D;0;\nupsert into “test” values(‘1’,’2’,’3’,’4’,’5’);\nCREATE table “student” (\nid VARCHAR NOT NULL PRIMARY KEY,\n“info”.”name” VARCHAR,\n“info”.”age” VARCHAR,\n“info”.”gender” VARCHAR ,\n“info”.”clazz” VARCHAR\n) column_encoded_bytes&#x3D;0;\nupsert into “student” values(‘1500110004’,’ 葛德曜 ‘,’24’,’n ü’,’ 理科三班 ‘);\n使用 create table 创建的关联表，如果对表进行了修改，源数据也会改变，同时如果关联表被删除，源表也会被删除。但是视图就不会，如果删除视图，源数据不会发生改变。\n6.4 Phoenix 二级索引对于 Hbase ，如果想精确定位到某行记录，唯一的办法就是通过 rowkey 查询。如果不通过 rowkey 查找数据，就必须逐行比较每一行的值，对于较大的表，全表扫描的代价是不可接受的。\n1. 开启索引支持\n# 关闭 hbase 集群\n\n\n\n\n\n\n\n\n\nstop-hbase.sh\n# 在 &#x2F;usr&#x2F;local&#x2F;soft&#x2F;hbase-1.4.6&#x2F;conf&#x2F;hbase-site.xml 中增加如下配置\n\n\n\n\n\n\n\n\n\n&lt;property&gt;\n&lt;name&gt;hbase.regionserver.wal.codec&lt;&#x2F;name&gt;\n&lt;value&gt;org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;hbase.rpc.timeout&lt;&#x2F;name&gt;\n&lt;value&gt;60000000&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;hbase.client.scanner.timeout.period&lt;&#x2F;name&gt;\n&lt;value&gt;60000000&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;phoenix.query.timeoutMs&lt;&#x2F;name&gt;\n&lt;value&gt;60000000&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n# 同步到所有节点\nscp hbase-site.xml node1:`pwd`\nscp hbase-site.xml node2:`pwd`\n# 修改 phoenix 目录下的 bin 目录中的 hbase-site.xml\n&lt;property&gt;\n&lt;name&gt;hbase.rpc.timeout&lt;&#x2F;name&gt;\n&lt;value&gt;60000000&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;hbase.client.scanner.timeout.period&lt;&#x2F;name&gt;\n&lt;value&gt;60000000&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;phoenix.query.timeoutMs&lt;&#x2F;name&gt;\n&lt;value&gt;60000000&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n# 启动 hbase\n\n\n\n\n\n\n\n\n\nstart-hbase.sh\n# 重新进入 phoenix 客户端\n\n\n\n\n\n\n\n\n\nsqlline.sql master,node1,node2\n2 、创建索引\n（ 1 ）全局索引\n全局索引适合读多写少的场景。如果使用全局索引，读数据基本不损耗性能，所有的性能损耗都来源于写数据。数据表的添加、删除和修改都会更新相关的索引表（数据删除了，索引表中的数据也会删除；数据增加了，索引表的数据也会增加）\n注意 : 对于全局索引在默认情况下，在查询语句中检索的列如果不在索引表中， Phoenix不会使用索引表将，除非使用 hint 。\n\n\n\n\n\n\n\n\n\n# 创建 DIANXIN.sql\nCREATE TABLE IF NOT EXISTS DIANXIN (\nmdn VARCHAR ,\nstart_date VARCHAR ,\nend_date VARCHAR ,\ncounty VARCHAR,\nx DOUBLE ,\ny DOUBLE,\nbsid VARCHAR,\ngrid_id VARCHAR,\nbiz_type VARCHAR,\nevent_type VARCHAR ,\ndata_source VARCHAR ,\nCONSTRAINT PK PRIMARY KEY (mdn,start_date)\n) column_encoded_bytes&#x3D;0;\n# 上传数据 DIANXIN.csv\n# 导入数据\npsql.py master,node1,node2 DIANXIN.sql DIANXIN.csv\n# 创建全局索引\nCREATE INDEX DIANXIN_INDEX ON DIANXIN ( end_date );\n# 查询数据 ( 索引未生效 )\nselect * from DIANXIN where end_date &#x3D; ‘20180503154014’;\n# 强制使用索引 （索引生效） hint\nselect &#x2F;*+ INDEX(DIANXIN DIANXIN_INDEX) *&#x2F; * from DIANXIN where end_date &#x3D; ‘20180503154014’;\nselect &#x2F;*+ INDEX(DIANXIN DIANXIN_INDEX) *&#x2F; * from DIANXIN where end_date &#x3D; ‘20180503154014’ and start_date &#x3D; ‘20180503154614’;\n# 取索引列，（索引生效）\nselect end_date from DIANXIN where end_date &#x3D; ‘20180503154014’;\n# 创建多列索引\nCREATE INDEX DIANXIN_INDEX1 ON DIANXIN ( end_date,COUNTY );\n# 多条件查询 （索引生效）\nselect end_date,MDN,COUNTY from DIANXIN where end_date &#x3D; ‘20180503154014’ and COUNTY &#x3D; ‘8340104’;\n# 查询所有列 ( 索引未生效 )\nselect * from DIANXIN where end_date &#x3D; ‘20180503154014’ and COUNTY &#x3D; ‘8340104’;\n# 查询所有列 （索引生效）\nselect &#x2F;*+ INDEX(DIANXIN DIANXIN_INDEX1) *&#x2F; * from DIANXIN where end_date &#x3D; ‘20180503154014’ and COUNTY &#x3D; ‘8340104’;\n# 单条件 ( 索引未生效 )\nselect end_date from DIANXIN where COUNTY &#x3D; ‘8340103’;\n# 单条件 ( 索引生效 ) end_date 在前\nselect COUNTY from DIANXIN where end_date &#x3D; ‘20180503154014’;\n# 删除索引\ndrop index DIANXIN_INDEX on DIANXIN;\n（ 2 ）本地索引\n本地索引适合写多读少的场景，或者存储空间有限的场景。和全局索引一样， Phoenix也会在查询的时候自动选择是否使用本地索引。本地索引因为索引数据和原数据存储在同一台机器上，避免网络数据传输的开销，所以更适合写多的场景。由于无法提前确定数据在哪个 Region 上，所以在读数据的时候，需要检查每个 Region 上的数据从而带来一些性能损耗。\n注意 : 对于本地索引，查询中无论是否指定 hint 或者是查询的列是否都在索引表中，都会使用索引表。\n# 创建本地索引\n\n\n\n\n\n\n\n\n\nCREATE LOCAL INDEX DIANXIN_LOCAL_IDEX ON DIANXIN(grid_id);\n# 索引生效\n\n\n\n\n\n\n\n\n\nselect grid_id from dianxin where grid_id&#x3D;’117285031820040’;\n# 索引生效\n\n\n\n\n\n\n\n\n\nselect * from dianxin where grid_id&#x3D;’117285031820040’;\n（ 3 ）覆盖索引\n覆盖索引是把原数据存储在索引数据表中，这样在查询时不需要再去 HBase 的原表获取数据就直接返回查询结果。\n注意：查询是 select 的列和 where 的列都需要在索引中出现。\n\n\n\n\n\n\n\n\n\n# 创建覆盖索引\nCREATE INDEX DIANXIN_INDEX_COVER ON DIANXIN ( x,y ) INCLUDE ( county );\n# 查询所有列 ( 索引未生效 )\nselect * from DIANXIN where x&#x3D;117.288 and y &#x3D;31.822;\n# 强制使用索引 ( 索引生效 )\nselect &#x2F;*+ INDEX(DIANXIN DIANXIN_INDEX_COVER) *&#x2F; * from DIANXIN where x&#x3D;117.288 and y &#x3D;31.822;\n# 查询索引中的列 ( 索引生效 ) mdn 是 DIANXIN 表的 RowKey 中的一部分\nselect x,y,county from DIANXIN where x&#x3D;117.288 and y &#x3D;31.822;\nselect mdn,x,y,county from DIANXIN where x&#x3D;117.288 and y &#x3D;31.822;\n# 查询条件必须放在索引中 select 中的列可以放在 INCLUDE （将数据保存在索引中）\nselect &#x2F;*+ INDEX(DIANXIN DIANXIN_INDEX_COVER) *&#x2F; x,y,count(*) from DIANXIN group by x,y;\n6.5 Phoenix JDBC# 导入依赖\n\n\n\n\n\n\n\n\n\n&lt;dependency&gt;\n&lt;groupId&gt;org.apache.phoenix&lt;&#x2F;groupId&gt;\n&lt;artifactId&gt;phoenix-core&lt;&#x2F;artifactId&gt;\n&lt;version&gt;4.15.0-HBase-1.4&lt;&#x2F;version&gt;\n&lt;&#x2F;dependency&gt;\njavaConnection conn = DriverManager.getConnection(&quot;jdbc:phoenix:master,node1,node2:2181&quot;);\nPreparedStatement ps = conn.prepareStatement(&quot;select /*+ INDEX(DIANXIN DIANXIN_INDEX) */ * from DIANXIN where end_date=?&quot;);\nps.setString(1, &quot;20180503212649&quot;);\nResultSet rs = ps.executeQuery();\nwhile (rs.next()) &#123;\nString mdn = rs.getString(&quot;mdn&quot;);\nString start_date = rs.getString(&quot;start_date&quot;);\nString end_date = rs.getString(&quot;end_date&quot;);\nString x = rs.getString(&quot;x&quot;);\nString y = rs.getString(&quot;y&quot;);\nString county = rs.getString(&quot;county&quot;);\nSystem.out.println(mdn + &quot;\\t&quot; + start_date + &quot;\\t&quot; + end_date + &quot;\\t&quot; + x + &quot;\\t&quot; + y + &quot;\\t&quot; + county);\n&#125;\nps.close();\nconn.close();6.6 Phoenix 调优学习链接： https://blog.csdn.net/jy02268879/article/details/81396026\n1 、建立索引超时，查询超时\n修改配置文件， hbase-site.xml\n两个位置\n&#x2F;usr&#x2F;local&#x2F;soft&#x2F;phoenix-4.15.0&#x2F;bin\n&#x2F;usr&#x2F;local&#x2F;soft&#x2F;hbase-1.4.6&#x2F;conf&#x2F; 所有节点\n增加配置\n\n\n\n\n\n\n\n\n\n&lt;property&gt;\n&lt;name&gt;hbase.rpc.timeout&lt;&#x2F;name&gt;\n&lt;value&gt;60000000&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;hbase.client.scanner.timeout.period&lt;&#x2F;name&gt;\n&lt;value&gt;60000000&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n&lt;name&gt;phoenix.query.timeoutMs&lt;&#x2F;name&gt;\n&lt;value&gt;60000000&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n需要重启 hbase\n2 、预分区\n\n\n\n\n\n\n\n\n\nCREATE TABLE IF NOT EXISTS STUDENT2 (\nid VARCHAR NOT NULL PRIMARY KEY,\nname VARCHAR,\nage BIGINT,\ngender VARCHAR ,\nclazz VARCHAR\n)split on(‘15001006|’,’15001007|’,’15001008|’) ;\n3 、在创建表的时候指定 salting 。会再 rowkey 前面加上一个随机的前缀，\n优点：不需要知道 rowkey 的分步情况\n缺点：不能再 hbase 中对数据进行查询和修改\n\n\n\n\n\n\n\n\n\nCREATE TABLE IF NOT EXISTS STUDENT3 (\nid VARCHAR NOT NULL PRIMARY KEY,\nname VARCHAR,\nage BIGINT,\ngender VARCHAR ,\nclazz VARCHAR\n)salt_buckets&#x3D;6;\nupsert into STUDENT3 values(‘1500100004’,’ 葛德曜 ‘,24,’ 男 ‘,’ 理科三班 ‘);\nupsert into STUDENT3 values(‘1500100005’,’ 宣谷芹 ‘,24,’ 男 ‘,’ 理科六班 ‘);\nupsert into STUDENT3 values(‘1500100006’,’ 羿彦昌 ‘,24,’ 女 ‘,’ 理科三班 ‘);\n4 、二级索引\n建立行键与列值的映射关系\n全局索引：读多写少， 会单独建立索引表\n本地索引：读少写多， 索引数据和原数据保存在同一台机器上\n\n全局索引，本地索引不同点 和 比较\n直白话：全局索引是表，适合重读轻写的场景 ， 本地索引是列族，适合重写轻读的场景\n\n索引数据\n\nglobal index 单独把索引数据存到一张表里，保证了原始数据的安全，侵入性小\nlocal index 把数据写到原始数据里面，侵入性强，原表的数据量 = 原始数据 + 索引数据，使原始数据更大\n\n性能方面\n\nglobal index 要多写出来一份数据，写的压力就大一点，但读的速度就非常快\nlocal index 只用写一份索引数据，节省不少空间，但多了一步通过 rowkey 查找数据，写的速度非常快，读的速度就没有直接取自己的列族数据快。\n第 7 章 Rowkey 的设计HBase 是三维有序存储的，通过 rowkey （行键）， column key （column family 和 qualifier ）和 TimeStamp （时间戳）这个三个维度可以对 HBase 中的数据进行快速定位。 HBase 中 rowkey 可以唯一标识一行记录，在 HBase 查询的时候，有三种方式：\n\n通过 get 方式，指定 rowkey 获取唯一一条记录\n\n通过 scan 方式，设置 startRow 和 stopRow 参数进行范围匹配\n\n全表扫描，即直接扫描整张表中所有行记录\n\n\nrowkey 长度原则\nrowkey 是一个二进制码流，可以是任意字符串，最大长度 64kb ，实际应用中一般为10-100bytes，以 byte[] 形式保存，一般设计成定长。\n建议越短越好，不要超过 16 个字节，原因如下：\n数据的持久化文件 HFile 中是按照 KeyValue 存储的，如果 rowkey 过长，比如超过 100 字节， 1000w 行数据，光 rowkey 就要占用 100*1000w&#x3D;10 亿个字节，将近 1G 数据，这样会极大影响 HFile 的存储效率；\nMemStore 将缓存部分数据到内存，如果 rowkey 字段过长，内存的有效利用率就会降低，系统不能缓存更多的数据，这样会降低检索效率。\nrowkey 散列原则\n如果 rowkey 按照时间戳的方式递增，不要将时间放在二进制码的前面，建议将 rowkey 的高位作为散列字段，由程序随机生成，低位放时间字段，这样将提高数据均衡分布在每个RegionServer ，\n以实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息，所有的数据都会集中在一个RegionServer 上，这样在数据检索的时候负载会集中在个别的 RegionServer 上，造成热点问题，会降低查询效率。\nrowkey 唯一原则\n必须在设计上保证其唯一性， rowkey 是按照字典顺序排序存储的，因此，设计 rowkey 的时候， 要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到块。\n热点问题\nHBase 中的行是按照 rowkey 的字典顺序排序的，这种设计优化了 scan 操作，可以将相关的行以及会被一起读取的行存取在临近位置，便于 scan 。然而糟糕的 rowkey 设计是热点的源头。 热点 发生在大量的 client 直接访问集群的一个或极少数个节点（访问可能是读，写或者其他操作）。大量 访问会使热点 region 所在的单个机器超出自身承受能力，引起性能下降甚至 region 不可用，这也会 影响同一个 RegionServer 上的其他 region ，由于主机无法服务其他 region 的请求。 设计良好的数据访问模式以使集群被充分，均衡的利用。\n为了避免写热点，设计 rowkey 使得不同行在同一个 region ，但是在更多数据情况下，数据应该被写入集群的多个 region ，而不是一个。\n下面是一些常见的避免热点的方法以及它们的优缺点：\n加盐\n这里所说的加盐不是密码学中的加盐，而是在 rowkey 的前面增加随机数，具体就是给 rowkey分配一个随机前缀以使得它和之前的 rowkey 的开头不同。分配的前缀种类数量应该和你想使用数据 分散到不同的 region 的数量一致。加盐之后的 rowkey 就会根据随机生成的前缀分散到各个 region 上，以避免热点。\n哈希\n哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的 rowkey ，可以使用 get 操作准确获取某一个行数据\n反转\n第三种防止热点的方法时反转固定长度或者数字格式的 rowkey 。这样可以使得 rowkey 中经常改变的部分（最没有意义的部分）放在前面。这样可以有效的随机 rowkey ，但是牺牲了 rowkey 的 有序性。\n反转 rowkey 的例子以手机号为 rowkey ，可以将手机号反转后的字符串作为 rowkey ，这样的就避免了以手机号那样比较固定开头导致热点问题\n时间戳反转\n一个常见的数据处理问题是快速获取数据的最近版本，使用反转的时间戳作为 rowkey 的一部分对这个问题十分有用，可以用 Long.Max_Value - timestamp 追加到 key 的末尾，例如[key][reverse_timestamp] , [key] 的最新值可以通过 scan [key] 获得 [key] 的第一条记录，因为HBase 中 rowkey 是有序的，第一条记录是最后录入的数据。比如需要保存一个用户的操作记录，按照操作时间倒序排序，在设计 rowkey 的时候，可以这样设计[userId 反转 ][Long.Max_Value - timestamp] ，在查询用户的所有操作记录数据的时候，直接指定反转后的 userId ， startRow 是 [userId 反转 ][000000000000],stopRow 是 [userId 反转][Long.Max_Value - timestamp] 如果需要查询某段时间的操作记录，startRow 是 [user 反转 ][Long.Max_Value - 起始时间 ] ，stopRow 是 [userId 反转 ][Long.Max_Value - 结束时间 ]\n其他一些建议\n尽量减少 rowkey 和列的大小，当具体的值在系统间传输时，它的 rowkey ，列簇、列名，时间戳也会一起传输。如果你的 rowkey 、列簇名、列名很大，甚至可以和具体的值相比较，那么将会造成大量的冗余，不利于数据的储存与传输列族尽可能越短越好，最好是一个字符列名也尽可能越短越好，冗长的列名虽然可读性好，但是更短的列名存储在 HBase 中会更好\n第 8 章 其他8.1 version 和 ttlhbase 可以保存多个版本的数据\n\n\n\n\n\n\n\n\n\ncreate ‘User’,{NAME &#x3D;&gt; ‘info’,VERSIONS &#x3D;&gt; 5}\nput ‘User’,’row1’,’info:age’,’21’\nput ‘User’,’row1’,’info:age’,’22’\nput ‘User’,’row1’,’info:age’,’23’\nput ‘User’,’row1’,’info:age’,’24’\nput ‘User’,’row1’,’info:age’,’25’\nput ‘User’,’row1’,’info:age’,’26’\n保存最新 5 个版本的数据\n指定查询多个版本的数据\n\n\n\n\n\n\n\n\n\nscan ‘User’ ,{VERSIONS &#x3D;&gt; 5}\nget ‘User’,’row1’, {COLUMN &#x3D;&gt; ‘info:age’, VERSIONS &#x3D;&gt; 5}\nhbsae 提供了 ttl 过期时间\ncreate ‘User1’,{NAME &#x3D;&gt; ‘info’,TTL &#x3D;&gt; 5}\nput ‘User1’,’row1’,’info:age’,’21’\n5 秒之后自动删除\n8.2 自增和追加put 插入数据\nget 获取数据\n自增\n\n\n\n\n\n\n\n\n\ncreate ‘count’,’info’\n默认增加 1\n写负数就是自减\n\n\n\n\n\n\n\n\n\nincr ‘count’,’row1’,’info:age’,10\n获取值\n\n\n\n\n\n\n\n\n\nget_counter ‘count’,’row1’,’info:age’\n追加\n\n\n\n\n\n\n\n\n\ncreate ‘score’,’info’\nput ‘score’,’row1’,’info:scores’,’100’\n在列值上追加新的内容\n\n\n\n\n\n\n\n\n\nappend ‘score’,’row1’,’info:scores’,’,80’\n8.3 新的数据排在前面的方法001_20180503\n001_20200731\n001_20200801\n001_20200802\n默认 rowkey 是字典升序\n索引新的数据会被排在后面\n怎么让新的数据排在前面来？\n大数减小数\nLong.MAX_VALUE\n30000000 - 20200802\n时间越大减完之后就越小，就会排在前面\n001_9799198\n001_9799199\n001_9799200\n查询 20200801 的数据\nArticle link： https://tqgoblin.site/post/csdn/HBase详解/  Author： Stephen  \n","slug":"csdn/HBase详解","date":"2021-02-12T12:54:44.000Z","categories_index":"大数据","tags_index":"hbase","author_index":"Stephen"},{"id":"2ea5b3e3cf982dd223319a88f2e53a29","title":"Hive详解","content":"第1章** Hive基本概念******1.1 什么是Hive1）hive简介\nHive：由 Facebook 开源用于解决海量结构化日志的数据统计工具。\nHive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件 映射为一张表，并提供类SQL 查询功能。\n\nHive本质：将HQL转化为Mapreduce程序\n\n \n（1）Hive 处理的数据存储在 HDFS\n（2）Hive 分析数据底层的实现是 MapReduce\n（3）执行程序运行在 Yarn 上\n1.2 Hive的优缺点1.2.1 优点（1）操作接口采用类 SQL 语法，提供快速开发的能力（简单、容易上手）。\n（2）避免了去写 MapReduce，减少开发人员的学习成本。\n（3）Hive 的执行延迟比较高，因此 Hive 常用于数据分析，对实时性要求  不高的场合。\n（4）Hive 优势在于处理大数据，对于处理小数据没有优势，因为 Hive 的执行延迟比较高。\n（5）Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函  数。\n1.2.2 缺点1）Hive 的 HQL 表达能力有限\n（1）迭代式算法无法表达\n（2）数据挖掘方面不擅长，由于MapReduce数据处理流程的限制，效率更\n高的算法却无法实现。\n2）Hive 的效率比较低\n（1）Hive 自动生成的 MapReduce 作业，通常情况下不够智能化\n（2）Hive 调优比较困难，粒度较粗\n1.3 Hive架构原理\n1）用户接口：****Client\nCLI（command-line interface）、JDBC&#x2F;ODBC(jdbc 访问 hive)、WEBUI（浏览器访问 hive）\n2）元数据：Metastore\n元数据包括：表名、表所属的数据库（默认是 default）、表的拥有者、列&#x2F;分区字段、表的类型（是否是外部表）、表的数据所在目录等；\n默认存储在自带的 derby 数据库中，推荐使用 MySQL 存储 Metastore\n3）Hadoop\n使用 HDFS 进行存储，使用 MapReduce 进行计算。 \n4）驱动器：Driver\n（1）解析器（SQL Parser）：将 SQL 字符串转换成抽象语法树 AST，这一步一般都用第 三方工具库完成，比如 antlr；对 AST 进行语法分析，比如表是否存在、字段是否存在、SQL 语义是否有误。\n（2）编译器（Physical Plan）：将 AST 编译生成逻辑执行计划。\n（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。\n（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是 MR&#x2F;Spark。\n\nHive 通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的 Driver，结合元数据(MetaStore)，将这些指令翻译成 MapReduce，提交到 Hadoop 中执行，最后，将执行返回的结果输出到用户交互接口。\n1.4 Hive和数据库比较由于 Hive 采用了类似 SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。\n本文将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是 Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。\n1.4.1 查询语言由于 SQL 被广泛的应用在数据仓库中，因此，专门针对 Hive 的特性设计了类 SQL 的查询语言 HQL。熟悉 SQL 开发的开发者可以很方便的使用 Hive 进行开发。\n1.4.2 数据更新由于 Hive 是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive 中不建议对数据的改写，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO … VALUES 添加数据，使用 UPDATE … SET 修 改数据。\n1.4.3 执行延迟Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导 致 Hive 执行延迟高的因素是 MapReduce 框架。由于 MapReduce 本身具有较高的延迟，因此 在利用 MapReduce 执行 Hive 查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。 当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive 的并行计算显然能体现出优势。\n1.4.4 数据规模由于 Hive 建立在集群上并可以利用 MapReduce 进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。\n第2章 Hive安装2.1 Hive的安装地址1.Hive官网地址：\nhttp://hive.apache.org/\n2.文档查看地址：https://cwiki.apache.org/confluence/display/Hive/GettingStarted\n3.下载地址：\n    http://archive.apache.org/dist/hive/\n4.github 地址\nhttps://github.com/apache/hive\n2.2 Hive安装部署1.Hive安装以及配置\n（1）把apache-hive-1.2.1-bin.tar.gz上传到linux的&#x2F;usr&#x2F;local&#x2F;packages目录下\n（2）解压 apache-hive-1.2.1-bin.tar.gz 到&#x2F;usr&#x2F;local&#x2F;soft&#x2F;目录下面\n[root@master packages]# tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /usr/local/soft/\n\n（3）修改apache-hive-1.2.1-bin.tar.gz 的名称为 hive\n[root@master soft]# mv apache-hive-1.2.1-bin/ hive\n\n（4）修改&#x2F;usr&#x2F;local&#x2F;soft&#x2F;hive&#x2F;conf 目录下的hive-env.sh.template 名称为 hive-env.sh\n[root@master conf]# mv hive-env.sh.template hive-env.sh\n\n（5）配置hive-env.sh 文件\n（a）配置HADOOP_HOME 路径\nexport HADOOP_HOME=/usr/local/soft/hadoop-2.7.6\n\n（b）配置HIVE_CONF_DIR 路径\nexport HIVE_CONF_DIR=/usr/local/soft/hive/conf\n\n（c）配置JAVA_HOM 路径\nexport JAVA_HOME=/usr/local/soft/jdk1.8.0_171\n\n（6）配置hive的环境变量\nexport HIVE_HOME=/usr/local/soft/hiveexportPATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:$PATH\n\n\n\nHive基本启动\n\n（1）启动Hive\n[root@master conf]# hive\n\n\n（2）查看数据库\nhive&gt; show databases;\n\n\n（3）打开默认数据库 \nhive&gt; use default;\n\n（4）显示 default 数据库中的表\nhive&gt; show tables;\n\n\n（5）创建一张表\nhive&gt; create table student(id int,name string,gender string);\n\n\n（6）查看表的结构\nhive&gt; desc student;\n\n\n（7）向表中插入数据\nhive&gt; insert into student values(2012003,\"lyj\",\"0\");\n\n（8）查询表中数据\nhive&gt; select * from student;\n\n（9）退出 hive\nhive&gt; quit;\n\n说明：（查看 hive 在 hdfs 中的结构）\n数据库：在 hdfs 中表现为${hive.metastore.warehouse.dir}目录下一个文件夹\n表：在 hdfs 中表现所属 db 目录下一个文件夹，文件夹中存放该表中的具体数据\n2.3 将本地文件导入Hive案例将本地&#x2F;usr&#x2F;local&#x2F;soft&#x2F;data&#x2F;student.txt 这个目录下的数据导入到 hive 的 student(id int, name string,gender string)表中。\n\n数据准备\n\n在&#x2F;usr&#x2F;local&#x2F;soft&#x2F;data&#x2F;这个目录下准备数据\n（1）在&#x2F;usr&#x2F;local&#x2F;soft&#x2F;目录下创建 data\n[root@master soft]# mkdir data\n\n（2）在&#x2F;usr&#x2F;local&#x2F;soft&#x2F;data目录下创建 student.txt 文件并添加数据\n[root@master data]# touch student.txt[root@master data]# vi student.txt2012003,lyj,02121001,lzh,12121002,wkh,12121003,jy ,02121004,syx,0\n\n注意：以 “,” 键间隔。\n\nHive实际操作\n\n（1）启动 hive\n[root@master data]# hive\n\n（2）显示数据库\nhive&gt; show databases;\n\n（3）使用 default 数据库\nhive&gt; use default;\n\n（4）显示 default 数据库中的表\nhive&gt; show tables;\n\n（5）删除已创建的 student 表\nhive&gt; drop table student;\n\n注意：没有表就忽略。\n（6）创建 student 表, 并声明文件分隔符’，’\ncreate table student(id int,name string)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n\n（7）加载&#x2F;usr&#x2F;local&#x2F;soft&#x2F;data&#x2F;student.txt 文件到 student 数据库表中。\nhive&gt; load data local inpath '/usr/local/soft/data/student.txt' into table student;\n\n\n（8）Hive 查询结果\nhive&gt; select * from student limit 10;OK2012003 lyj2121001 lzh2121002 wkh2121003 jy2121004 syx2121005 ntt2121006 gyl2121007 hcr2121008 xhy\n\n\n遇到的问题\n\n再打开一个客户端窗口启动 hive，会产生 java.sql.SQLException 异常。\n[root@master data]# hiveLogging initialized using configuration injar:file:/usr/local/soft/hive/lib/hive-common-1.2.1.jar!/hive-log4j.propertieException in thread \"main\" java.lang.RuntimeException:java.lang.RuntimeException: Unable to instantiateorg.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClientatorg.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)atsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)atsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)at java.lang.reflect.Method.invoke(Method.java:498)at org.apache.hadoop.util.RunJar.run(RunJar.java:221)at org.apache.hadoop.util.RunJar.main(RunJar.java:136)Caused by: java.lang.RuntimeException: Unable to instantiateorg.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClientatorg.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:86)atorg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)atorg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)atorg.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)atorg.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)... 8 more\n\n原因是，Metastore 默认存储在自带的 derby 数据库中，推荐使用 MySQL 存储 Metastore;\n2.4 MySql安装1.安装mysql5.7\n（1）下载yum Repository\n[root@master packages]# wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm\n\n（2）安装yum Repository\n[root@master packages]# yum -y install mysql57-community-release-el7- 10.noarch.rpm\n\n（3）安装mysql5.7\n[root@master packages]# yum -y install mysql-community-server --nogpgcheck\n\n\n（4）开机自启动\n[root@master packages]# systemctl enable mysqld.service\n\n（5）启动mysql\n[root@master packages]# systemctl start mysqld.service\n\n（6）查看状态\n[root@master packages]# systemctl status mysqld.service\n\n\n（7）获取临时密码\n[root@master packages]#grep \"password\" /var/log/mysqld.log\n\n\n（8）登录mysql\n[root@master packages]#mysql -uroot -p\n\n（9）关闭密码复杂验证\nset global validate_password_policy=0;set global validate_password_length=1;\n\n（10）设置密码\nalter user user() identified by \"123456\";\n\n（11）修改权限\nuse mysql;GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION; --修改权限flush privileges; --刷新权限select host,user,authentication_string from user; --查看权限\n\n（12）卸载yum Repository\n#因为安装了Yum Repository，以后每次yum操作都会自动更新，需要把这个卸载掉：yum -y remove mysql57-community-release-el7-10.noarch\n\n2. 卸载mysql\n删除依赖包rpm -qa |grep -i mysqlyum remove mysql-community mysql-community-server mysql-community-libs mysql-community-common清理文件find / -name mysql\n\n3.修改MySQL编码\n(1) 修改mysql编码为UTF-8\n1、编辑配置文件vim /etc/my.cnf2、加入以下内容：[client]default-character-set = utf8mb4[mysqld]character-set-server = utf8mb4collation-server = utf8mb4_general_ci3、重启mysqlsystemctl restart mysqld4、登录mysqlmysql -uroot -p1234565、查看mysql当前字符集show variables like '%char%';\n\n2.5 Hive 元数据配置到** MySql******2.5.1 驱动拷贝1．上传 mysql-connector-java-5.1.27.tar.gz 驱动包至&#x2F;usr&#x2F;local&#x2F;packages\n2.拷贝&#x2F;usr&#x2F;local&#x2F;packages目录下的 mysql-connector-java-5.1.49.jar 到&#x2F;usr&#x2F;local&#x2F;soft&#x2F;hive&#x2F;lib&#x2F;\n[root@master packages]# cp mysql-connector-java-5.1.49.jar/usr/local/soft/hive/lib/\n\n2.5.2 配置** Metastore 到 MySql******1．在&#x2F;usr&#x2F;local&#x2F;soft&#x2F;hive&#x2F;conf目录下创建一个名为hive-site.xml的文件并配置\n[root@master conf]# vim hive-site.xml&lt;?xml version=\"1.0\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;&lt;value&gt;jdbc:mysql://master:3306/hive?useSSL=false&lt;/value&gt;&lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;&lt;value&gt;123456&lt;/value&gt;&lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt;\n\n2．将hive的jline-2.12.jar拷贝到hadoop对应目录下，hive的 jline-2.12.jar 位置在 ：&#x2F;usr&#x2F;local&#x2F;soft&#x2F;hive&#x2F;lib&#x2F;jline-2.12.jar\n[root@master conf]# cd /usr/local/soft/hive/lib/[root@master lib]# cp jline-2.12.jar /usr/local/soft/hadoop-2.7.6/share/hadoop/yarn/lib/\n\n3．配置完毕后，如果启动 hive 异常，可以重新启动虚拟机。（重启后，别忘了启 动 hadoop 集群）\n2.5.3 多窗口启动** Hive 测试******1．先启动 MySQL\n[root@master conf]# mysql -uroot -p123456\n\n查看有几个数据库\nmysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys |+--------------------+4 rows in set (0.00 sec)mysql&gt;\n\n2．再次打开多个窗口，分别启动 hive\n[root@master conf]# hive\n\n3．启动 hive 后，回到 MySQL 窗口查看数据库，显示增加了 hive 数据库\nmysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || hive || mysql || performance_schema || sys |+--------------------+5 rows in set (0.00 sec)mysql&gt;\n\n4.修改mysql元数据库hive，让其hive支持utf-8编码以支持中文\n（1）切换到hive数据库\nuse hive;\n\n（2）修改字段注释字符集\nalter table COLUMNS_V2 modify column COMMENT varchar(256) character setutf8;\n\n（3）修改表注释字符集\nalter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;\n\n（4）修改分区表参数，以支持分区键能够用中文表示\nalter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000)character set utf8;alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000)character set utf8;\n\n（5）修改索引注解(可选)\nalter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;\n\n2.6 HiveJDBC访问2.6.1 启动 hiveserver2 服务[root@master conf]# hiveserver2\n\n2.6.2 启动 beeline[root@master conf]# beelineBeeline version 1.2.1 by Apache Hivebeeline&gt;\n\n2.6.3 连接** hiveserver2******[root@master conf]# beelineBeeline version 1.2.1 by Apache Hivebeeline&gt; !connect jdbc:hive2://master:10000Connecting to jdbc:hive2://master:10000Enter username for jdbc:hive2://master:10000: rootEnter password for jdbc:hive2://master:10000:Connected to: Apache Hive (version 1.2.1)Driver: Hive JDBC (version 1.2.1)Transaction isolation: TRANSACTION_REPEATABLE_READ0: jdbc:hive2://master:10000&gt;\n\n2.7 Hive常用交互命令[root@master conf]# hive -helpusage: hive-d,--define &lt;key=value&gt; Variable subsitution to apply to hivecommands. e.g. -d A=B or --define A=B--database &lt;databasename&gt; Specify the database to use-e &lt;quoted-query-string&gt; SQL from command line-f &lt;filename&gt; SQL from files-H,--help Print help information--hiveconf &lt;property=value&gt; Use value for given property--hivevar &lt;key=value&gt; Variable subsitution to apply to hive-i &lt;filename&gt; Initialization SQL file-S,--silent Silent mode in interactive shell-v,--verbose Verbose mode (echo executed SQL to theconsole)[root@master conf]#\n\n\n“-e”不进入 hive 的交互窗口执行 sql 语句\n\n[root@master conf]# hive -e \"select id from student;\"\n\n\n“-f”执行脚本中 sql 语句\n\n（1）在&#x2F;usr&#x2F;local&#x2F;soft&#x2F;data 目录下创建 hivef.sql 文件\n[root@master data]# touch hivef.sql文件中写入正确的 sql 语句[root@master data]# vi hivef.sqlselect * from student;\n\n（2）执行文件中的 sql 语句并将结果写入文件中\n[root@master data]# hive -f hivef.sql &gt;/usr/local/soft/data/hive_result.txt\n\n2.8 Hive其他命令操作1．退出 hive 窗口：\nhive&gt; quit;hive&gt; exit;\n\n在新版的 hive 中没区别了，在以前的版本是有的： exit:先隐性提交数据，再退出； quit:不提交数据，退出； \n2．在 hive cli 命令窗口中如何查看 hdfs 文件系统\nhive&gt; dfs -ls /;\n\n3．在 hive cli 命令窗口中如何查看本地文件系统\nhive&gt; ! ls /usr/local/soft/data;\n\n4．查看在 hive 中输入的所有历史命令\n[root@master ~]# cat .hivehistory\n\n2.9 Hive常见属性配置2.9.1 Hive 数据仓库位置配置1）Default 数据仓库的最原始位置是在 hdfs 上的：&#x2F;user&#x2F;hive&#x2F;warehouse 路径下。\n2）在仓库目录下，没有对默认的数据库 default 创建文件夹。如果某张表属于 default 数 据库，直接\n在数据仓库目录下创建一个文件夹。\n3）修改 default 数据仓库原始位置（将 hive-default.xml.template 如下配置信息拷贝到 hive-site.xml文件中）\n&lt;property&gt;&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;&lt;value&gt;/user/hive/warehouse&lt;/value&gt;&lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt;\n\n配置同组用户有执行权限\n[root@master conf]# hadoop fs -chmod 777 /user/hive/warehouse\n\n2.9.2 查询后信息显示配置1）在 hive-site.xml 文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。\n&lt;property&gt;&lt;name&gt;hive.cli.print.header&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;description&gt;Whether to print the names of the columns in query output.&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.cli.print.current.db&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;description&gt;Whether to include the current database in the Hive prompt.&lt;/description&gt;&lt;/property&gt;\n\n2）重新启动 hive，对比配置前后差异。\n配置前\nhive&gt; select * from student limit 10;OK2012003 lyj2121001 lzh2121002 wkh2121003 jy2121004 syx2121005 ntt2121006 gyl2121007 hcr2121008 xhyTime taken: 1.147 seconds, Fetched: 10 row(s)\n\n配置后\nhive (default)&gt; select * from student limit 10;OKstudent.id student.name2012003 lyj2121001 lzh2121002 wkh2121003 jy2121004 syx2121005 ntt2121006 gyl2121007 hcr2121008 xhyTime taken: 0.371 seconds, Fetched: 10 row(s)\n\n2.9.3 Hive 运行日志信息配置1．Hive 的 log 默认存放在&#x2F;tmp&#x2F;root&#x2F;hive.log 目录下（当前用户名下）\n2．修改 hive 的 log 存放日志到&#x2F;usr&#x2F;local&#x2F;soft&#x2F;hive&#x2F;logs\n（1）修改&#x2F;usr&#x2F;local&#x2F;soft&#x2F;hive&#x2F;conf&#x2F;hive-log4j.properties.template   文件名称为 hive-log4j.properties\n[root@master conf]# mv hive-log4j.properties.template hive-log4j.properties\n\n（2）在 hive-log4j.properties 文件中修改 log 存放位置\n[root@master conf]# vim hive-log4j.propertieshive.log.dir=/usr/local/soft/hive/logs\n\n2.9.4 参数配置方式1．查看当前所有的配置信息\nhive (default)&gt; set;\n\n2．参数的配置三种方式\n（1）配置文件方式\n默认配置文件：hive-default.xml\n用户自定义配置文件：hive-site.xml\n注意：用户自定义配置会覆盖默认配置。另外，Hive 也会读入 Hadoop 的配置，因为 Hive 是作为 Hadoop 的客户端启动的，Hive 的配置会覆盖 Hadoop 的配置。配置文件的设定对本机启动的所有 Hive 进程都有效。\n（2）命令行参数方式\n启动 Hive 时，可以在命令行添加-hiveconf param&#x3D;value 来设定参数。\n#例如[root@master conf]# hive -hiveconf mapred.reduce.tasks=3;#查看参数设置：hive (default)&gt; set mapred.reduce.tasks;mapred.reduce.tasks=3hive (default)&gt;\n\n注意：仅对本次 hive 启动有效\n（3）参数声明方式\n可以在 HQL 中使用 SET 关键字设定参数\nhive (default)&gt; set mapred.reduce.tasks=100;#查看参数设置hive (default)&gt; set mapred.reduce.tasks;mapred.reduce.tasks=100hive (default)&gt;\n\n注意：仅对本次 hive 启动有效\n上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系 统级的参数，例如 log4j 相关的设定，必须用前两种方式设定，因为那些参数的读取在会话 建立以前已经完成了。\n第3 章 Hive数据类型\n基本数据类型\n\n\n对于 Hive 的 String 类型相当于数据库的 varchar 类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储 2GB 的字符数。\n3.2 集合数据类型\n Hive 有三种复杂数据类型 ARRAY、MAP 和 STRUCT。ARRAY 和 MAP 与 Java 中的 Array 和 Map 类似，而 STRUCT 与 C 语言中的 Struct 类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。\n\n案例实操\n\n（1）假设某表有如下一行，我们用 JSON 格式来表示其数据结构。在 Hive 下访问的格 式为\n{\"name\": \"songsong\",\"friends\": [\"bingbing\", \"lili\"], //列表 Array,\"children\": { //键值 Map,\"xiao song\": 18,\"xiaoxiao song\": 19}\"address\": { //结构 Struct,\"street\": \"hui long guan\",\"city\": \"beijing\"}}\n\n（2）基于上述数据结构，我们在 Hive 里创建对应的表，并导入数据。\n创建本地测试文件 test.txt\nsongsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijingyangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing\n\n注意：MAP，STRUCT 和 ARRAY 里的元素间关系都可以用同一个字符表示，这里用“_”。\n（3）Hive 上创建测试表 test\ncreate table test(name string,friends array &lt; string &gt; ,children map &lt; string, int &gt; ,address struct &lt; street: string, city: string &gt;)row format delimited fields terminated by ','collection items terminated by '_'map keys terminated by ':'lines terminated by '\\n';\n\n字段解释：\nrow format delimited fields terminated by ‘,’ -- 列分隔符\ncollection items terminated by ‘_‘  --MAP STRUCT 和 ARRAY 的分隔符(数 据分割符号)\nmap keys terminated by ‘:’– MAP 中的 key 与 value 的分隔符\nlines terminated by ‘\\n’;– 行分隔符\n（4）导入文本数据到测试表\nload data local inpath ‘&#x2F;root&#x2F;data&#x2F;test.txt’ into table test;\n（5）访问三种集合列里的数据，以下分别是 ARRAY，MAP，STRUCT 的访问方式\nhive (default)&gt; select friends[1],children['xiao song'],address.city fromtestwhere name=\"songsong\";OK_c0 _c1 citylili 18 beijingTime taken: 0.076 seconds, Fetched: 1 row(s)\n\n3.3 类型转化Hive 的原子数据类型是可以进行隐式转换的，类似于 Java 的类型转换，例如某表达式使用 INT 类型，TINYINT 会自动转换为 INT 类型，但是 Hive 不会进行反向转化，例如，某表 达式使用 TINYINT 类型，INT 不会自动转换为 TINYINT 类型，它会返回错误，除非使用 CAST 操作。\n1****）隐式类型转换规则如下 \n（1）任何整数类型都可以隐式地转换为一个范围更广的类型，如 TINYINT 可以转换成 INT，INT 可以转换成 BIGINT。\n（2）所有整数类型、FLOAT 和 STRING 类型都可以隐式地转换成 DOUBLE。\n（3）TINYINT、SMALLINT、INT 都可以转换为 FLOAT。\n（4）BOOLEAN 类型不可以转换为任何其它的类型。\n2****）可以使用 CAST 操作显示进行数据类型转换 \n例如 CAST(‘1’ AS INT)将把字符串’1’ 转换成整数 1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。\n0: jdbc:hive2://hadoop102:10000&gt; select '1'+2, cast('1'as int) + 2;+------+------+--+| _c0 | _c1 |+------+------+--+| 3.0 | 3 |+------+------+--+\n\n第4章 DDL数据定义4.1 创建数据库CREATE DATABASE [IF NOT EXISTS] database_name[COMMENT database_comment][LOCATION hdfs_path][WITH DBPROPERTIES (property_name=property_value, ...)];\n\n1）创建一个数据库，数据库在 HDFS 上的默认存储路径是&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;*.db。\nhive (default)&gt; create database db_hive;\n\n2）避免要创建的数据库已经存在错误，增加 if not exists 判断。（标准写法）\nhive (default)&gt; create database db_hive;FAILED: Execution Error, return code 1 fromorg.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already existshive (default)&gt; create database if not exists db_hive;\n\n3）创建一个数据库，指定数据库在 HDFS 上存放的位置\nhive (default)&gt; create database db_hive2 location '/db_hive2.db';\n\n4.2 查询数据库4.2.1 显示数据库1）显示数据库\nhive&gt; show databases;\n\n2）过滤显示查询的数据库\nhive&gt; show databases like 'db_hive*';OKdb_hivedb_hive_1\n\n4.2.2 查看数据库详情1）显示数据库信息\nhive&gt; desc database db_hive;OKdb_hivehdfs://hadoop102:9820/user/hive/warehouse/db_hive.dbatguiguUSER\n\n2）显示数据库详细信息，****extended\nhive&gt; desc database extend\n\n4.2.3 切换当前数据库hive (default)&gt; use db_hive;\n\n4.3 修改数据库用户可以使用 ALTER DATABASE 命令为某个数据库的 DBPROPERTIES 设置键-值对属性值，来描述这个数据库的属性信息。\nhive (default)&gt; alter database db_hiveset dbproperties('createtime'='20170830');\n\n在 hive 中查看修改结果\nhive&gt; desc database extended db_hive;db_name comment location owner_name owner_type parametersdb_hive hdfs://hadoop102:9820/user/hive/warehouse/db_hive.dbatguigu USER {createtime=20170830}\n\n4.4 删除数据库1）删除空数据库\nhive&gt;drop database db_hive2;\n\n2）如果删除的数据库不存在，最好采用 if exists 判断数据库是否存在\nhive&gt; drop database db_hive;FAILED: SemanticException [Error 10072]: Database does not exist: db_hivehive&gt; drop database if exists db_hive2;\n\n3）如果数据库不为空，可以采用 cascade 命令，强制删除\nhive&gt; drop database db_hive;FAILED: Execution Error, return code 1 fromorg.apache.hadoop.hive.ql.exec.DDLTask.InvalidOperationException(message:Database db_hive is not empty. One ormore tables exist.)hive&gt; drop database db_hive cascade;\n\n4.5 创建表1）建表语法\nCREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name[(col_name data_type [COMMENT col_comment], ...)][COMMENT table_comment][PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)][CLUSTERED BY (col_name, col_name, ...)[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS][ROW FORMAT row_format][STORED AS file_format][LOCATION hdfs_path][TBLPROPERTIES (property_name=property_value, ...)][AS select_statement]\n\n2）字段解释说明\n（1）CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；\n用户可以用 IF NOT EXISTS 选项来忽略这个异常。\n（2）EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实 际数据的路径（LOCATION），在删除表的时候，内部表的元数据和数据会被一起删除，而外 部表只删除元数据，不删除数据。\n（3）COMMENT：为表和列添加注释。\n（4）PARTITIONED BY 创建分区表\n（5）CLUSTERED BY 创建分桶表\n（6）SORTED BY 不常用，对桶中的一个或多个列另外排序\n（7）ROW FORMAT\nDELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]\n[MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]\n| SERDE serde_name [WITH SERDEPROPERTIES (property_name&#x3D;property_value,\nproperty_name&#x3D;property_value, …)]\n用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive 通过 SerDe 确定表 的具体的列的数据。\nSerDe 是 Serialize&#x2F;Deserilize 的简称， hive 使用 Serde 进行行对象的序列与反序列化。\n（8）STORED AS 指定存储文件类型\n常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列 式存储格式文件） 如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。\n（9）LOCATION ：指定表在 HDFS 上的存储位置。\n（10）AS：后跟查询语句，根据查询结果创建表。\n（11）LIKE 允许用户复制现有的表结构，但是不复制数据。 \n4.5.1 管理表1）理论\n默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive 会（或多或 少地）控制着数据的生命周期。Hive 默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，&#x2F;user&#x2F;hive&#x2F;warehouse)所定义的目录的子目录下。\n当我们删除一个管理表时，Hive 也会删除这个表中数据。管理表不适合和其他工具共享数据。 \n2）案例实操\n（0）原始数据\n1001 ss11002 ss21003 ss31004 ss41005 ss51006 ss61007 ss71008 ss81009 ss91010 ss101011 ss111012 ss121013 ss131014 ss141015 ss151016 ss16\n\ncreate table if not exists student(id int, name string)row format delimited fields terminated by '\\t'stored as textfilelocation '/user/hive/warehouse/student';\n\n（1）普通创建表\n（2）根据查询结果创建表（查询的结果会添加到新创建的表中）\ncreate table if not exists student2 as select id, name from student;\n\n（3）根据已经存在的表结构创建表\ncreate table if not exists student3 like student;\n\n4.5.2 外部表1）理论\n因为表是外部表，所以 Hive 并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。\n2****）管理表和外部表的使用场景\n每天将收集到的网站日志定期流入 HDFS 文本文件。在外部表（原始日志表）的基础上 做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过 SELECT+INSERT 进入内部表。\n3****）案例实操\n分别创建部门和员工外部表，并向表中导入数据。\n（0）原始数据\ndept:\n10 ACCOUNTING 170020 RESEARCH 180030 SALES 190040 OPERATIONS 1700\n\nemp： \n7369 SMITH CLERK 7902 1980-12-17 800.00 207499 ALLEN SALESMAN 7698 1981-2-20 1600.00 300.00 307521 WARD SALESMAN 7698 1981-2-22 1250.00 500.00 307566 JONES MANAGER 7839 1981-4-2 2975.00 207654 MARTIN SALESMAN 7698 1981-9-28 1250.00 1400.00 307698 BLAKE MANAGER 7839 1981-5-1 2850.00 307782 CLARK MANAGER 7839 1981-6-9 2450.00 107788 SCOTT ANALYST 7566 1987-4-19 3000.00 207839 KING PRESIDENT 1981-11-17 5000.00 107844 TURNER SALESMAN 7698 1981-9-8 1500.00 0.00 307876 ADAMS CLERK 7788 1987-5-23 1100.00 207900 JAMES CLERK 7698 1981-12-3 950.00 307902 FORD ANALYST 7566 1981-12-3 3000.00 207934 MILLER CLERK 7782 1982-1-23 1300.00 10\n\n（1）上传数据到 HDFS\nhive (default)&gt; dfs -mkdir /student;hive (default)&gt; dfs -put /usr/local/soft/datas/student.txt /student;\n\n（2）建表语句，创建外部表\n创建部门表\ncreate external table if not exists dept(deptno int,dname string,loc int)row format delimited fields terminated by '\\t';\n\n创建员工表\ncreate external table if not exists emp(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)row format delimited fields terminated by '\\t';\n\n（3）查看创建的表\nhive (default)&gt;show tables;\n\n（4）查看表格式化数据\nhive (default)&gt; desc formatted dept;Table Type: EXTERNAL_TABLE\n\n（5）删除外部表\nhive (default)&gt; drop table dept;\n\n外部表删除后，hdfs 中的数据还在，但是 metadata 中 dept 的元数据已被删除\n4.5.3 管理表与外部表的互相转换（1）查询表的类型\nhive (default)&gt; desc formatted student2;Table Type: MANAGED_TABLE\n\n（2）修改内部表 student2 为外部表\nalter table student2 set tblproperties('EXTERNAL'='TRUE');\n\n（3）查询表的类型\nhive (default)&gt; desc formatted student2;Table Type: EXTERNAL_TABLE\n\n（4）修改外部表 student2 为内部表\nalter table student2 set tblproperties('EXTERNAL'='FALSE');\n\n（5）查询表的类型\nhive (default)&gt; desc formatted student2;Table Type: MANAGED_TABLE\n\n注意：(‘EXTERNAL’&#x3D;’TRUE’)和(‘EXTERNAL’&#x3D;’FALSE’)为固定写法，区分大小写！\n4.6 修改表4.6.1 重命名表1）语法\nALTER TABLE table_name RENAME TO new_table_name\n\n2）实操案例\nhive (default)&gt; alter table dept_partition2 rename to dept_partition3;\n\n4.6.2 增加、修改和删除表分区见后面章节分区表基本操作\n4.6.3 增加**&#x2F;修改&#x2F;**替换列信息1）语法\n（1）更新列\nALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_namecolumn_type [COMMENT col_comment] [FIRST|AFTER column_name]\n\n（2）增加和替换列\nALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENTcol_comment], ...)\n\n注：ADD 是代表新增一字段，字段位置在所有列后面(partition 列前)，REPLACE 则是表示替换表中所有字段。\n2）实操案例\n（1）查询表结构\nhive&gt; desc dept;\n\n（2）添加列\nhive (default)&gt; alter table dept add columns(deptdesc string);\n\n（3）查询表结构\nhive&gt; desc dept;\n\n（4）更新列\nhive (default)&gt; alter table dept change column deptdesc desc string;\n\n（5）查询表结构\nhive&gt; desc dept;\n\n（6）替换列\nhive (default)&gt; alter table dept replace columns(deptno string, dnamestring, loc string);\n\n（7）查询表结构\nhive&gt; desc dept;\n\n4.7 删除表hive (default)&gt; drop table dept;\n\n第5章 DML 数据操作5.1 数据导入5.1.1 向表中装载数据（Load）1）语法\nhive&gt; load data [local] inpath '数据的 path' [overwrite] into tablestudent [partition (partcol1=val1,…)];\n\n（1）load data:表示加载数据\n（2）local:表示从本地加载数据到 hive 表；否则从 HDFS 加载数据到 hive 表\n（3）inpath:表示加载数据的路径\n（4）overwrite:表示覆盖表中已有数据，否则表示追加\n（5）into table:表示加载到哪张表\n（6）student:表示具体的表\n（7）partition:表示上传到指定分区\n2）实操案例\n（0）创建一张表\nhive (default)&gt; create table student(id string, name string) row formatdelimited fields terminated by '\\t';\n\n（1）加载本地文件到 hive\nhive (default)&gt; load data local inpath'/opt/module/hive/datas/student.txt' into table default.student;\n\n（2）加载 HDFS 文件到 hive 中\n上传文件到 HDFS\nhive (default)&gt; dfs -put /usr/local/soft/datastudent.txt/user/atguigu/hive;\n\n加载 HDFS 上数据\nhive (default)&gt; load data inpath '/user/atguigu/hive/student.txt' intotable default.student;\n\n（3）加载数据覆盖表中已有的数据\n上传文件到 HDFS\nhive (default)&gt; dfs -put /usr/local/soft/data/student.txt /user/atguigu/hive;\n\n加载数据覆盖表中已有的数据\nhive (default)&gt; load data inpath '/user/atguigu/hive/student.txt'overwrite into table default.student;\n\n5.1.2 通过查询语句向表中插入数据（Insert）1）创建一张表\nhive (default)&gt; create table student_par(id int, name string) row formatdelimited fields terminated by '\\t';\n\n2）基本插入数据\nhive (default)&gt; insert into table student_parvalues(1,'wangwu'),(2,'zhaoliu');\n\n3）基本模式插入（根据单张表查询结果）\nhive (default)&gt; insert overwrite table student_parselect id, name from student where month='201709';\n\ninsert into：以追加数据的方式插入到表或分区，原有数据不会删除\ninsert overwrite：会覆盖表中已存在的数据\n注意：insert 不支持插入部分字段\n4）多表（多分区）插入模式（根据多张表查询结果）\nhive (default)&gt; from studentinsert overwrite table student partition(month='201707')select id, name where month='201709'insert overwrite table student partition(month='201706')select id, name where month='201709';\n\n5.1.3 查询语句中创建表并加载数据（As Select）详见 4.5.1 章创建表。\n根据查询结果创建表（查询的结果会添加到新创建的表中）\ncreate table if not exists student3as select id, name from student;\n\n5.1.4 创建表时通过 Location 指定加载数据路径1）上传数据到 hdfs 上\nhive (default)&gt; dfs -mkdir /student;hive (default)&gt; dfs -put /usr/local/soft/datas/student.txt /student;\n\n2）创建表，并指定在 hdfs 上的位置\nhive (default)&gt; create external table if not exists student5(id int, name string)row format delimited fields terminated by '\\t'location '/student;\n\n3）查询数据\nhive (default)&gt; select * from student5;\n\n5.1.5 Import 数据到指定 Hive 表中注意：先用 export 导出后，再将数据导入。 \nhive (default)&gt; import table student2from '/user/hive/warehouse/export/student';\n\n5.2 数据导出5.2.1 Insert 导出hive (default)&gt; insert overwrite local directory'/usr/local/soft/dataexport/student'select * from student;\n\n1）将查询的结果导出到本地\n2）将查询的结果格式化导出到本地\nhive(default)&gt;insert overwrite local directory'/usr/local/soft/dataexport/student1'ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'select * from student;\n\n3）将查询的结果导出到 HDFS 上(没有 local)\nhive (default)&gt; insert overwrite directory '/user/atguigu/student2'ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'select * from student;\n\n5.2.2 Hadoop 命令导出到本地hive (default)&gt; dfs -get /user/hive/warehouse/student/student.txt/usr/local/soft/data/export/student3.txt;\n\n5.2.3 Hive Shell 命令导出基本语法：（hive -f&#x2F;-e 执行语句或者脚本 &gt; file****）\n[root@master hive]$ bin/hive -e 'select * from default.student;' &gt;/usr/local/soft/dataexport/student4.txt;\n\n5.2.4 Export 导出到 HDFS 上(defahiveult)&gt; export table default.studentto '/user/hive/warehouse/export/student';\n\nexport 和 import 主要用于两个 Hadoop 平台集群之间 Hive 表迁移。\n5.2.5 Sqoop 导出安装参考：Sqoop安装_小白文的博客-CSDN博客_sqoop安装\n使用参考：sqoop导入mysql数据到Hive的各种方案_mysql数据导入到hive_海阔天空&amp;沫语的博客-CSDN博客 \n5.2.6 清除表中数据（Truncate）注意：Truncate 只能删除管理表，不能删除外部表中数据\nhive (default)&gt; truncate table student;\n\n第 6 章 查询https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select\n查询语句语法：\nSELECT [ALL | DISTINCT] select_expr, select_expr, ...FROM table_reference[WHERE where_condition][GROUP BY col_list][ORDER BY col_list][CLUSTER BY col_list| [DISTRIBUTE BY col_list] [SORT BY col_list]][LIMIT number]\n\n6.1 基本查询（Select…From）6.1.1 全表和特定列查询0）数据准备\n（0）原始数据\ndept:\n10 ACCOUNTING 170020 RESEARCH 180030 SALES 190040 OPERATIONS 1700\n\nemp：\n7369 SMITH CLERK 7902 1980-12-17 800.00 207499 ALLEN SALESMAN 7698 1981-2-20 1600.00 300.00 307521 WARD SALESMAN 7698 1981-2-22 1250.00 500.00 307566 JONES MANAGER 7839 1981-4-2 2975.00 207654 MARTIN SALESMAN 7698 1981-9-28 1250.00 1400.00 307698 BLAKE MANAGER 7839 1981-5-1 2850.00 307782 CLARK MANAGER 7839 1981-6-9 2450.00 107788 SCOTT ANALYST 7566 1987-4-19 3000.00 207839 KING PRESIDENT 1981-11-17 5000.00 107844 TURNER SALESMAN 7698 1981-9-8 1500.00 0.00 307876 ADAMS CLERK 7788 1987-5-23 1100.00 207900 JAMES CLERK 7698 1981-12-3 950.00 307902 FORD ANALYST 7566 1981-12-3 3000.00 207934 MILLER CLERK 7782 1982-1-23 1300.00 10\n\n（1）创建部门表\ncreate table if not exists dept(deptno int,dname string,loc int)row format delimited fields terminated by '\\t';\n\n（2）创建员工表\ncreate table if not exists emp(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)row format delimited fields terminated by '\\t';\n\n（3）导入数据\nload data local inpath '/usr/local/soft/datas/dept.txt' into tabledept;load data local inpath '/usr/local/soft/datas/emp.txt' into table emp;\n\n1）全表查询\nhive (default)&gt; select * from emp;hive (default)&gt; select empno,ename,job,mgr,hiredate,sal,comm,deptno from emp ;\n\n2）选择特定列查询\nhive (default)&gt; select empno, ename from emp;\n\n注意：\n（1）SQL 语言大小写不敏感。\n（2）SQL 可以写在一行或者多行\n（3）关键字不能被缩写也不能分行\n（4）各子句一般要分行写。\n（5）使用缩进提高语句的可读性。\n6.1.2 列别名1****）重命名一个列\n2****）便于计算\n3）紧跟列名，也可以在列名和别名之间加入关键字‘AS****’\n4****）案例实操\n查询名称和部门\nhive (default)&gt; select ename AS name, deptno dn from emp;\n\n6.1.3 算术运算符\n6.1.4 常用函数1）求总行数（count）\nhive (default)&gt; select count(*) cnt from emp;\n\n2）求工资的最大值（max）\nhive (default)&gt; select max(sal) max_sal from emp;\n\n3）求工资的最小值（min）\nhive (default)&gt; select min(sal) min_sal from emp;\n\n4）求工资的总和（sum****）\nhive (default)&gt; select sum(sal) sum_sal from emp;\n\n5）求工资的平均值（avg****）\nhive (default)&gt; select avg(sal) avg_sal from emp;\n\n6.1.5 Limit 语句典型的查询会返回多行数据。LIMIT 子句用于限制返回的行数。\nhive (default)&gt; select * from emp limit 5;hive (default)&gt; select * from emp limit 2;\n\n6.1.6 Where 语句1）使用 WHERE 子句，将不满足条件的行过滤掉\n2）WHERE 子句紧随 FROM 子句\n3）案例实操\n查询出薪水大于 1000 的所有员工\nhive (default)&gt; select * from emp where sal &gt;1000;\n\n注意：where 子句中不能使用字段别名\n6.1.7 比较运算符（Between&#x2F;In&#x2F; Is Null）1）下面表中描述了谓词操作符，这些操作符同样可以用于 JOIN…ON 和 HAVING 语句中\n\n6.1.8 Like 和 RLike1）使用 LIKE 运算选择类似的值\n2）选择条件可以包含字符或数字:\n% 代表零个或多个字符(任意个字符)。\n_ 代表一个字符\n3）****RLIKE 子句\nRLIKE 子句是 Hive 中这个功能的一个扩展，其可以通过 Java 的正则表达式这个更强大的语言来指定匹配条件\n6.1.9 逻辑运算符（And&#x2F;Or&#x2F;Not）\n6.2 分组6.2.1 Group By 语句GROUP BY 语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。\n6.2.2 Having 语句1）having 与 where 不同点\n（1）where 后面不能写分组函数，而 having 后面可以使用分组函数。\n（2）having 只用于 group by 分组统计语句。 \n6.3 Join 语句6.3.1 等值 JoinHive 支持通常的 SQL JOIN 语句。\n6.3.2 表的别名1）好处\n（1）使用别名可以简化查询。\n（2）使用表名前缀可以提高执行效率。\n6.3.3 内连接hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno;\n\n内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。\n6.3.4 左外连接左外连接：JOIN 操作符左边表中符合 WHERE 子句的所有记录将会被返回。\nhive (default)&gt; select e.empno, e.ename, d.deptno from emp e left joindept d on e.deptno = d.deptno;\n\n6.3.5 右外连接右外连接：JOIN 操作符右边表中符合 WHERE 子句的所有记录将会被返回。\nhive (default)&gt; select e.empno, e.ename, d.deptno from emp e right joindept d on e.deptno = d.deptno;\n\n6.3.6 满外连接满外连接：将会返回所有表中符合 WHERE 语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值替代。 \nhive (default)&gt; select e.empno, e.ename, d.deptno from emp e full joindept d on e.deptno = d.deptno;\n\n6.3.7 多表连接注意：连接 n 个表，至少需要 n-1 个连接条件。例如：连接三个表，至少需要两个连接条件。\n数据准备\n1700 Beijing1800 London1900 Tokyo\n\n1）创建位置表\ncreate table if not exists location(loc int,loc_name string)row format delimited fields terminated by '\\t';\n\n2）导入数据\nhive (default)&gt; load data local inpath '/usr/local/soft/datas/location.txt'into table location;\n\n3****）多表连接查询\nhive (default)&gt;SELECT e.ename, d.dname, l.loc_nameFROM emp eJOIN dept dON d.deptno = e.deptnoJOIN location lON d.loc = l.loc;\n\n大多数情况下，Hive 会对每对 JOIN 连接对象启动一个 MapReduce 任务。本例中会首先 启动一个 MapReduce job 对表 e 和表 d 进行连接操作，然后会再启动一个MapReduce job 将第一个 MapReduce job 的输出和表 l;进行连接操作。\n注意：为什么不是表 d 和表 l 先进行连接操作呢？这是因为 Hive 总是按照从左到右的顺序执行的。\n优化：当对 3 个或者更多表进行 join 连接时，如果每个 on 子句都使用相同的连接键的话，那么只会产生一个 MapReduce job。\n6.3.8 笛卡尔积1）笛卡尔集会在下面条件下产生\n（1）省略连接条件\n（2）连接条件无效\n（3）所有表中的所有行互相连接\n2）案例实操\nhive (default)&gt; select empno, dname from emp, dept;\n\n6.4 排序6.4.1 全局排序（Order By）Order By：全局排序，只有一个 Reducer\n1）使用 ORDER BY 子句排序\nASC（ascend）: 升序（默认）\nDESC（descend）: 降序\n2）ORDER BY 子句在 SELECT 语句的结尾\n3****）案例实操\n（1）查询员工信息按工资升序排列\nhive (default)&gt; select * from emp order by sal;\n\n（2）查询员工信息按工资降序排列\nhive (default)&gt; select * from emp order by sal desc;\n\n**6.4.2 按照别名排序 **按照员工薪水的 2 倍排序\nhive (default)&gt; select ename, sal*2 twosal from emp order by twosal;\n\n6.4.3 多个列排序按照部门和工资升序排序\nhive (default)&gt; select ename, deptno, sal from emp order by deptno, sal;\n\n6.4.4 每个 Reduce 内部排序（Sort By）Sort By：对于大规模的数据集 order by 的效率非常低。在很多情况下，并不需要全局排序，此时可以使用 sort by。\nSort by 为每个 reducer 产生一个排序文件。每个 Reducer 内部进行排序，对全局结果集 来说不是排序。\n1）设置 reduce 个数\nhive (default)&gt; set mapreduce.job.reduces=3;\n\n2）查看设置 reduce 个数\nhive (default)&gt; set mapreduce.job.reduces;\n\n3）根据部门编号降序查看员工信息\nhive (default)&gt; select * from emp sort by deptno desc;\n\n4）将查询结果导入到文件中（按照部门编号降序排序）\nhive (default)&gt; insert overwrite local directory'/usr/local/soft/data/sortby-result'select * from emp sort by deptno desc;\n\n6.4.5 分区（Distribute By）Distribute By：\n在有些情况下，我们需要控制某个特定行应该到哪个 reducer，通常是为了进行后续的聚集操作。distribute by 子句可以做这件事。distribute by 类似 MR 中 partition（自定义分区），进行分区，结合 sort by 使用。 对于 distribute by 进行测试，一定要分配多 reduce 进行处理，否则无法看到 distribute by 的效果。\n1）案例实操：\n（1）先按照部门编号分区，再按照员工编号降序排序。\nhive (default)&gt; set mapreduce.job.reduces=3;hive (default)&gt; insert overwrite local directory'/usr/local/soft/data/distribute-result' select * from emp distribute bydeptno sort by empno desc;\n\n注意： \n➢ distribute by 的分区规则是根据分区字段的 hash 码与 reduce 的个数进行模除后，\n余数相同的分到一个区。\n➢ Hive 要求 DISTRIBUTE BY 语句要写在 SORT BY 语句之前。\n6.4.6 Cluster By当 distribute by 和 sorts by 字段相同时，可以使用 cluster by 方式。\ncluster by 除了具有 distribute by 的功能外还兼具 sort by 的功能。但是排序只能是升序排序，不能指定排序规则为 ASC 或者 DESC。\n（1）以下两种写法等价\nhive (default)&gt; select * from emp cluster by deptno;hive (default)&gt; select * from emp distribute by deptno sort by deptno;\n\n注意：按照部门编号分区，不一定就是固定死的数值，可以是 20 号和 30 号部门分到一 个分区里面去。\n第 7 章 分区表和分桶表7.1 分区表分区表实际上就是对应一个 HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所 有的数据文件。Hive 中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过 WHERE 子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多\n7.1.1 分区表基本操作1）引入分区表（需要根据日期对日志进行管理, 通过部门信息模拟）\ndept_20220810.logdept_20220811.logdept_20220812.log\n\n2）创建分区表语法\nhive (default)&gt; create table dept_partition(deptno int, dname string, loc string)partitioned by (day string)row format delimited fields terminated by '\\t';\n\n注意：分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。\n3）加载数据到分区表中\n（1） 数据准备\ndept_20220810.log\n10 ACCOUNTING 170020 RESEARCH 1800\n\ndept_20220811.log\n30 SALES 190040 OPERATIONS 1700\n\ndept_20220812.log\n50 TEST 200060 DEV 1900\n\n（2） 加载数据\nhive (default)&gt; load data local inpath'/usr/local/soft/data/dept_20220810.log' into table dept_partitionpartition(day='20220810');hive (default)&gt; load data local inpath'/usr/local/soft/data/dept_20220811.log' into table dept_partitionpartition(day='20220811');hive (default)&gt; load data local inpath'/usr/local/soft/data/dept_20220812.log' into table dept_partitionpartition(day='20220812');\n\n注意：分区表加载数据时，必须指定分区\n4）查询分区表中数据\n单分区查询\nhive (default)&gt; select * from dept_partition where day='20220810';\n\n多分区联合查询\nunionselect * from dept_partition where day='20220811'unionselect * from dept_partition where day='20220812';hive (default)&gt; select * from dept_partition where day='20220810'orday='20220811'or day='20220812';\n\n5）增加分区\n创建单个分区\nhive (default)&gt; alter table dept_partition add partition(day='20220813');\n\n同时创建多个分区\nhive (default)&gt; alter table dept_partition add partition(day='20220814')partition(day='20220815');\n\n6）删除分区\n删除单个分区\nhive (default)&gt; alter table dept_partition drop partition (day='20220815');\n\n同时删除多个分区\nhive (default)&gt; alter table dept_partition drop partition(day='20220813'), partition(day='20220814');\n\n7）查看分区表有多少分区\nhive&gt; show partitions dept_partition;//推荐这种方式（直接从元数据中获取分区信息）hive&gt;select distinct pt from students_pt; // 不推荐\n\n8）查看分区表结构\nhive&gt; desc formatted dept_partition;# Partition Information# col_name data_type commentmonth string\n\n7.1.2 二级分区思考: 如何一天的日志数据量也很大，如何再将数据拆分? \n1）创建二级分区表\nhive (default)&gt; create table dept_partition2(deptno int, dname string, loc string)partitioned by (day string, hour string)row format delimited fields terminated by '\\t';\n\n2）正常的加载数据\n（1）加载数据到二级分区表中\nhive (default)&gt; load data local inpath'/usr/local/soft/data/dept_20220810.log' into tabledept_partition2 partition(day='20220810', hour='12');\n\n（2）查询分区数据\nhive (default)&gt; select * from dept_partition2 where day='20220810' andhour='12';\n\n3）把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式\n（1）方式一：上传数据后修复\n上传数据\nhive (default)&gt; dfs -mkdir -p/user/hive/warehouse/mydb.db/dept_partition2/day=20220810/hour=13;hive (default)&gt; dfs -put /usr/local/soft/data/dept_20220810.log/user/hive/warehouse/mydb.db/dept_partition2/day=20220810/hour=13;\n\n查询数据（查询不到刚上传的数据）\nhive (default)&gt; select * from dept_partition2 where day='20220810' andhour='13';\n\n执行修复命令\nhive&gt; msck repair table dept_partition2;\n\n再次查询数据\nhive (default)&gt; select * from dept_partition2 where day='20220810' andhour='13';\n\n（2）方式二：上传数据后添加分区\n上传数据\nhive (default)&gt; dfs -mkdir -p/user/hive/warehouse/mydb.db/dept_partition2/day=20220811/hour=14;hive (default)&gt; dfs -put /usr/local/soft/data/dept_20220811.log/user/hive/warehouse/mydb.db/dept_partition2/day=220220811/hour=14;\n\n执行添加分区\nhive (default)&gt; alter table dept_partition2 addpartition(day='220220811',hour='14');\n\n查询数据\nhive (default)&gt; select * from dept_partition2 where day='220220811' andhour='14';\n\n（3）方式三：创建文件夹后 load 数据到分区\n创建目录\nhive (default)&gt; dfs -mkdir -p/user/hive/warehouse/mydb.db/dept_partition2/day=220220811/hour=15;\n\n上传数据\nhive (default)&gt; load data local inpath'/usr/local/soft/data/dept_220220811.log' into tabledept_partition2 partition(day='220220811',hour='15');\n\n查询数据\nhive (default)&gt; select * from dept_partition2 where day='220220811' andhour='15';\n\n7.1.3 动态分区调整关系型数据库中，对分区表 Insert 数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive 中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用 Hive 的动态分区，需要进行相应的配置。 \n1****）开启动态分区参数设置\n（1）开启动态分区功能（默认 true，开启）\nhive.exec.dynamic.partition=true\n\n（2）设置为非严格模式（动态分区的模式，默认 strict，表示必须指定至少一个分区为静态分区，nonstrict 模式表示允许所有的分区字段都可以使用动态分区。） \nhive.exec.dynamic.partition.mode=nonstrict\n\n（3）在所有执行 MR 的节点上，最大一共可以创建多少个动态分区。默认 1000\nhive.exec.max.dynamic.partitions=1000\n\n（4）在每个执行 MR 的节点上，最大可以创建多少个动态分区。该参数需要根据实际 的数据来设定。比如：源数据中包含了一年的数据，即 day 字段有 365 个值，那么该参数就需要设置成大于 365，如果使用默认值 100，则会报错。 \nhive.exec.max.dynamic.partitions.pernode=100\n\n（5）整个 MR Job 中，最大可以创建多少个 HDFS 文件。默认 100000\nhive.exec.max.created.files=100000\n\n（6）当有空分区生成时，是否抛出异常。一般不需要设置。默认 false\nhive.error.on.empty.partition=false\n\n2）案例实操\n需求：将 dept 表中的数据按照地区（loc 字段），插入到目标表 dept_partition 的相应分区中。\n（1）创建目标分区表\nhive (default)&gt; create table dept_partition_dy(id int, name string)partitioned by (loc int) row format delimited fields terminated by '\\t';\n\n（2）设置动态分区\nset hive.exec.dynamic.partition.mode = nonstrict;hive (default)&gt; insert into table dept_partition_dy partition(loc) select deptno, dname, loc from dept;\n\n（3）查看目标分区表的分区情况\nhive (default)&gt; show partitions dept_partition;\n\n思考：目标分区表是如何匹配到分区字段的？\n7.2 分桶表分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理 的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围 划分。分桶是将数据集分解成更容易管理的若干部分的另一个技术。 \n分区针对的是数据的存储路径；分桶针对的是数据文件。\n1）先创建分桶表\n（1）数据准备\n1001 ss11002 ss21003 ss31004 ss41005 ss51006 ss61007 ss71008 ss81009 ss91010 ss101011 ss111012 ss121013 ss131014 ss141015 ss151016 ss16\n\n（2）创建分桶表\ncreate table stu_buck(id int, name string)clustered by(id)into 4 bucketsrow format delimited fields terminated by '\\t';\n\n（3）查看表结构\nhive (default)&gt; desc formatted stu_buck;Num Buckets: 4\n\n（4）导入数据到分桶表中，load 的方式\nhive (default)&gt; load data inpath '/student.txt' into table stu_buck;\n\n（5）查看创建的分桶表中是否分成 4 个桶\n（6）查询分桶的数据\nhive(default)&gt; select * from stu_buck;\n\n（7）分桶规则：\n根据结果可知：Hive 的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中\n2）分桶表操作需要注意的事项:****\n（1）reduce 的个数设置为-1,让 Job 自行决定需要用多少个 reduce 或者将 reduce 的个数设置为大于等于分桶表的桶数\n（2）从 hdfs 中 load 数据到分桶表中，避免本地文件找不到问题\n（3）不要使用本地模式\n3）insert 方式将数据导入分桶表 \nhive(default)&gt;insert into table stu_buck select * from student_insert;\n\n7.3 抽样查询对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive 可以通过对表进行抽样来满足这个需求。\n语法: TABLESAMPLE(BUCKET x OUT OF y)\n查询表 stu_buck 中的数据。\nhive (default)&gt; select * from stu_buck tablesample(bucket 1 out of 4 onid);\n\n注意：x 的值必须小于等于 y 的值，否则\nFAILED: SemanticException [Error 10061]: Numerator should not be biggerthan denominator in sample clause for table stu_buck\n\n第 8 章 函数8.1 系统内置函数1）查看系统自带的函数\nhive&gt; show functions;\n\n2）显示自带的函数的用法\nhive&gt; desc function upper;\n\n3）详细显示自带的函数的用法\nhive&gt; desc function extended upper;\n\n8.2 常用内置函数8.2.1 空字段赋值1）函数说明\nNVL：给值为 NULL 的数据赋值，它的格式是 NVL( value，default_value)。它的功能是如果 value 为 NULL，则 NVL 函数返回 default_value 的值，否则返回 value 的值，如果两个参数 都为 NULL ，则返回 NULL。\n2）数据准备：采用员工表\n3****）查询：如果员工的 comm 为 NULL，则用-1 代替\nhive (default)&gt; select comm,nvl(comm, -1) from emp;\n\n4）查询：如果员工的 comm 为 NULL****，则用领导 id 代替\nhive (default)&gt; select comm, nvl(comm,mgr) from emp;\n\n8.2.2 CASE WHEN THEN ELSE END1）数据准备\n2）需求\n求出不同部门男女各多少人。结果如下： \ndept_Id 男 女A 2 1B 1 2\n\n3）创建本地 emp_sex.txt****，导入数据\n[root@master datas]#vimemp_sex.txt悟空 A 男大海 A 男宋宋 B 男凤姐 A 女婷姐 B 女婷婷 B 女\n\n4****）创建 hive 表并导入数据\ncreate table emp_sex(name string,dept_id string,sex string)row format delimited fields terminated by \"\\t\";load data local inpath '/usr/local/soft/dataemp_sex.txt' into tableemp_sex;\n\n5****）按需求查询数据\ndept_id,sum(case sex when '男' then 1 else 0 end) male_count,sum(case sex when '女' then 1 else 0 end) female_countfrom emp_sexgroup by dept_id;\n\n8.2.3 行转列1）相关函数说明\nCONCAT(string A&#x2F;col, string B&#x2F;col…)：返回输入字符串连接后的结果，支持任意个输入字符串;\nCONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参 数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将 为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;\n注意: CONCAT_WS must be “string or array&lt;string&gt;\nCOLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生 Array 类型字段。\n2）数据准备\n3）需求\n把星座和血型一样的人归类到一起。结果如下：\n射手座,A 大海|凤姐白羊座,A 孙悟空|猪八戒白羊座,B 宋宋|苍老师\n\n4）创建本地 constellation.txt****，导入数据\n[root@master datas]# vim constellation.txt孙悟空 白羊座 A大海 射手座 A宋宋 白羊座 B猪八戒 白羊座 A凤姐 射手座 A苍老师 白羊座 B\n\n5）创建 hive 表并导入数据\ncreate table person_info(name string,constellation string,blood_type string)row format delimited fields terminated by \",\";load data local inpath \"/usr/local/soft/data/constellation.txt\" into table person_info;\n\n6）按需求查询数据\nSELECTt1.c_b,CONCAT_WS(\"|\",collect_set(t1.name))FROM (SELECTNAME,CONCAT_WS(',',constellation,blood_type) c_bFROM person_info)t1GROUP BY t1.c_b\n\n使用concat（）函数做字符串的拼接；\n使用concat_ws（）和collect_set（）进行合并行\n将上面列表中一个user可能会占用多行转换为每个user占一行的目标表格式，实际是“列转行”\ncollect_set的作用：\n（1）去重，对group by后面的user进行去重\n（2）对group by以后属于同一user的形成一个集合，结合concat_ws对集合中元素使用，进行分隔形成字符串\n8.2.4 列转行1）函数说明\nEXPLODE(col)：将 hive 一列中复杂的 Array 或者 Map 结构拆分成多行。\nLATERAL VIEW\n用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias\n解释：用于和 split, explode 等 UDTF 一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。\n2）数据准备\n3）需求\n将电影分类中的数组数据展开。结果如下：\n《疑犯追踪》 悬疑《疑犯追踪》 动作《疑犯追踪》 科幻《疑犯追踪》 剧情《Lie to me》 悬疑《Lie to me》 警匪《Lie to me》 动作《Lie to me》 心理《Lie to me》 剧情《战狼 2》 战争《战狼 2》 动作《战狼 2》 灾难\n\n4）创建本地 movie.txt****，导入数据\n[root@master datas]# vi movie_info.txt《疑犯追踪》 悬疑,动作,科幻,剧情《Lie to me》悬疑,警匪,动作,心理,剧情《战狼 2》 战争,动作,灾难\n\n5）创建 hive 表并导入数据\ncreate table movie_info(movie string,category string)row format delimited fields terminated by \"\\t\";load data local inpath \"/usr/local/soft/data/movie.txt\" into tablemovie_info;\n\n6）按需求查询数据\nSELECTmovie,category_nameFROMmovie_infolateral VIEWexplode(split(category,\",\")) movie_info_tmp AS category_name;\n\n8.2.5 窗口函数（开窗函数）1）相关函数说明\nOVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化。\nCURRENT ROW：当前行\nn PRECEDING：往前 n 行数据\nn FOLLOWING：往后 n 行数据\nUNBOUNDED：起点，\nUNBOUNDED PRECEDING 表示从前面的起点，\nUNBOUNDED FOLLOWING 表示到后面的终点\nLAG(col,n,default_val)：往前第 n 行数据\nLEAD(col,n, default_val)：往后第 n 行数据\nNTILE(n)：把有序窗口的行分发到指定数据的组中，各个组有编号，编号从 1 开始，对于每一行，NTILE 返回此行所属的组的编号。注意：n 必须为 int 类型。\n2）数据准备：name，orderdate，****cost\njack,2017-01-01,10tony,2017-01-02,15jack,2017-02-03,23tony,2017-01-04,29jack,2017-01-05,46jack,2017-04-06,42tony,2017-01-07,50jack,2017-01-08,55mart,2017-04-08,62mart,2017-04-09,68neil,2017-05-10,12mart,2017-04-11,75neil,2017-06-12,80mart,2017-04-13,94\n\n3）需求\n（1）查询在 2017 年 4 月份购买过的顾客及总人数\n（2）查询顾客的购买明细及月购买总额\n（3）上述的场景, 将每个顾客的 cost 按照日期进行累加\n（4）查询每个顾客上次的购买时间\n（5）查询前 20%时间的订单信息\n4）创建本地 business.txt****，导入数据\n[root@master datas]$ vi business.txt\n\n5）创建 hive 表并导入数据\ncreate table business(name string,orderdate string,cost int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';load data local inpath \"/usr/local/soft/data/business.txt\" into tablebusiness;\n\n6）按需求查询数据\n（1） 查询在 2017 年 4 月份购买过的顾客及总人数\nselect name,count(*) over ()from businesswhere substring(orderdate,1,7) = '2017-04'group by name;\n\n（2） 查询顾客的购买明细及月购买总额\nselect name,orderdate,cost,sum(cost) over(partition by month(orderdate))from business;\n\n（3） 将每个顾客的 cost 按照日期进行累加\nselect name,orderdate,cost,sum(cost) over() as sample1,--所有行相加sum(cost) over(partition by name) as sample2,--按 name 分组，组内数据相加sum(cost) over(partition by name order by orderdate) as sample3,--按 name分组，组内数据累加sum(cost) over(partition by name order by orderdate rows betweenUNBOUNDED PRECEDING and current row ) as sample4 ,--和 sample3 一样,由起点到当前行的聚合sum(cost) over(partition by name order by orderdate rows between 1PRECEDING and current row) as sample5, --当前行和前面一行做聚合sum(cost) over(partition by name order by orderdate rows between 1PRECEDING AND 1 FOLLOWING ) as sample6,--当前行和前边一行及后面一行sum(cost) over(partition by name order by orderdate rows between currentrow and UNBOUNDED FOLLOWING ) as sample7 --当前行及后面所有行from business;\n\nrows 必须跟在 order by 子句之后，对排序的结果进行限制，使用固定的行数来限制分区中的数据行数量\n（4） 查看顾客上次的购买时间\nselect name,orderdate,cost,lag(orderdate,1,'1900-01-01') over(partition by name order by orderdate )as time1, lag(orderdate,2) over (partition by name order by orderdate) astime2from business;\n\n（5） 查询前 20%时间的订单信息\nselect * from (select name,orderdate,cost, ntile(5) over(order by orderdate) sortedfrom business) twhere sorted = 1;\n\n8.2.6 Rank1）函数说明\nRANK() 排序相同时会重复，总数不会变\nDENSE_RANK() 排序相同时会重复，总数会减少\nROW_NUMBER() 会根据顺序计算\n2）数据准备\n\n3）需求\n计算每门学科成绩排名。\n4）创建本地 score.txt****，导入数据\n[root@masterdatas]$ vi score.txt\n\n5）创建 hive 表并导入数据\ncreate table score(name string,subject string,score int)row format delimited fields terminated by \"\\t\";load data local inpath '/usr/local/soft/data/score.txt' into table score;\n\n6）按需求查询数据\nselect name,subject,score,rank() over(partition by subject order by score desc) rp,dense_rank() over(partition by subject order by score desc) drp,row_number() over(partition by subject order by score desc) rmpfrom score;\n\n扩展：求出每门学科前三名的学生？\n8.4 自定义函数****学习网址：https://www.cnblogs.com/sx66/p/12039552.html\n****1.****UDF：一进一出\n（1）创建maven项目，并加入依赖\n&lt;dependency&gt;&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;&lt;artifactId&gt;hive-exec&lt;/artifactId&gt;&lt;version&gt;1.2.1&lt;/version&gt;&lt;/dependency&gt;\n\n（2）编写代码，继承org.apache.hadoop.hive.ql.exec.UDF，实现evaluate方法，在evaluate方法中实现自己的逻辑\nimport org.apache.hadoop.hive.ql.exec.UDF;public class HiveUDF extends UDF {// hadoop =&gt; #hadoop$public String evaluate(String col1) {// 给传进来的数据 左边加上 # 号 右边加上 $String result = \"#\" + col1 + \"$\";return result;}}\n\n（3）打成jar包并上传至Linux虚拟机\n（4） 在hive shell中，使用 add jar 路径将jar包作为资源添加到hive环境中\nadd jar /usr/local/soft/jars/HiveUDF2-1.0.jar;\n\n（5）使用jar包资源注册一个临时函数，fxxx1是你的函数名，’MyUDF’是主类名\ncreate temporary function fxxx1 as 'MyUDF';\n\n（6）使用函数名处理数据\nselect fxx1(name) as fxx_name from students limit 10;#施笑槐$#吕金鹏$#单乐蕊$#葛德曜$#宣谷芹$#边昂雄$#尚孤风$#符半双$#沈德昌$#羿彦昌$\n\n****2.****UDTF：一进多出\nhive(default)&gt; select myudtf(\"hello,world,hadoop,hive\", \",\");helloworldhadoophive\n\n方法二：自定UDTF\n\n代码\n\npackage com.shujia.hive_function;import org.apache.hadoop.hive.ql.exec.UDFArgumentException;import org.apache.hadoop.hive.ql.metadata.HiveException;import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;import java.util.ArrayList;import java.util.List;public class HiveUDTF extends GenericUDTF {private ArrayList&lt;String&gt; outList = new ArrayList&lt;&gt;();@Overridepublic StructObjectInspector initialize(StructObjectInspector argOIs)throws UDFArgumentException {//1.定义输出数据的列名和类型List&lt;String&gt; fieldNames = new ArrayList&lt;&gt;();List&lt;ObjectInspector&gt; fieldOIs = new ArrayList&lt;&gt;();//2.添加输出数据的列名和类型fieldNames.add(\"lineToWord\");fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);returnObjectInspectorFactory.getStandardStructObjectInspector(fieldNames,fieldOIs);}@Overridepublic void process(Object[] args) throws HiveException {//1.获取原始数据String arg = args[0].toString();//2.获取数据传入的第二个参数，此处为分隔符String splitKey = args[1].toString();//3.将原始数据按照传入的分隔符进行切分String[] fields = arg.split(splitKey);//4.遍历切分后的结果，并写出for (String field : fields) {//集合为复用的，首先清空集合outList.clear();//将每一个单词添加至集合outList.add(field);//将集合内容写出forward(outList);}}@Overridepublic void close() throws HiveException {}}\n\n\nSQL\n\nselect my_udtf(\"key1:value1,key2:value2,key3:value3\");\n\n字段：id,col1,col2,col3,col4,col5,col6,col7,col8,col9,col10,col11,col12 共13列数据：a,1,2,3,4,5,6,7,8,9,10,11,12b,11,12,13,14,15,16,17,18,19,20,21,22c,21,22,23,24,25,26,27,28,29,30,31,32转成3列：id,hours,value例如：a,1,2,3,4,5,6,7,8,9,10,11,12a,0时,1a,2时,2a,4时,3a,6时,4\n\ncreate table udtfData(id string,col1 string,col2 string,col3 string,col4 string,col5 string,col6 string,col7 string,col8 string,col9 string,col10 string,col11 string,col12 string)row format delimited fields terminated by ',';\n\n代码：\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentException;import org.apache.hadoop.hive.ql.metadata.HiveException;import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;import java.util.ArrayList;public class HiveUDTF2 extends GenericUDTF {@Overridepublic StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException {ArrayList&lt;String&gt; filedNames = new ArrayList&lt;String&gt;();ArrayList&lt;ObjectInspector&gt; fieldObj = new ArrayList&lt;ObjectInspector&gt;();filedNames.add(\"col1\");fieldObj.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);filedNames.add(\"col2\");fieldObj.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);return ObjectInspectorFactory.getStandardStructObjectInspector(filedNames, fieldObj);}public void process(Object[] objects) throws HiveException {int hours = 0;for (Object obj : objects) {hours = hours + 1;String col = obj.toString();ArrayList&lt;String&gt; cols = new ArrayList&lt;String&gt;();cols.add(hours + \"时\");cols.add(col);forward(cols);}}public void close() throws HiveException {}}\n\n添加jar资源:\nadd jar /usr/local/soft/HiveUDF2-1.0.jar;\n\n注册udtf函数：\ncreate temporary function my_udtf as 'MyUDTF';\n\nSQL:\nselect id,hours,value from udtfData lateral view my_udtf(col1,col2,col3,col4,col5,col6,col7,col8,col9,col10,col11,col12) t as hours,value ;\n\n连续登陆问题\n在电商、物流和银行可能经常会遇到这样的需求：统计用户连续交易的总额、连续登陆天数、连续登陆开始和结束时间、间隔天数等\n数据：\n注意：每个用户每天可能会有多条记录\nid datestr amount1,2019-02-08,6214.231,2019-02-08,6247.321,2019-02-09,85.631,2019-02-09,967.361,2019-02-10,85.691,2019-02-12,769.851,2019-02-13,943.861,2019-02-14,538.421,2019-02-15,369.761,2019-02-16,369.761,2019-02-18,795.151,2019-02-19,715.651,2019-02-21,537.712,2019-02-08,6214.232,2019-02-08,6247.322,2019-02-09,85.632,2019-02-09,967.362,2019-02-10,85.692,2019-02-12,769.852,2019-02-13,943.862,2019-02-14,943.182,2019-02-15,369.762,2019-02-18,795.152,2019-02-19,715.652,2019-02-21,537.713,2019-02-08,6214.233,2019-02-08,6247.323,2019-02-09,85.633,2019-02-09,967.363,2019-02-10,85.693,2019-02-12,769.853,2019-02-13,943.863,2019-02-14,276.813,2019-02-15,369.763,2019-02-16,369.763,2019-02-18,795.153,2019-02-19,715.653,2019-02-21,537.71\n\n建表语句\ncreate table deal_tb(id string,datestr string,amount string)row format delimited fields terminated by ',';\n\n计算逻辑\n（1）先按用户和日期分组求和，使每个用户每天只有一条数据\nselect id,datestr,sum(amount) as sum_amountfrom deal_tbgroup by id,datestr\n\n（2）根据用户ID分组按日期排序，将日期和分组序号相减得到连续登陆的开始日期，如果开始日期相同说明连续登陆\nselect tt1.id,tt1.datestr,tt1.sum_amount,date_sub(tt1.datestr,rn) as grpfrom(select t1.id,t1.datestr,t1.sum_amount,row_number() over(partition by id order by datestr) as rnfrom( select id,datestr,sum(amount) as sum_amountfrom deal_tbgroup by id,datestr) t1) tt1\n\n（3）统计用户连续交易的总额、连续登陆天数、连续登陆开始和结束时间、间隔天数\nselect ttt1.id,ttt1.grp,round(sum(ttt1.sum_amount),2) as sc_sum_amount,count(1) as sc_days,min(ttt1.datestr) as sc_start_date,max(ttt1.datestr) as sc_end_date,datediff(ttt1.grp,lag(ttt1.grp,1) over(partition by ttt1.id order by ttt1.grp)) as iv_daysfrom(select tt1.id,tt1.datestr,tt1.sum_amount,date_sub(tt1.datestr,rn) as grpfrom(select t1.id,t1.datestr,t1.sum_amount,row_number() over(partition by id order by datestr) as rnfrom(select id,datestr,sum(amount) as sum_amountfrom deal_tbgroup by id,datestr) t1) tt1) ttt1group by ttt1.id,ttt1.grp;\n\n结果\n1 2019-02-07 13600.23 3 2019-02-08 2019-02-10 NULL1 2019-02-08 2991.650 5 2019-02-12 2019-02-16 11 2019-02-09 1510.8 2 2019-02-18 2019-02-19 11 2019-02-10 537.71 1 2019-02-21 2019-02-21 12 2019-02-07 13600.23 3 2019-02-08 2019-02-10 NULL2 2019-02-08 3026.649 4 2019-02-12 2019-02-15 12 2019-02-10 1510.8 2 2019-02-18 2019-02-19 22 2019-02-11 537.71 1 2019-02-21 2019-02-21 13 2019-02-07 13600.23 3 2019-02-08 2019-02-10 NULL3 2019-02-08 2730.04 5 2019-02-12 2019-02-16 13 2019-02-09 1510.8 2 2019-02-18 2019-02-19 13 2019-02-10 537.71 1 2019-02-21 2019-02-21 1\n\n第 9章 压缩和存储9.1 Hadoop 压缩配置9.1.1 MR 支持的压缩编码\n为了支持多种压缩&#x2F;解压缩算法，Hadoop 引入了编码&#x2F;解码器，如下表所示：\n\n压缩性能的比较：\nhttp://google.github.io/snappy/\nOn a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250\nMB&#x2F;sec or more and decompresses at about 500 MB&#x2F;sec or more.\n9.1.2 压缩参数配置要在 Hadoop 中启用压缩，可以配置如下参数（mapred-site.xml 文件中）：\n9.2 开启 Map 输出阶段压缩（MR** 引擎）**开启 map 输出阶段压缩可以减少 job 中 map 和 Reduce task 间数据传输量。具体配置如下：\n1）案例实操：\n（1）开启 hive 中间传输数据压缩功能\nhive (default)&gt;set hive.exec.compress.intermediate=true;\n\n（2）开启 mapreduce 中 map 输出压缩功能\nhive (default)&gt;set mapreduce.map.output.compress=true;\n\n（3）设置 mapreduce 中 map 输出数据的压缩方式\nhive (default)&gt;set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;\n\n（4）执行查询语句\nhive (default)&gt; select count(ename) name from emp;\n\n9.3 开启 Reduce 输出阶段压缩\n当 Hive 将 输 出 写 入 到 表 中 时 ， 输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为 true，来开启输出结果压缩功能。\n1）案例实操：\n（1）开启 hive 最终输出数据压缩功能\nhive (default)&gt;set hive.exec.compress.output=true;\n\n（2）开启 mapreduce 最终输出数据压缩\nhive (default)&gt;set mapreduce.output.fileoutputformat.compress=true;\n\n（3）设置 mapreduce 最终数据输出压缩方式\nhive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec =org.apache.hadoop.io.compress.SnappyCodec;\n\n（4）设置 mapreduce 最终数据输出压缩为块压缩\nhive (default)&gt; setmapreduce.output.fileoutputformat.compress.type=BLOCK;\n\n（5）测试一下输出结果是否是压缩文件\nhive (default)&gt; insert overwrite local directory'/opt/module/data/distribute-result' select * from emp distribute bydeptno sort by empno desc;\n\n9.4 文件存储格式Hive 支持的存储数据的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET\n9.4.1 列式存储和行式存储\n如图所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。\n1）行存储的特点\n查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。\n2）列存储的特点\n因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的\n数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。\nTEXTFILE 和 SEQUENCEFILE 的存储格式都是基于行存储的；\nORC 和 PARQUET 是基于列式存储的。\n9.4.2 TextFile 格式默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合 Gzip、Bzip2使用，但使用 Gzip 这种方式，hive 不会对数据进行切分，从而无法对数据进行并行操作。\n9.4.3 Orc 格式Orc (Optimized Row Columnar)是 Hive 0.11 版里引入的新的存储格式。\n如下图所示可以看到每个 Orc 文件由 1 个或多个 stripe 组成，每个stripe 一般为 HDFS的块大小，每一个 stripe 包含多条记录，这些记录按照列进行独立存储，对应到 Parquet中的 row group 的概念。每个 Stripe 里有三部分组成，分别是 Index Data，Row Data，Stripe Footer：\n\n1）Index Data：一个轻量级的 index，默认是每隔 1W 行做一个索引。这里做的索引应该只是记录某行的各字段在 Row Data 中的 offset。\n2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个 Stream 来存储。\n3）Stripe Footer：存的是各个 Stream 的类型，长度等信息。每个文件有一个 File Footer，这里面存的是每个 Stripe 的行数，每个 Column 的数据类型信息等；每个文件的尾部是一个 PostScript，这里面记录了整个文件的压缩类型以及FileFooter 的长度信息等。在读取文件时，会 seek 到文件尾部读 PostScript，从里面解析到File Footer 长度，再读 FileFooter，从里面解析到各个 Stripe 信息，再读各个 Stripe，即从后往前读\n9.4.4 Parquet 格式Parquet 文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的\n数据和元数据，因此 Parquet 格式文件是自解析的。\n（1）行组(Row Group)：每一个行组包含一定的行数，在一个 HDFS 文件中至少存储一\n个行组，类似于 orc 的 stripe 的概念。\n（2）列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。\n（3）页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。\n通常情况下，在存储 Parquet 数据的时候会按照 Block 大小设置行组的大小，由于一般 情况下每一个 Mapper 任务处理数据的最小单位是一个 Block，这样可以把每一个行组由一个 Mapper 任务处理，增大任务执行并行度。Parquet 文件的格式。\n\n上图展示了一个 Parquet 文件的内容，一个文件中可以存储多个行组，文件的首位都是 该文件的 Magic Code，用于校验它是否是一个 Parquet 文件，Footer length 记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的 Schema 信息。除了文件中每一个行组的元数据，每一 页的开始都会存储该页的元数据，在 Parquet 中，有三种类型的页：数据页、字典页和索引 页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前 Parquet 中还不支持索引页\n9.4.5 主流文件存储格式对比实验从存储文件的压缩比和查询速度两个角度对比。\n存储文件的压缩比测试：\n1）测试数据\nLog.txt 见文档\n2）TextFile\n（1）创建表，存储数据格式为 TEXTFILE\ncreate table log_text (track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by '\\t'stored as textfile;\n\n（2）向表中加载数据\nhive (default)&gt; load data local inpath '/opt/module/hive/datas/log.data'into table log_text ;\n\n（3）查看表中数据大小\nhive (default)&gt; dfs -du -h /user/hive/warehouse/log_text;\n\n18.13 M &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_text&#x2F;log.data\n3）ORC\n（1）创建表，存储数据格式为 ORC\ncreate table log_orc(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by '\\t'stored as orctblproperties(\"orc.compress\"=\"NONE\"); -- 设置 orc 存储不使用压缩\n\n（2）向表中加载数据\nhive (default)&gt; insert into table log_orc select * from log_text;\n\n（3）查看表中数据大小\nhive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc/ ;\n\n7.7 M &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_orc&#x2F;000000_0\n4）Parquet\n（1）创建表，存储数据格式为 parquet\ncreate table log_parquet(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by '\\t'stored as parquet;\n\n（2）向表中加载数据\nhive (default)&gt; insert into table log_parquet select * from log_text;\n\n（3）查看表中数据大小\nhive (default)&gt; dfs -du -h /user/hive/warehouse/log_parquet/;\n\n13.1 M &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_parquet&#x2F;000000_0\n存储文件的对比总结：\nORC &gt; Parquet &gt; textFile\n存储文件的查询速度测试：\n（1）TextFile\nhive (default)&gt; insert overwrite local directory'/opt/module/data/log_text' select substring(url,1,4) from log_text;\n\n（2）ORC\nhive (default)&gt; insert overwrite local directory'/opt/module/data/log_orc' select substring(url,1,4) from log_orc;\n\n（3）Parquet\nhive (default)&gt; insert overwrite local directory'/opt/module/data/log_parquet' select substring(url,1,4) fromlog_parquet;\n\n存储文件的查询速度总结：查询速度相近。\n9.5 存储和压缩结合9.5.1 测试存储和压缩官网： https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC\nORC 存储方式的压缩：\n注意：所有关于 ORCFile 的参数都是在 HQL 语句的 TBLPROPERTIES 字段里面出现\n1）创建一个 ZLIB 压缩的 ORC 存储方式\n（1）建表语句\ncreate table log_orc_zlib(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by '\\t'stored as orctblproperties(\"orc.compress\"=\"ZLIB\");\n\n（2）插入数据\ninsert into log_orc_zlib select * from log_text;\n\n（3）查看插入后数据\nhive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_zlib/ ;\n\n2.78 M &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_orc_none&#x2F;000000_0\n2）创建一个 SNAPPY 压缩的 ORC 存储方式\n（1）建表语句\ncreate table log_orc_snappy(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by '\\t'stored as orctblproperties(\"orc.compress\"=\"SNAPPY\");\n\n（2）插入数据 \ninsert into log_orc_snappy select * from log_text;\n\n（3）查看插入后数据\nhive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_snappy/;\n\n3.75 M &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_orc_snappy&#x2F;000000_0\nZLIB 比 Snappy 压缩的还小。原因是 ZLIB 采用的是 deflate 压缩算法。比 snappy 压缩的 压缩率高。\n3）创建一个 SNAPPY 压缩的 parquet 存储方式\n（1）建表语句\ncreate table log_parquet_snappy(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by '\\t'stored as parquettblproperties(\"parquet.compression\"=\"SNAPPY\");\n\n（2）插入数据\ninsert into log_parquet_snappy select * from log_text;\n\n（3）查看插入后数据\nhive (default)&gt; dfs -du -h /user/hive/warehouse/log_parquet_snappy/;\n\n6.39 MB &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F; log_parquet_snappy &#x2F;000000_0\n4）存储方式和压缩总结\n在实际的项目开发当中，hive 表的数据存储格式一般选择：orc 或 parquet。压缩方式一 般选择 snappy，lzo。\n第10章 企业级调优10.1 执行计划（Explain）1) 基本语法\nEXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query\n2）案例操作\n（1）查看下面这条语句的执行计划\n没有生成 MR 任务的\nhive (default)&gt; explain select * from emp;ExplainSTAGE DEPENDENCIES:Stage-0 is a root stageSTAGE PLANS:Stage: Stage-0Fetch Operatorlimit: -1Processor Tree:TableScanalias: empStatistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETEColumn stats: NONESelect Operatorexpressions: empno (type: int), ename (type: string), job(type: string), mgr (type: int), hiredate (type: string), sal (type:double), comm (type: double), deptno (type: int)outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5,_col6, _col7Statistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETEColumn stats: NONEListSink\n\n有生成 MR 任务的\nhive (default)&gt; explain select deptno, avg(sal) avg_sal from emp group bydeptno;ExplainSTAGE DEPENDENCIES:Stage-1 is a root stageStage-0 depends on stages: Stage-1STAGE PLANS:Stage: Stage-1Map ReduceMap Operator Tree:TableScanalias: empStatistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETEColumn stats: NONESelect Operatorexpressions: sal (type: double), deptno (type: int)outputColumnNames: sal, deptnoStatistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETEColumn stats: NONEGroup By Operatoraggregations: sum(sal), count(sal)keys: deptno (type: int)mode: hashoutputColumnNames: _col0, _col1, _col2Statistics: Num rows: 1 Data size: 7020 Basic stats:COMPLETE Column stats: NONEReduce Output Operatorkey expressions: _col0 (type: int)sort order: +Map-reduce partition columns: _col0 (type: int)Statistics: Num rows: 1 Data size: 7020 Basic stats:COMPLETE Column stats: NONEvalue expressions: _col1 (type: double), _col2 (type:bigint)Execution mode: vectorizedReduce Operator Tree:Group By Operatoraggregations: sum(VALUE._col0), count(VALUE._col1)keys: KEY._col0 (type: int)mode: mergepartialoutputColumnNames: _col0, _col1, _col2Statistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETEColumn stats: NONESelect Operatorexpressions: _col0 (type: int), (_col1 / _col2) (type: double)outputColumnNames: _col0, _col1Statistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETEColumn stats: NONEFile Output Operatorcompressed: falseStatistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETEColumn stats: NONEtable:input format:org.apache.hadoop.mapred.SequenceFileInputFormatoutput format:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormatserde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDeStage: Stage-0Fetch Operatorlimit: -1Processor Tree:ListSink\n\n（2）查看详细执行计划\nhive (default)&gt; explain extended select * from emp;hive (default)&gt; explain extended select deptno, avg(sal) avg_sal from empgroup by deptno;\n\n10.2 Fetch 抓取Fetch 抓取是指，Hive 中对某些情况的查询可以不必使用 MapReduce 计算。例如：SELECT * FROM employees;在这种情况下，Hive 可以简单地读取 employee 对应的存储目录下的文件，然后输出查询结果到控制台。\n在 hive-default.xml.template 文件中 hive.fetch.task.conversion 默认是 more，老版本 hive 默认是 minimal，该属性修改为 more 以后，在全局查找、字段查找、limit 查找等都不走mapreduce。\n&lt;property&gt;&lt;name&gt;hive.fetch.task.conversion&lt;/name&gt;&lt;value&gt;more&lt;/value&gt;&lt;description&gt;Expects one of [none, minimal, more].Some select queries can be converted to single FETCH task minimizinglatency.Currently the query should be single sourced not having any subqueryand should not have any aggregations or distincts (which incurs RS),lateral views and joins.0. none : disable hive.fetch.task.conversion1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only2. more : SELECT, FILTER, LIMIT only (support TABLESAMPLE andvirtual columns)&lt;/description&gt;&lt;/property\n\n1）案例实操：\n（1）把 hive.fetch.task.conversion 设置成 none，然后执行查询语句，都会执行 mapreduce程序。 \nhive (default)&gt; set hive.fetch.task.conversion=none;hive (default)&gt; select * from emp;hive (default)&gt; select ename from emp;hive (default)&gt; select ename from emp limit 3;\n\n（2）把 hive.fetch.task.conversion 设置成 more，然后执行查询语句，如下查询方式都不会执行 mapreduce 程序。\nhive (default)&gt; set hive.fetch.task.conversion=more;hive (default)&gt; select * from emp;hive (default)&gt; select ename from emp;hive (default)&gt; select ename from emp limit 3;\n\n10.3 本地模式大多数的 Hadoop Job 是需要 Hadoop 提供的完整的可扩展性来处理大数据集的。不过，有时 Hive 的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际 job 的执行时间要多的多。对于大多数这种情况，Hive 可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。\n用户可以通过设置 hive.exec.mode.local.auto 的值为 true，来让 Hive 在适当的时候自动启动这个优化。\nset hive.exec.mode.local.auto=true; //开启本地 mr//设置 local mr 的最大输入数据量，当输入数据量小于这个值时采用 local mr 的方式，默认为 134217728，即 128Mset hive.exec.mode.local.auto.inputbytes.max=50000000;//设置 local mr 的最大输入文件个数，当输入文件个数小于这个值时采用 local mr 的方式，默认为 4set hive.exec.mode.local.auto.input.files.max=10;\n\n1）案例实操：\n（2）关闭本地模式（默认是关闭的），并执行查询语句\nhive (default)&gt; select count(*) from emp group by deptno;\n\n10.4 表的优化10.4.1 小表大表 Join（MapJOIN）将 key 相对分散，并且数据量小的表放在 join 的左边，可以使用 map join 让小的维度表先进内存。在 map 端完成 join。\n实际测试发现：新版的 hive 已经对小表 JOIN 大表和大表 JOIN 小表进行了优化。小表放在左边和右边已经没有区别。\n案例实操\n1）需求介绍\n测试大表 JOIN 小表和小表 JOIN 大表的效率\n2）开启 MapJoin 参数设置\n（1）设置自动选择 Mapjoin\nset hive.auto.convert.join = true; 默认为 true\n\n（2）大表小表的阈值设置（默认 25M 以下认为是小表）：\nset hive.mapjoin.smalltable.filesize = 25000000;\n\n3）****MapJoin 工作机制\n\n4）建大表、小表和 JOIN 后表的语句\n// 创建大表create table bigtable(id bigint, t bigint, uid string, keyword string,url_rank int, click_num int, click_url string) row format delimitedfields terminated by '\\t';// 创建小表create table smalltable(id bigint, t bigint, uid string, keyword string,url_rank int, click_num int, click_url string) row format delimitedfields terminated by '\\t';// 创建 join 后表的语句create table jointable(id bigint, t bigint, uid string, keyword string,url_rank int, click_num int, click_url string) row format delimitedfields terminated by '\\t';\n\n5）分别向大表和小表中导入数据\nhive (default)&gt; load data local inpath '/opt/module/data/bigtable' intotable bigtable;hive (default)&gt;load data local inpath '/opt/module/data/smalltable' intotable smalltable;\n\n6）小表 JOIN 大表语句\ninsert overwrite table jointableselect b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_urlfrom smalltable sjoin bigtable bon b.id = s.id;\n\n7）大表 JOIN 小表语句\ninsert overwrite table jointableselect b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_urlfrom bigtable bjoin smalltable son s.id = b.id;\n\n10.4.2 大表 Join 大表1）空 KEY 过滤\n有时 join 超时是因为某些 key 对应的数据太多，而相同 key 对应的数据都会发送到相同的 reducer 上，从而导致内存不够。此时我们应该仔细分析这些异常的 key，很多情况下，这些 key 对应的数据是异常数据，我们需要在 SQL 语句中进行过滤。例如 key 对应的字段为空，操作如下：\n案例实操\n（1）配置历史服务器\n配置 mapred-site.xml\n&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;hadoop102:10020&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;&lt;value&gt;hadoop102:19888&lt;/value&gt;&lt;/property&gt;\n\n启动历史服务器\nsbin/mr-jobhistory-daemon.sh start historyserver\n\n（2）创建原始数据空 id 表\n// 创建空 id 表create table nullidtable(id bigint, t bigint, uid string, keyword string,url_rank int, click_num int, click_url string) row format delimitedfields terminated by '\\t';\n\n（3）分别加载原始数据和空 id 数据到对应表中\nhive (default)&gt; load data local inpath '/opt/module/data/nullid' intotable nullidtable;\n\n（4）测试不过滤空 id\nhive (default)&gt; insert overwrite table jointable select n.* fromnullidtable n left join bigtable o on n.id = o.id;\n\n（5）测试过滤空 id\nhive (default)&gt; insert overwrite table jointable select n.* from (select* from nullidtable where id is not null) n left join bigtable o on n.id = o.id;\n\n2）空 key 转换\n有时虽然某个 key 为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join 的结果中，此时我们可以表 a 中 key 为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的 reducer 上。例如：\n案例实操：\n不随机分布空 null 值：\n（1）设置 5 个 reduce 个数\nset mapreduce.job.reduces &#x3D; 5;\n（2）JOIN 两张表\ninsert overwrite table jointableselect n.* from nullidtable n left join bigtable b on n.id = b.id;\n\n结果：如下图所示，可以看出来，出现了数据倾斜，某些 reducer 的资源消耗远大于其他 reducer。\n随机分布空 null 值\n（1）设置 5 个 reduce 个数\nset mapreduce.job.reduces = 5;\n\n（2）JOIN 两张表\ninsert overwrite table jointableselect n.* from nullidtable n full join bigtable o onnvl(n.id,rand()) = o.id;\n\n结果：如下图所示，可以看出来，消除了数据倾斜，负载均衡 reducer 的资源消耗\n\n3）SMB(Sort Merge Bucket join)\n（1）创建第二张大表\ncreate table bigtable2(id bigint,t bigint,uid string,keyword string,url_rank int,click_num int,click_url string)row format delimited fields terminated by '\\t';load data local inpath '/opt/module/data/bigtable' into table bigtable2;\n\n测试大表直接 JOIN\ninsert overwrite table jointableselect b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_urlfrom bigtable sjoin bigtable2 bon b.id = s.id;\n\n（2）创建分通表 1,桶的个数不要超过可用 CPU 的核数\ncreate table bigtable_buck1(id bigint,t bigint,uid string,keyword string,url_rank int,click_num int,click_url string)clustered by(id)sorted by(id)into 6 bucketsrow format delimited fields terminated by '\\t';load data local inpath '/opt/module/data/bigtable' into tablebigtable_buck1;\n\n（3）创建分通表 2,桶的个数不要超过可用 CPU 的核数\ncreate table bigtable_buck2(id bigint,t bigint,uid string,keyword string,url_rank int,click_num int,click_url string)clustered by(id)sorted by(id)into 6 bucketsrow format delimited fields terminated by '\\t';load data local inpath '/opt/module/data/bigtable' into table\n\n（4）设置参数\nset hive.optimize.bucketmapjoin = true;set hive.optimize.bucketmapjoin.sortedmerge = true;sethive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\n\n（5）测试\ninsert overwrite table jointableselect b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_urlfrom bigtable_buck1 sjoin bigtable_buck2 bon b.id = s.id;\n\n10.4.3 Group By默认情况下，Map 阶段同一 Key 数据分发给一个 reduce，当一个 key 数据过大时就倾斜了。\n\n并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进行 部分聚合，最后在 Reduce 端得出最终结果。\n1）开启 Map 端聚合参数设置\n（1）是否在 Map 端进行聚合，默认为 True\nset hive.map.aggr = true\n\n（2）在 Map 端进行聚合操作的条目数目\nset hive.groupby.mapaggr.checkinterval = 100000\n\n（3）有数据倾斜的时候进行负载均衡（默认是 false）\nset hive.groupby.skewindata = true\n\n当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出 结果会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。\nhive (default)&gt; select deptno from emp group by deptno;Stage-Stage-1: Map: 1 Reduce: 5 Cumulative CPU: 23.68 sec HDFS Read:19987 HDFS Write: 9 SUCCESSTotal MapReduce CPU Time Spent: 23 seconds 680 msecOKdeptno102030\n\n优化以后\nhive (default)&gt; set hive.groupby.skewindata = true;hive (default)&gt; select deptno from emp group by deptno;Stage-Stage-1: Map: 1 Reduce: 5 Cumulative CPU: 28.53 sec HDFS Read:18209 HDFS Write: 534 SUCCESSStage-Stage-2: Map: 1 Reduce: 5 Cumulative CPU: 38.32 sec HDFS Read:15014 HDFS Write: 9 SUCCESSTotal MapReduce CPU Time Spent: 1 minutes 6 seconds 850 msecOKdeptno102030\n\n10.4.4 Count(Distinct) 去重统计数据量小的时候无所谓，数据量大的情况下，由于 COUNT DISTINCT 操作需要用一个Reduce Task 来完成，这一个 Reduce 需要处理的数据量太大，就会导致整个 Job 很难完成，一般 COUNT DISTINCT 使用先 GROUP BY 再 COUNT 的方式替换,但是需要注意 group by 造成的数据倾斜问题.\n1）案例实操\n（1）创建一张大表\nhive (default)&gt; create table bigtable(id bigint, time bigint, uid string,keywordstring, url_rank int, click_num int, click_url string) row formatdelimitedfields terminated by '\\t';\n\n（2）加载数据\nhive (default)&gt; load data local inpath '/opt/module/data/bigtable' intotable bigtable;\n\n（3）设置 5 个 reduce 个数\nset mapreduce.job.reduces = 5;\n\n（4）执行去重 id 查询\nhive (default)&gt; select count(distinct id) from bigtable;Stage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 7.12 sec HDFS Read:120741990 HDFS Write: 7 SUCCESSTotal MapReduce CPU Time Spent: 7 seconds 120 msecOKc0100001Time taken: 23.607 seconds, Fetched: 1 row(s)\n\n（5）采用 GROUP by 去重 id\nhive (default)&gt; select count(id) from (select id from bigtable group byid) a;Stage-Stage-1: Map: 1 Reduce: 5 Cumulative CPU: 17.53 sec HDFS Read:120752703 HDFS Write: 580 SUCCESSStage-Stage-2: Map: 1 Reduce: 1 Cumulative CPU: 4.29 sec2 HDFS Read:9409 HDFS Write: 7 SUCCESSTotal MapReduce CPU Time Spent: 21 seconds 820 msecOK_c0100001Time taken: 50.795 seconds, Fetched: 1 row(s)\n\n虽然会多用一个 Job 来完成，但在数据量大的情况下，这个绝对是值得的。\n10.4.5 笛卡尔积尽量避免笛卡尔积，join 的时候不加 on 条件，或者无效的 on 条件，Hive 只能使用 1 个reducer 来完成笛卡尔积。\n10.4.6 行列过滤列处理：在 SELECT 中，只拿需要的列，如果有分区，尽量使用分区过滤，少用 SELECT *。\n行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在 Where 后面，那么就会先全表关联，之后再过滤，比如：\n案例实操：\n1）测试先关联两张表，再用 where 条件过滤\nhive (default)&gt; select o.id from bigtable bjoin bigtable o on o.id = b.idwhere o.id &lt;= 10;\n\nTime taken: 34.406 seconds, Fetched: 100 row(s)\n2）通过子查询后，再关联表\nhive (default)&gt; select b.id from bigtable bjoin (select id from bigtable where id &lt;= 10) o on b.id = o.id;\n\nTime taken: 30.058 seconds, Fetched: 100 row(s)\n10.4.7 分区详见 7.1 章。\n10.4.8 分桶详见 7.2 章。\n10.5 合理设置 Map 及 Reduce 数1）通常情况下，作业会通过 input 的目录产生一个或者多个 map 任务。\n主要的决定因素有：input 的文件总个数，input 的文件大小，集群设置的文件块大小。\n2）是不是 map 数越多越好？\n答案是否定的。如果一个任务有很多小文件（远远小于块大小 128m），则每个小文件也会被当做一个块，用一个 map 任务来完成，而一个 map 任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的 map 数是受限的。\n3）是不是保证每个 map 处理接近 128m 的文件块，就高枕无忧了？\n答案也是不一定。比如有一个 127m 的文件，正常会用一个 map 去完成，但这个文件只针对上面的问题 2 和 3，我们需要采取两种方式来解决：即减少 map 数和增加 map 数；\n10.5.1 复杂文件增加 Map 数当 input 的文件都很大，任务逻辑复杂，map 执行非常慢的时候，可以考虑增加 Map 数，来使得每个 map 处理的数据量减少，从而提高任务的执行效率。\n增加 map 的方法为：根据\ncomputeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))&#x3D;blocksize&#x3D;128M 公式，\n调整 maxSize 最大值。让 maxSize 最大值低于 blocksize 就可以增加 map 的个数。\n案例实操：\n1）执行查询\nhive (default)&gt; select count(*) from emp;Hadoop job information for Stage-1: number of mappers: 1; number ofreducers: 1\n\n2）设置最大切片值为 100 个字节\nhive (default)&gt; set mapreduce.input.fileinputformat.split.maxsize=100;hive (default)&gt; select count(*) from emp;Hadoop job information for Stage-1: number of mappers: 6; number ofreducers: 1\n\n10.5.2 小文件进行合并1）在 map 执行前合并小文件，减少 map 数：CombineHiveInputFormat 具有对小文件进行合 并的功能（系统默认的格式）。HiveInputFormat 没有对小文件合并功能。\nset hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n\n2）在 Map-Reduce 的任务结束时合并小文件的设置：\n在 map-only 任务结束时合并小文件，默认 true\nSET hive.merge.mapfiles = true;\n\n在 map-reduce 任务结束时合并小文件，默认 false\nSET hive.merge.mapredfiles = true;\n\n合并文件的大小，默认 256M\nSET hive.merge.size.per.task = 268435456;\n\n当输出文件的平均大小小于该值时，启动一个独立的 map-reduce 任务进行文件 merge\nSET hive.merge.smallfiles.avgsize = 16777216;\n\n10.5.3 合理设置 Reduce 数1）调整 reduce 个数方法一\n（1）每个 Reduce 处理的数据量默认是 256MB\nhive.exec.reducers.bytes.per.reducer=256000000\n\n（2）每个任务最大的 reduce 数，默认为 1009\nhive.exec.reducers.max=1009\n\n（3）计算 reducer 数的公式\nN=min(参数 2，总输入数据量/参数 1)\n\n2）调整 reduce 个数方法二\n在 hadoop 的 mapred-default.xml 文件中修改\nset mapreduce.job.reduces = 15;\n\n设置每个 job 的 Reduce 个数\n3）reduce 个数并不是越多越好\n（1）过多的启动和初始化 reduce 也会消耗时间和资源；\n（2）另外，有多少个 reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；\n在设置 reduce 个数的时候也需要考虑这两个原则：处理大数据量利用合适的 reduce 数；使单个 reduce 任务处理数据量大小要合适；\n10.6 并行执行Hive 会将一个查询转化成一个或者多个阶段。这样的阶段可以是 MapReduce 阶段、抽样阶段、合并阶段、limit 阶段。或者 Hive 执行过程中可能需要的其他阶段。默认情况下，Hive 一次只会执行一个阶段。不过，某个特定的 job 可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个 job 的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么 job 可能就越快完成。\n通过设置参数 hive.exec.parallel 值为 true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果 job 中并行阶段增多，那么集群利用率就会增加。\nset hive.exec.parallel=true; //打开任务并行执行set hive.exec.parallel.thread.number=16; //同一个 sql 允许最大并行度，默认为8。\n\n当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。\n10.7 严格模式Hive 可以通过设置防止一些危险操作：\n1）分区表不使用分区过滤\n将 hive.strict.checks.no.partition.filter 设置为 true 时，对于分区表，除非 where 语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。\n2）使用 order by 没有 limit 过滤\n将 hive.strict.checks.orderby.no.limit 设置为 true 时，对于使用了 order by 语句的查询，要 求必须使用 limit 语句。因为 order by 为了执行排序过程会将所有的结果数据分发到同一个Reducer 中进行处理，强制要求用户增加这个 LIMIT 语句可以防止 Reducer 额外执行很长一段时间。\n3）笛卡尔积\n将 hive.strict.checks.cartesian.product 设置为 true 时，会限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在 执行 JOIN 查询的时候不使用 ON 语句而是使用 where 语句，这样关系数据库的执行优化器就可以高效地将 WHERE 语句转化成那个 ON 语句。不幸的是，Hive 并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。\n10.8 JVM 重用10.9 压缩Article link： https://tqgoblin.site/post/csdn/Hive详解/  Author： Stephen  \n","slug":"csdn/Hive详解","date":"2021-02-06T03:50:49.000Z","categories_index":"大数据","tags_index":"hive","author_index":"Stephen"},{"id":"f7e86999f1d4e7aaf5b2a2d3ab660579","title":"Hadoop（部署篇）","content":"Hadoop三种运行模式Hadoop 运行模式包括：本地模式、伪分布式模式以及完全分布式模式。\nHadoop 官方网站：http://hadoop.apache.org/\n本地运行模式1. 官方 Grep 案例\n①创建在 hadoop-2.7.6 文件下面创建一个 input 文件夹：\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# mkdir input\n②将 Hadoop 的 xml 配置文件复制到 input\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# cp etc&#x2F;hadoop&#x2F;*.xml input\n③执行 share 目录下的 MapReduce 程序\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.7.6.jar grep input output ‘dfs[a-z.]+’\n④查看输出结果：\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# cat output&#x2F;*\n伪分布式运行模式**1.**启动 HDFS 并运行 MapReduce 程序\n（1）分析\n        ①配置集群\n        ②启动、测试集群增、删、查\n        ③执行 WordCount 案例\n（2）执行步骤\n        ①配置集群\n（a）配置：hadoop-env.sh\n主节点中，获取 jdk 的路径：\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# echo $JAVA_HOME\n编辑 hadoop-env.sh，并添加 jdk 路径：\n\n\n\n\n\n\n\n\n\n[root@master hadoop]# vim hadoop-env.sh\nexport JAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;soft&#x2F;jdk1.8.0_171\n\n（b）配置：core-site.xml\n\n\n\n\n\n\n\n\n\n&lt;!– 指定 HDFS 中 NameNode 的地址 –&gt;\n&lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;\n        &lt;value&gt;hdfs:&#x2F;&#x2F;master:9000&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;!– 指定 Hadoop 运行时产生文件的存储目录 –&gt;\n&lt;property&gt;\n        &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;\n        &lt;value&gt;&#x2F;usr&#x2F;local&#x2F;soft&#x2F;hadoop-2.7.6&#x2F;tmp&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n（c）配置：hdfs-site.xml\n\n\n\n\n\n\n\n\n\n&lt;!– 指定 HDFS 副本的数量 –&gt;\n&lt;property&gt;\n        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;\n        &lt;value&gt;1&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n②启动集群\n（a）格式化 NameNode（第一次启动时格式化，以后就不要总格式化）\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# hadoop namenode -format\n \n（b）启动 NameNode\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# hadoop-daemon.sh start namenode\n \n（c）启动 DataNode\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# hadoop-daemon.sh start datanode\n③查看集群\n（a）查看是否启动成功\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# jps\n\n\n\n\n\n\n\n\n\n7461 NameNode\n7559 DataNode\n7641 Jps\n（b）web 端查看 HDFS 文件系统\n\n\n\n\n\n\n\n\n\nhttp://master:50070\n \n（c）查看产生的 Log 日志\n说明：在企业中遇到 Bug 时，经常根据日志提示信息去分析问题、解决 Bug。\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# cd logs&#x2F;\n[root@master logs]# ll\n\n（d）思考：为什么不能一直格式化 NameNode，格式化 NameNode，要注意什\n么？\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# cd tmp&#x2F;dfs&#x2F;name&#x2F;current\n[root@master current]# cat VERSION\n#Mon Jul 11 10:27:14 CST 2022\nnamespaceID&#x3D;1179648118\nclusterID&#x3D;CID-92721281-f46b-419c-bab2-e23edc300e06\ncTime&#x3D;0\nstorageType&#x3D;NAME_NODE\nblockpoolID&#x3D;BP-1720451217-192.168.18.133-1657506434948\nlayoutVersion&#x3D;-63\n[root@master data]# cd current&#x2F;\n[root@master current]# ll\n总用量 4\ndrwx——. 4 root root 54 7 月 11 10:30 BP-1720451217-192.168.18.133-1657506434948\n-rw-r–r–. 1 root root 229 7 月 11 10:30 VERSION\n[root@master current]# cat VERSION\n#Mon Jul 11 10:30:12 CST 2022\nstorageID&#x3D;DS-7efd22c8-fc53-4e03-beae-6699e4398181\nclusterID&#x3D;CID-92721281-f46b-419c-bab2-e23edc300e06\ncTime&#x3D;0\ndatanodeUuid&#x3D;5774abb7-ab49-4740-8747-a6923bd17d8e\nstorageType&#x3D;DATA_NODE\nlayoutVersion&#x3D;-56\n注意：格式化 NameNode，会产生新的集群 id,导致 NameNode 和 DataNode的集群 id 不一致，集群找不到已往数据。所以，格式 NameNode 时，一定要先删除 data 数据和 log 日志，然后再格式化 NameNode。\n ④操作集群\n（a）在 HDFS 文件系统上创建一个 input 文件夹\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# hadoop dfs -mkdir &#x2F;input\n（b）将测试文件内容上传到文件系统上\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# hadoop dfs -put wordcountinput&#x2F;wc.input &#x2F;input\n（c）查看上传的文件是否正确\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# hadoop dfs -ls &#x2F;input\n[root@master hadoop-2.7.6]# hadoop dfs -cat &#x2F;input&#x2F;wc.input\n（d）运行 MapReduce 程序\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.7.6.jar wordcount &#x2F;input &#x2F;output\n（e）查看输出结果\n命令行查看：\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# hadoop dfs -cat &#x2F;output&#x2F;*\n 2. 启动 YARN 并运行 MapReduce 程序\n（1）分析\n①配置集群在 YARN 上运行 MR\n②启动、测试集群增、删、查\n③在 YARN 上执行 WordCount 案例\n（2）执行步骤\n①配置集群\n（a）配置 yarn-env.sh\n配置一下 JAVA_HOME\n\n\n\n\n\n\n\n\n\nexport JAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;soft&#x2F;jdk1.8.0_171\n（b）配置 yarn-site.xml\n\n\n\n\n\n\n\n\n\n&lt;!– 指定 YARN 的 ResourceManager 的地址 –&gt;\n&lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt;\n        &lt;value&gt;master&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;!– Reducer 获取数据的方式 –&gt;\n&lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n（c）配置：mapred-env.sh\n配置一下 JAVA_HOME\n\n\n\n\n\n\n\n\n\nexport JAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;soft&#x2F;jdk1.8.0_171\n（d）配置： (对 mapred-site.xml.template 重新命名为) mapred-site.xml\n\n\n\n\n\n\n\n\n\n[root@master hadoop]# mv mapred-site.xml.template mapred-site.xml\n[root@master hadoop]# vim mapred-site.xml\n&lt;!– 指定 MR 运行在 YARN 上 –&gt;\n&lt;property&gt;\n        &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;\n        &lt;value&gt;yarn&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n②启动集群\n（a）启动前必须保证 NameNode 和 DataNode 已经启动\n（b）启动 ResourceManager\n\n\n\n\n\n\n\n\n\n[root@master hadoop]# yarn-daemon.sh start resourcemanager\n（c）启动 NodeManager\n\n\n\n\n\n\n\n\n\n[root@master hadoop]# yarn-daemons.sh start nodemanager\n③集群操作\n（a）YARN 的浏览器页面查看：http://master:8088/cluster\n\n（b）执行 MapReduce 程序：\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.7.6.jar wordcount &#x2F;input &#x2F;output\n（c）查看运行结果：\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# hadoop dfs -cat &#x2F;output&#x2F;*\n \n**3.**配置历史服务器\n为了查看程序的历史运行情况，需要配置一下历史服务器。具体配置步骤如\n下：\n1. 配置 mapred-site.xml\n在该文件里面增加如下配置。\n\n\n\n\n\n\n\n\n\n&lt;!– 历史服务器端地址 –&gt;\n&lt;property&gt;\n        &lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt;\n        &lt;value&gt;master:10020&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;!– 历史服务器 web 端地址 –&gt;\n&lt;property&gt;\n        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;&#x2F;name&gt;\n        &lt;value&gt;master:19888&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n2. 启动历史服务器\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# mr-jobhistory-daemon.sh start historyserver\n3. 查看历史服务器是否启动\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# jps\n10016 Jps\n9234 NodeManager\n7461 NameNode\n7559 DataNode\n9948 JobHistoryServer\n8941 ResourceManager\n4. 查看 JobHistory\n\n\n\n\n\n\n\n\n\nhttp://master:19888/jobhistory\n\n4. 配置日志的聚集\n日志聚集概念：应用运行完成以后，将程序运行日志信息上传到 HDFS 系统上。\n日志聚集功能好处：可以方便的查看到程序运行详情，方便开发调试。\n注意：开启日志聚集功能，需要重新启动 NodeManager 、ResourceManager和 HistoryManager。\n开启日志聚集功能具体步骤如下：\n1. 配置 yarn-site.xml\n\n\n\n\n\n\n\n\n\n[root@master hadoop]# vim yarn-site.xml\n在该文件里面增加如下配置。\n\n\n\n\n\n\n\n\n\n&lt;!– 日志聚集功能使能 –&gt;\n&lt;property&gt;\n        &lt;name&gt;yarn.log-aggregation-enable&lt;&#x2F;name&gt;\n        &lt;value&gt;true&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;!– 日志保留时间设置 7 天 –&gt;\n&lt;property&gt;\n        &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;&#x2F;name&gt;\n        &lt;value&gt;604800&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n2. 关闭 NodeManager 、ResourceManager 和 HistoryManager\n\n\n\n\n\n\n\n\n\n[root@master hadoop]# yarn-daemon.sh stop resourcemanager\n[root@master hadoop]# yarn-daemons.sh stop nodemanager\n[root@master hadoop]# mr-jobhistory-daemon.sh stop historyserver\n3. 启动 NodeManager 、ResourceManager 和 HistoryManager\n[root@master hadoop]# yarn-daemon.sh start resourcemanager\n[root@master hadoop]# yarn-daemon.sh start nodemanager\n[root@master hadoop]# mr-jobhistory-daemon.sh start historyserver\n4. 删除 HDFS 上已经存在的输出文件\n\n\n\n\n\n\n\n\n\n[root@master hadoop]# hadoop dfs -rm -r &#x2F;output\n5. 执行 WordCount 程序\n\n\n\n\n\n\n\n\n\n[root@master hadoop-2.7.6]# hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.7.6.jar wordcount &#x2F;input &#x2F;output\n6. 查看日志\n\n\n\n\n\n\n\n\n\nhttp://master:19888/jobhistory\n \n\n5. 配置文件说明\nHadoop 配置文件分两类：默认配置文件和自定义配置文件，只有用户想修\n改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。\n（1）默认配置文件\n \n（2）自定义配置文件：\ncore-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml 四个\n配置文件存放在$HADOOP_HOME&#x2F;etc&#x2F;hadoop 这个路径上，用户可以根据项目需求\n重新进行修改配置。\n完全分布式运行模式（开发重点）1. 准备工作\n（1）设备\n三台虚拟机：master、node1、node2\n（2）时间同步\ndate\n（3）修改主机名\n三台主机名分别是：master、node1、node2\n（4）关闭防火墙\n\n\n\n\n\n\n\n\n\nsystemctl stop firewalld\n（5）查看防火墙状态\n\n\n\n\n\n\n\n\n\nsystemctl status firewalld\n（6）取消防火墙自启\n\n\n\n\n\n\n\n\n\nsystemctl disable firewalld\n（7）免密登录\n三台都需要免密：\n\n\n\n\n\n\n\n\n\n# 1、生成密钥\nssh-keygen -t rsa\n# 2、配置免密登录\nssh-copy-id master\nssh-copy-id node1\nssh-copy-id node2\n# 3、测试免密登录\nssh node1\n2. 搭建 Hadoop 集群\n（1）上传安装包并解压\n①使用 xftp 上传压缩包至 master 的&#x2F;usr&#x2F;local&#x2F;packages&#x2F;\n②解压\ntar -zxvf hadoop-2.7.6.tar.gz -C &#x2F;usr&#x2F;local&#x2F;soft&#x2F;\n（2）配置环境变量\n\n\n\n\n\n\n\n\n\nvim &#x2F;etc&#x2F;profile\nJAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;soft&#x2F;jdk1.8.0_171\nHADOOP_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;soft&#x2F;hadoop-2.7.6\nexport PATH&#x3D;$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin:$PATH\n# 重新加载环境变量\nsource &#x2F;etc&#x2F;profile\n（3）修改 Hadoop 配置文件\n①进入 hadoop 的配置文件\n\n\n\n\n\n\n\n\n\ncd &#x2F;usr&#x2F;local&#x2F;soft&#x2F;hadoop-2.7.6&#x2F;etc&#x2F;hadoop&#x2F;\n②core-site.xml\n\n\n\n\n\n\n\n\n\n&lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;\n        &lt;value&gt;hdfs:&#x2F;&#x2F;master:9000&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n        &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;\n        &lt;value&gt;&#x2F;usr&#x2F;local&#x2F;soft&#x2F;hadoop-2.7.6&#x2F;tmp&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n        &lt;name&gt;fs.trash.interval&lt;&#x2F;name&gt;\n        &lt;value&gt;1440&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n③hadoop-env.sh\n\n\n\n\n\n\n\n\n\nexport JAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;soft&#x2F;jdk1.8.0_171\n\n④hdfs-site.xml\n\n\n\n\n\n\n\n\n\n&lt;property&gt;\n        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;\n        &lt;value&gt;1&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n        &lt;name&gt;dfs.permissions&lt;&#x2F;name&gt;\n        &lt;value&gt;false&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n⑤mapred-site.xml.template\n重命名文件：\n\n\n\n\n\n\n\n\n\ncp mapred-site.xml.template mapred-site.xml\nvim mapred-site.xml\n&lt;property&gt;\n        &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;\n        &lt;value&gt;yarn&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n        &lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt;\n        &lt;value&gt;master:10020&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;&#x2F;name&gt;\n        &lt;value&gt;master:19888&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n⑥slaves\n\n\n\n\n\n\n\n\n\nnode1\nnode2\n⑦yarn-site.xml\n\n\n\n\n\n\n\n\n\n&lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt;\n        &lt;value&gt;master&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n        &lt;name&gt;yarn.log-aggregation-enable&lt;&#x2F;name&gt;\n        &lt;value&gt;true&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt;\n        &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;&#x2F;name&gt;\n        &lt;value&gt;604800&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n（4）分发 Hadoop 到 node1、node2\n\n\n\n\n\n\n\n\n\ncd &#x2F;usr&#x2F;local&#x2F;soft&#x2F;\nscp -r hadoop-2.7.6&#x2F; node1:`pwd`\nscp -r hadoop-2.7.6&#x2F; node2:`pwd`\n（5）格式化 namenode（第一次启动的时候需要执行）\n\n\n\n\n\n\n\n\n\nhdfs namenode -format\n\n（6）启动 Hadoop 集群\n\n\n\n\n\n\n\n\n\nstart-all.sh\n（7）检查 master、node1、node2 上的进程\n①master\n\n②node1 \n\n ③node2\n\n（8）访问 HDFS 的 WEB 界面\n\n\n\n\n\n\n\n\n\nhttp://master:50070\n\n（9）访问 YARN 的 WEB 界面\n\n\n\n\n\n\n\n\n\nhttp://master:8088/cluster\n\n3.scp&amp;rsync 的使用\n（1） scp（secure copy）安全拷贝\n①scp 定义：\nscp 可以实现服务器与服务器之间的数据拷贝。（from server1 to server2）\n②基本语法\nscp -r &#x2F;usr&#x2F;local&#x2F;soft&#x2F;data&#x2F; node1:`pwd`\n命令 递归 要拷贝的文件路径&#x2F;名称 目的用户@主机:目的路径 &#x2F;名称\n③案例实操\n在 master 上，将 master 中&#x2F;usr&#x2F;local&#x2F;soft&#x2F;下的 data&#x2F;目录拷贝到 node1\n中的 soft 目录下：\n\n\n\n\n\n\n\n\n\n[root@master soft]# scp -r data&#x2F; node1:`pwd`\n（2）rsync 远程同步工具\nrsync 主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。\nrsync 和 scp 区别：用 rsync 做文件的复制要比 scp 的速度快，rsync 只对差异文件做更新。scp 是把所有文件都复制过去。\n①基本语法\n[root@master soft]# rsync -rvl data&#x2F; node2:`pwd`\n命令 选项参数 要拷贝的文件路径&#x2F;名称 目的用户@主机:目的路径&#x2F;名称\n选项参数说明:\n选项 \n②案例实操\n把 master 机器上的&#x2F;usr&#x2F;local&#x2F;packages 目录同步到 node1 服务器的 root\n用户下的&#x2F;usr&#x2F;local&#x2F;packages 目录\n\n\n\n\n\n\n\n\n\n[root@master local]# rsync -rvl packages&#x2F;\nnode1:&#x2F;usr&#x2F;local&#x2F;packages&#x2F;\nArticle link： https://tqgoblin.site/post/csdn/Hadoop（部署篇）/  Author： Stephen  \n","slug":"csdn/Hadoop（部署篇）","date":"2021-02-05T15:07:30.000Z","categories_index":"大数据","tags_index":"hadoop","author_index":"Stephen"},{"id":"1008d683f9094b98f7665efd116e8351","title":"Hadoop 简介","content":"1、HDFS定义HDFS (Hadoop Distributed File System) ，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。\nHDFS 的使用场景：适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用。\n优缺点优点：\n（ 1 ）高容错性\n        ①数据自动保存多个副本。它通过增加副本的形式，提高容错性。\n        ②某一个副本丢失以后，它可以自动恢复。\n（2）适合处理大数据\n        ①数据规模：能够处理数据规模达到 GB 、 TB 、甚至 PB 级别的数据；\n        ②文件规模：能够处理百万规模以上的文件数量，数量相当之大。\n（3）可构建在廉价机器上，通过多副本机制，提高可靠性。\n缺点：\n（ 1 ）不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。\n（2）无法高效的对大量小文件进行存储。\n①存储大量小文件的话，它会占用 NameNode 大量的内存来存储文件目录和块信息。这\n样是不可取的，因为 NameNode 的内存总是有限的；\n②小文件存储的寻址时间会超过读取时间，它违反了 HDFS 的设计目标。\n（3）不支持并发写入、文件随机修改。\n        ①一个文件只能有一个写，不允许多个线程同时写；\n        ②仅支持数据 append ( 追加 ) ，不支持文件的随机修改。\n核心思想\n（ 1 ） NameNode (nn) ：就是 Master ， 它是一个主管、管理者。\n①管理 HDFS 的名称空间；\n        ②配置副本策略；\n③管理数据块(Block) 映射信息；\n        ④处理客户端读写请求。\n（2） DataNode ：就是 Slave 。 NameNode 下达命令， DataNode 执行实际的操作。\n        ①存储实际的数据块；\n        ③执行数据块的读&#x2F; 写操作。\n（3） Client ：就是客户端。\n①文件切分。文件上传 HDFS 的时候， Client 将文件切分成一个一个的 Block, 然后进行上传；\n②与 NameNode 交互，获取文件的位置信息；\n③与 DataNode 交互，读取或者写入数据；\n④Client 提供一些命令来管理 HDFS, 比如 NameNode 格式化；\n⑤Client 可以通过一些命令来访问 HDFS ，比如对 HDFS 增删查改操作；\n（4） Secondary NameNode ：并非 NameNode 的热备。当 NameNode 挂掉的时候， 它并不能马上替换 NameNode 并提供服务。\n        ①辅助 NameNode ，分 担其工 作量 ，比如 定期合 并 Fsimage 和 Edits, 并推送给NameNode ；\n        ②在紧急情况下，可辅助恢复 NameNode 。\n重点HDFS 中的文件在物理上是分块存储 (Block) ，块的大小可以通过配置参数 ( dfs.blocksize)来规定，默认大小在 Hadoop2.x 版本中是 128M ，老版本中是 64M 。\n\n思考：为什么块的大小不能设置太小，也不能设置太大？\n        (1) HDFS 的块设置太小，会增加寻址时间，程序一直在找块的开始位置；\n        (2)如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。\n总结 : HDFS 块的大小设置主要取决于磁盘传输速率。\n2、MapReduce定义MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完的分布式运算程序，并发运行在一个Hadoop集群上\n优缺点优点：\n（1）MapReduce易于编程它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。（2）良好的扩展性当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。\n（3）高容错性MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的。（4）适合PB级以上海量数据的离线处理可以实现上千台服务器集群并发工作，提供数据处理能力。\n缺点：\n（1）不擅长实时计算MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果（2）不擅长流式计算流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的不擅长DAG(有向图)计算\n（3）多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下\n核心思想\n（1 ）分布式的运算程序往往需要分成至少 2 个阶段。\n（2 ）第一个阶段的 MapTask 并发实例，完全并行运行，互不相干。\n（3 ）第二个阶段的 ReduceTask 并发实例互不相干，但是他们的数据依赖于上一个阶段的\n所有 MapTask 并发实例的输出。\n（4 ） MapReduce 编程模型只能包含一个 Map 阶段和一个 Reduce 阶段，如果用户的业务\n逻辑非常复杂，那就只能多个 MapReduce 程序，串行运行。\nMapReduce进程 \n执行流程3、Yarn定义yarn是一种通用的资源管理系统和调度平台。资源管理系统 ：管理集群内的硬件资源，和程序运行相关，比如内存，CPU等。调度平台：多个程序同时申请计算资源时提供分配，调度的规则（算法）。通用：不仅仅支持MapReduce程序，理论上支持各种计算程序如spark，flink。yarn不关系程序的计算内容，只关心程序所需的资源，在程序申请资源的时候根据调度算法分配资源，计算结束之后回收计算资源。使用yarn作为资源调度平台的计算框架自身需要提供ApplicationMaster来负责计算任务的调度。\n优缺点优点\n（1）将资源管理和作业控制分离，减小JobTracker压力\n（2）YARN的设计大大减小了 JobTracker（也就是现在的 ResourceManager）的资源消耗，并且让监测每一个 Job 子任务 (tasks) 状态的程序分布式化了，更安全、更优美。\n（3）老的框架中，JobTracker一个很大的负担就是监控job下的tasks的运行状况，现在，这个部分就扔给ApplicationMaster做了而ResourceManager中有一个模块叫做ApplicationsManager(ASM)，它负责监测ApplicationMaster的运行状况。\n（4）能够支持不同的计算框架\n（5）资源管理更加合理\n（6）使用Container对资源进行抽象，Container不同于MRv1中的slot，它是一个动态资源划分单位，是根据应用程序的需求动态生成的，比之前以slot数目更合理。\n（7）且使用了轻量级资源隔离机制Cgroups进行资源隔离。\n（8）Container的设计避免了之前的map slot&#x2F;reduce slot分开造成集群资源闲置的尴尬情况。\n 缺点：\n（1）各个应用无法感知集群整体资源的使用情况，只能等待上层调度推送信息。\n（2）资源分配采用轮询、ResourceOffer机制（mesos)，在分配过程中使用悲观锁，并发粒度小。\n（3）缺乏一种有效的竞争或优先抢占的机制。\n（4）简化了双层调度器中的全局资源管理器，改为由一个Cell State来记录集群内的资源使用情况，这些使用情况都是共享的数据，以此来达到与全局资源管理器相同的效果。\n（5）所有任务访问共享数据时，采用乐观并发控制方法。\n核心思想  \n步骤说明：1，客户端向ResourceManager中的ApplicationManager提交作业申请，申请jobID。2，ApplicationManager 返回一个jobID，以及运行的hdfs临时路径(hdfs:&#x2F;&#x2F;… jobID)。3，客户端将作业的jar包，配置信息等上传到分配的hdfs临时路径(hdfs:&#x2F;&#x2F;… jobID)中。4，客户端上传文件成功后，向ApplicationManager发送执行作业请求。5，ApplicationManager将请求转发给Scheduler，申请执行所需资源。6，调度器将作业放置到相关队列中，当执行到该作业时，开始让ApplicationManager分配Containers。7，ApplicationManager命令NodeManager使用分配的container资源启动ApplicationMaster。8，ApplicationMaster启动后去分配的hdfs临时路径(hdfs:&#x2F;&#x2F;… jobID)中读取作业的具体信息，根据分片信息创建map任务，reduce任务。9，ApplicationMaster向Scheduler请求资源来执行map任务，reduce任务。10，Scheduler返回申请结果。11，AppMaster通知NodeManager，启动map，reduce任务。12，NodeManager启动map，reduce任务。13，map，reduce任务读取数据，进行逻辑计算。计算过程中如果有map，reduce任务执行失败了，AppMaster负责重启任务。14，程序执行成功后，AppMaster向Scheduler发送请求，释放资源。\n摘抄部分原文链接：https://blog.csdn.net/weixin\\_43172032/article/details/117759068 \n调度策略  Yarn中有三种资源调度器：FIFO调度器（FIFO Scheduler）、容量调度器（Capacity Scheduler）、公平调度器（Fair Scheduler）。\n（1）FIFO调度器\n        简介：顾名思义，FIFO调度器把应用放在队列里，按照先进先出的提交顺序执行应用。\n        优点：简单，不需要额外配置。\n        缺点：不适合共享集群。大应用会占满整个集群的资源，导致小应用长时间等待。\n（2）容量调度器\n**        简介：**容量调度器设有一个专门的队列给小作业使用。\n        优点：大作业不会占满全部资源，小作业不需要长时间等待大作业完成。\n        缺点：整个集群的资源利用率降低了，大作业需要更长的时间来执行。\n（3）公平调度器\n**        简介：**公平调度器旨在为所有运行的应用公平地分配资源。\n       ** 优点：**同时解决了FIFO大作业占满整个集群资源的问题和Capacity小作业队列空闲导致集群资源利用率降低的问题。\n        缺点：存在延迟问题，后面的作业需要等待前面的作业让出资源。\n具体策略介绍请看：https://blog.csdn.net/m0_37795099/article/details/124211350\n三、Hadoop 生态圈Hadoop 生态圈是指围绕 Hadoop 软件框架为核心而出现的越来越多的相关软件框架，这些软件框架和 Hadoop 框架一起构成了一个生机勃勃的 Hadoop 生态圈。在特定场景下，Hadoop 有时也指代 Hadoop 生态圈。\nHadoop 生态圈的架构图\n\nArticle link： [https://tqgoblin.site/post/csdn/Hadoop 简介&#x2F;](https://tqgoblin.site/post/csdn/Hadoop 简介&#x2F;)  Author： Stephen  \n","slug":"csdn/Hadoop 简介","date":"2021-02-05T10:11:13.000Z","categories_index":"大数据","tags_index":"hadoop","author_index":"Stephen"},{"id":"ec8e41983198176e48b34bdb3d17a7d0","title":"虚拟机上搭建Linux环境","content":"一、虚拟机环境准备用虚拟机搭建三台linux环境\n**1.**虚拟机安装第一步：打开 VMware Workstation 软件，如下图所示：\n\n第二步：单击“** 创建虚拟机 **”，弹出“欢迎使用新建虚拟机向导”界面，选择“**典型（推荐）（ T） **”，再单击“** 下一步 **”，如下图所示：\n \n第三步：选择“** 稍后安装操作系统（ S） **”，再单击“** 下一步 **”，如\n下图所示：\n\n第四步：选择“** Linux(L) **”，再在版本(V)下拉框中选择“** CetOS64 位 **”，最后点击“**下一步 **”，如下图所示：\n \n第五步：在虚拟机名称(V)框中写入虚拟机名称（任意起），再选择安装位置， 最后单击“**下一步**”，如下图所示： \n第六步：最大磁盘大小建议“** 20GB 以上 **”，然后选择“** 将虚拟磁盘存 储为单个文件(O) **”,最后单击“**下一步**”，如下图所示： \n 第七步：单击“**完成**”，虚拟机创建完成。如下图所示：\n \n2.CentOS7 安装第一步：选择刚刚安装好的虚拟机，点击“** 虚拟机 **”再点击“** 设置 **”， 如下图所示：\n \n第二步：选择“**内存 **”，分配一个合适的内存大小，学习用建议“** 4GB **”即可，再选择“**处理器 **”，建议处理器数量选择“** 2 **”每个处理器的核心数量选择“**2 **”，再选择“** CD&#x2F;DVD **”，选择“** 使用 ISO 映像文件 **”找到 ISO 文件，最后选择“**网络适配器 **”，建议选择“** NAT 模式 **”，单击确定即可，如下图所示：\n \n\n\n\n第三步：开启虚拟机，并选择“** Install CentOS7 **”，进入安装界面，如下图所示：\n \n第四步：选择 “**中文**”，点击继续，如下图所示： \n第五步：点击** 安装位置 **，选择** 本地标准磁盘 **，点击完成；点击** 软件选择 **，选** GNOME 桌面 **，点击完成；\n \n\n 第六步：点击**开始安装**，来到用户设置界面，首先**设置 ROOT 密码**，其次**创建用户**，如下图所示：\n \n第七步：点击**重启**。\n\n第八步：重启后，点击** LICENSING **，** 接收许可 **；再点击** 系统 **，**连接网络 **，并** 设置主机名 **，最后点击完成配置，如下如所示：\n \n\n\n第九步：单击用户名，输入密码登录即可。\n\n\n3. CentOS7 安装后的配置（1）修改主机名\nbashvim /etc/hostname\n（2）关闭防火墙\nbashsystemctl stop firewalld（3）查看防火墙状态\nbashsystemctl status firewalld（4）取消防火墙自启\nbashsystemctl disable firewalld（5）静态 IP 配置\n①编辑网络配置文件:\nbashvim /etc/sysconfig/network-scripts/ifcfg-ens33\n\n\n\n\n\n\n\n\nTYPE&#x3D;EthernetBOOTPROTO&#x3D;staticHWADDR&#x3D;00:0C:29:B0:5F:08NAME&#x3D;ens33DEVICE&#x3D;ens33ONBOOT&#x3D;yesIPADDR&#x3D;192.168.159.101GATEWAY&#x3D;192.168.159.2NETMASK&#x3D;255.255.255.0DNS1&#x3D;192.168.40.2DNS2&#x3D;223.6.6.6\n需要修改：\nHWADDR（mac 地址）\n\n\nIPADDR（根据自己的网段，自定义 IP 地址），根据NAT模式的子网网段随便选择一个ip比如此图我选192.168.159.101\n\nGATEWAY（根据自己的网段填写对应的网关地址）根据上图的显示我的网段我设置 192.168.159.2\n②关闭 NetworkManager，并取消开机自启\nbashsystemctl stop NetworkManager\nsystemctl disable NetworkManager③重启网络服务\nbashsystemctl restart network（6）查看开机默认启动模式\nbashsystemctl get-default（7）修改开机为命令行模式\nbashsystemctl set-default multi-user.target（8）重启虚拟机\nbashreboot（9）添加映射关系\n①windows 下\n\n\n\n\n\n\n\n\n\n#C:\\Windows\\System32\\drivers\\etc\\hosts\n192.168.159.101 master\n192.168.159.102 node1\n192.168.159.103 node2\n②Linux 下\n\n\n\n\n\n\n\n\n\n#vim &#x2F;etc&#x2F;hosts\n192.168.159.101 master\n192.168.159.102 node1\n192.168.159.102 node2\n二、安装 JDK**1.**删除系统自带 JDK（1）查询是否安装 Java 软件\nbashrpm -qa|grep &quot;java&quot;（2）删除自带 JDK：\nbashrpm -e 文件名 --nodeps\n2. 安装 JDK（1）创建 packages、soft 目录\nbashcd /usr/local/\nmkdir packages\nmkdir soft（2）通过 xftp 上传 jdk\n\n（3）解压 jdk 至 soft 目录下\nbashtar -zxvf jdk-8u171-linux-x64.tar.gz -C /usr/local/soft/（4）配置环境变量\n\n\n\n\n\n\n\n\n\nvim &#x2F;etc&#x2F;profile\n#JDK\nexport JAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;soft&#x2F;jdk1.8.0_171\nexport PATH&#x3D;$JAVA_HOME&#x2F;bin:$PATH\n#使环境变量生效\nsource &#x2F;etc&#x2F;profile\n（5）验证\n \n三、虚拟机克隆1. 首先关闭 master 这台虚拟机，克隆两台虚拟机分别为：node1、node2。\n2. 点击**快照管理器** 选择**克隆**。\n\n3. 点击**下一页**。 \n4. 选择**虚拟机中的当前状态**，点击**下一页**。\n\n5.选择**创建完整克隆**，点击**下一页**。\n\n6.输入虚拟机名称以及虚拟机存放路径，点击**完成**。\n\n7. 像修改master节点一样分别修改 node1、node2 的主机名和网络。\nArticle link： https://tqgoblin.site/post/csdn/虚拟机上搭建Linux环境/  Author： Stephen  \n","slug":"csdn/虚拟机上搭建Linux环境","date":"2021-01-10T09:14:25.000Z","categories_index":"运维","tags_index":"linux linux","author_index":"Stephen"},{"id":"e42cdbfca7d3b003c339c6ab5827f117","title":"GitLab CI/CD 实现自动构建部署项目","content":"一、环境准备首先需要有一台 GitLab 服务器，然后需要有个项目；这里示例项目以 Spring Boot 项目为例，然后最好有一台专门用来 Build 的机器，实际生产中如果 Build 任务不频繁可适当用一些业务机器进行 Build。\n\nGitLab IP :10.88.9.21  (version 12.6.0)\nRunner IP :10.88.9.37  (version 12.5.0)\n\n二、GitLab CI 简介GitLab CI 是 GitLab 默认集成的 CI 功能，GitLab CI 通过在项目内 .gitlab-ci.yaml 配置文件读取 CI 任务并进行相应处理；GitLab CI 通过其称为 GitLab Runner 的 Agent 端进行 build 操作；Runner 本身可以使用多种方式安装，比如使用 Docker 镜像启动等；Runner 在进行 build 操作时也可以选择多种 build 环境提供者；比如直接在 Runner 所在宿主机 build、通过新创建虚拟机(vmware、virtualbox)进行 build等；同时 Runner 支持 Docker 作为 build 提供者，即每次 build 新启动容器进行 build；GitLab CI 其大致架构如下\n三、GitLab搭建安准基础依赖\n安装基础依赖\n\nbashsudo yum install -y curl policycoreutils-python openssh-server启动ssh服务&amp;设置为开机启动\nbashsudo systemctl enable sshd\nsudo systemctl start sshd \n安装Postfix\n\nPostfix是一个邮件服务器，GitLab发送邮件需要用到\nbash# 安装postfix\nsudo yum install -y postfixbash# 启动postfix并设置为开机启动\nsudo systemctl enable postfix\nsudo systemctl start postfix\n开放ssh以及http服务（80端口)\n\nbash# 开放ssh、http服务\nsudo firewall-cmd --add-service=ssh --permanent\nsudo firewall-cmd --add-service=http --permanentbash# 重载防火墙规则\nsudo firewall-cmd --reload安装GitLab本次我们部署的是社区版:gitlab-ce，如果要部署商业版可以把关键字替换为：gitlab-ee\n\nYum安装GitLab\n\nbash# 新建 /etc/yum.repos.d/gitlab-ce.repo，内容为\n[gitlab-ce]\nname=Gitlab CE Repository\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el$releasever/\ngpgcheck=0\nenabled=1\n# 再执行\nsudo yum makecache\nsudo yum install gitlab-ce配置GitLabGitLab默认的配置文件路径是&#x2F;etc&#x2F;gitlab&#x2F;gitlab.rb\nbash#修改配置文件\nsudo vim /etc/gitlab/gitlab.rb\n#配置首页地址（大约在第15行）可以用IP代替域名，这里根据自己需求来即可\nexternal_url &#39;http://gitlab.ecloud.com&#39;启动并访问GitLab\n启动GitLab\n\nbash#重新配置并启动\nsudo gitlab-ctl reconfigure\n#完成后将会看到如下输出\nRunning handlers complete\nChef Client finished, 432/613 resources updated in 03 minutes 43 seconds\ngitlab Reconfigured!\n访问 GitLab\n\n将设置的域名DNS解析到服务器IP，或者修改本地host将域名指向服务器IP。 访问：http://gitlab.ecloud.com \n\n设置密码\n\n这时候会提示为管理员账号设置密码。管理员账号默认username是root。 设置完成之后即可使用root账号登录，登陆后会进入欢迎界面。\n四、GitLab CI 配置增加 RunnerGitLab CI 在进行构建时会将任务下发给 Runner，让 Runner 去执行；所以先要添加一个 Runner\nbash# 新建 /etc/yum.repos.d/gitlab-runner.repo，内容为\n[gitlab-runner]\nname=gitlab-runner\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-runner/yum/el7\nrepo_gpgcheck=0\ngpgcheck=0\nenabled=1\ngpgkey=https://packages.gitlab.com/gpg.key\n# 再执行\nsudo yum makecache\nsudo yum install gitlab-runnergitlab-runner 注册\n首先要先获取gitlab-ci的Token:项目主页 -&gt; Sttings -&gt; CI/CD -&gt; Runners Expand \n\n登录安装runner的服务器，使用命令注册gitlab-runner:\n\n\nbashgitlab-runner register\n输入gitlab的服务URL，这个使用的是http://gitlab.ecloud.com\n输入gitlab-ci的Toekn，获取方式参考上图\n关于集成服务中对于这个runner的描述\n给这个gitlab-runner输入一个标记，这个tag非常重要，在后续的使用过程中需要使用这个tag来指定gitlab-runner\n是否运行在没有tag的build上面。在配置gitlab-ci的时候，会有很多job，每个job可以通过tags属性来选择runner。这里为true表示如果job没有配置tags，也执行\n是否锁定runner到当前项目\n选择执行器，gitlab-runner实现了很多执行器，可用在不同场景中运行构建，详情可见GitLab Runner Executors，这里选用Shell模式\n刷新页面就可以看到新增的一个Runner:\n\n\n 9.这个GitLabRunner就安装好了，下一步就是把项目集成到gitlab-ci中，开始持续集成了。\ngitlab-runner 使用\n创建 CI 配置文件一切准备就绪以后，就可以编写 CI 脚本了；GitLab 依靠读取项目根目录下的 .gitlab-ci.yml 文件来执行相应的 CI 操作；以下为测试项目的 .gitlab-ci.yml 配置\n\nyaml# These are the default stages. You don&#39;t need to explicitly define them. But you could define any stages you need.\nstages:\n- build\n- deploy\n- catLog\n- kill\n\nvariables:\nMAVEN_CLI_OPTS: &quot;-s .m2/settings.xml --batch-mode&quot;\nMAVEN_OPTS: &quot;-Dmaven.repo.local=/export/servers/repository_boot&quot;\nSHELL_NAME: &quot;ry.sh&quot;\ncache:\npaths:\n# - .m2/repository\n- target/\n\n# This is the name of the job. You can choose it freely.\nmaven_build:\n# A job is always executed within a stage. If no stage is set, it defaults to &#39;test&#39;\nstage: build\n# Since we require Maven for this job, we can restrict the job to runners with a certain tag. Of course, we need to configure a runner with the tag maven with a maven installation\ntags:\n- dev\n# 使用当前作业的名称和当前分支或标签（仅包括二进制文件目录）创建档案\nartifacts:\nname: &quot;$CI_JOB_NAME-$CI_COMMIT_REF_NAME&quot;\nexpire_in: 4 week\npaths:\n- target/*.jar\n# Here you can execute arbitrate terminal commands.\n# If any of the commands returns a non zero exit code the job fails\nscript:\n- echo &quot;Building project with maven&quot;\n- mvn $MAVEN_CLI_OPTS clean\n- mvn $MAVEN_CLI_OPTS install\ndeploy_jdk8:\nstage: deploy\ntags:\n- dev\nscript:\n- echo &quot;Deploy....&quot;\n- sh $SHELL_NAME restart\nwhen: on_success\ninterruptible: true\n\ncat_boot_log:\nstage: catLog\ntags:\n- dev\nscript:\n- echo &quot;cat  log....&quot;\n- sh $SHELL_NAME cat\nwhen: on_success\n\nkill_progress:\nstage: kill\ntags:\n- dev\nscript:\n- echo &quot;kill progress&quot;\n- sh $SHELL_NAME stop\nwhen: manual\n\n关于 .gitlab-ci.yml 具体配置更完整的请参考 官方文档\n\n\n\n\n\n\n\n\n\n文中测试项目:  点击下载\nArticle link： https://tqgoblin.site/post/gitlab-cicd/  Author： Stephen  \n","slug":"gitlab-cicd","date":"2019-12-22T02:48:00.000Z","categories_index":"运维","tags_index":"GitLab","author_index":"Stephen"},{"id":"77d3e41277be521fb04715814e170fee","title":"Activiti 学习笔记（二）","content":"一、Activiti 入门体验流程定义Palette（画板）在 eclipse 或 idea 中安装 activiti-designer 插件即可使用，画板中包括以下结点：\n\nConnection—连接\nEvent—事件\nTask—任务\nGateway—网关\nContainer—容器\nBoundary event—边界事件\nIntermediate event- -中间事件\n流程图设计完毕保存生成.bpmn 文件。\n\n新建流程 （IDEA）\n\n起完名字 holiday 后（默认扩展名为 bpmn），就可以看到进入了流程设计页面，如图所示：\n\n\n绘制流程左侧区域是绘图区，右侧区域是 palette 画板区域 ，鼠标先点击画板的元素即可在左侧绘图。\n指定流程定义Key流程定义 key 即流程定义的标识，在 idea 中通过 properties 视图查看流程的 key建议：相同的业务流程，流程定义的 key 名字定义一样，比如，如果需要创建新的业务流程，请假流程则使用新的 key。\n\n\n指定任务负责人在 properties 视图指定每个任务结点的负责人，比如下边是填写请假单的负责人为 zhangsan\n\n\n\n\n\n\n\n\n\n\n\n生成png流程图 参考 Idea创建bpmn文件没有png图片解决办法\n部署流程定义部署流程定义就是要将上边绘制的图形即流程定义（.bpmn）部署在工作流程引擎 activiti 中，方法如下：\njava    /**\n     * 流程定义的部署\n     * activiti表有哪些？\n     *  act_re_deployment  部署信息\n        act_re_procdef     流程定义的一些信息\n        act_ge_bytearray   流程定义的bpmn文件及png文件\n     */\n    public static void main(String[] args) &#123;\n        //1.创建ProcessEngine对象\n        ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine();\n\n        //2.得到RepositoryService实例\n        RepositoryService repositoryService = processEngine.getRepositoryService();\n\n        //3.进行部署\n        Deployment deployment = repositoryService.createDeployment()\n                .addClasspathResource(&quot;diagram/holiday.bpmn&quot;)  //添加bpmn资源\n                .addClasspathResource(&quot;diagram/holiday.png&quot;)\n                .name(&quot;请假申请单流程&quot;)\n                .deploy();\n\n        //4.输出部署的一些信息\n        System.out.println(deployment.getName());\n        System.out.println(deployment.getId());\n    &#125;启动一个流程实例流程定义部署在 activiti 后就可以通过工作流管理业务流程了，也就是说上边部署的请假申请流程可以使用了。针对该流程，启动一个流程表示发起一个新的请假申请单，这就相当于 java 类与 java 对象的关系，类定义好后需要 new 创建一个对象使用，当然可以 new 多个对象。对于请假申请流程，张三发起一个请假申请单需要启动一个流程实例，请假申请单发起一个请假单也需要启动一个流程实例。代码如下：\njava    /**\n    * 启动流程实例:\n    *     前提是先已经完成流程定义的部署工作\n    *\n    *     背后影响的表：\n    *       act_hi_actinst     已完成的活动信息\n         act_hi_identitylink   参与者信息\n         act_hi_procinst   流程实例\n         act_hi_taskinst   任务实例\n         act_ru_execution   执行表\n         act_ru_identitylink   参与者信息\n         act_ru_task  任务\n    */\npublic class ActivitiStartInstance &#123;\n    public static void main(String[] args) &#123;\n        //1.得到ProcessEngine对象\n        ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine();\n\n        //2.得到RunService对象\n        RuntimeService runtimeService = processEngine.getRuntimeService();\n\n        //3.创建流程实例  流程定义的key需要知道 holiday\n        ProcessInstance processInstance = runtimeService.startProcessInstanceByKey(&quot;holiday&quot;);\n\n\n        //4.输出实例的相关信息\n        System.out.println(&quot;流程部署ID&quot;+processInstance.getDeploymentId());//null\n        System.out.println(&quot;流程定义ID&quot;+processInstance.getProcessDefinitionId());//holiday:1:4\n        System.out.println(&quot;流程实例ID&quot;+processInstance.getId());//2501\n        System.out.println(&quot;活动ID&quot;+processInstance.getActivityId());//null\n\n    &#125;任务查询流程启动后，各各任务的负责人就可以查询自己当前需要处理的任务，查询出来的任务都是该用户的待办任务。\njava     //zhangsan完成自己任务列表的查询\n    public static void main(String[] args) &#123;\n        //1.得到ProcessEngine对象\n        ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine();\n\n        //2.得到TaskService对象\n        TaskService taskService = processEngine.getTaskService();\n\n        //3.根据流程定义的key,负责人assignee来实现当前用户的任务列表查询\n        Task task = taskService.createTaskQuery()\n                .processDefinitionKey(&quot;holiday&quot;)\n                .taskAssignee(&quot;zhangsan&quot;)\n                .singleResult();\n\n        //4.任务列表的展示\n        System.out.println(&quot;流程实例ID:&quot;+task.getProcessInstanceId()); //2501\n        System.out.println(&quot;任务ID:&quot;+task.getId());  //2505\n        System.out.println(&quot;任务负责人:&quot;+task.getAssignee()); //zhangsan\n        System.out.println(&quot;任务名称:&quot;+task.getName()); //填写请假申请单\n\n    &#125;\n任务处理任务负责人查询待办任务，选择任务进行处理，完成任务。\njava    /**\n     * zhangsan完成自己的任务\n     * 处理当前用户的任务\n     * 背后操作的表：\n     *   act_hi_actinst\n         act_hi_identitylink\n         act_hi_taskinst\n         act_ru_identitylink\n         act_ru_task\n     */\n    public static void main(String[] args) &#123;\n        //1.得到ProcessEngine对象\n        ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine();\n\n        //2.得到TaskService对象\n        TaskService taskService = processEngine.getTaskService();\n\n        //3.处理任务,结合当前用户任务列表的查询操作的话,任务ID:2505\n        taskService.complete(&quot;2505&quot;);\n        \n    &#125;流程定义删除java    /**\n     * 删除已经部署的流程定义\n     *\n     * 背后影响的表:\n     * act_ge_bytearray\n       act_re_deployment\n       act_re_procdef\n     */\npublic class DeleteProcessDefinition &#123;\n\n    /**\n     * 注意事项：\n     *     1.当我们正在执行的这一套流程没有完全审批结束的时候，此时如果要删除流程定义信息就会失败\n     *     2.如果公司层面要强制删除,可以使用repositoryService.deleteDeployment(&quot;1&quot;,true);\n     *     //参数true代表级联删除，此时就会先删除没有完成的流程结点，最后就可以删除流程定义信息  false的值代表不级联\n     *\n     * @param args\n     */\n    public static void main(String[] args) &#123;\n        //1.得到ProcessEngine对象\n        ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine();\n\n        //2.创建RepositoryService对象\n        RepositoryService repositoryService = processEngine.getRepositoryService();\n\n        //3.执行删除流程定义  参数代表流程部署的id\n        repositoryService.deleteDeployment(&quot;1&quot;);\n    &#125;\n&#125;\n\n\n\n\n\n\n\n\n说明：\n\n使用 repositoryService 删除流程定义\n如果该流程定义下没有正在运行的流程，则可以用普通删除。\n如果该流程定义下存在已经运行的流程，使用普通删除报错，可用级联删除方法将流程及相关记录全部删除。项目开发中使用级联删除的情况比较多，删除操作一般只开放给超级管理员使用。\n\n流程定义资源查询通过流程定义对象获取流程定义资源，获取 bpmn 和 png。\njava/**\n * 需求：\n * 1.从Activiti的act_ge_bytearray表中读取两个资源文件\n * 2.将两个资源文件保存到路径：   /Users/mac/Desktop/diagram/\n *\n * 技术方案：\n *     1.第一种方式使用actviti的api来实现\n *     2.第二种方式：其实就是原理层面，可以使用jdbc的对blob类型，clob类型数据的读取，并保存\n *        IO流转换，最好commons-io.jar包可以轻松解决IO操作\n *\n * 真实应用场景：用户想查看这个请假流程具体有哪些步骤要走？\n *\n *\n */\npublic class QueryBpmnFile &#123;\n\n    public static void main(String[] args) throws IOException &#123;\n        //1.得到ProcessEngine对象\n        ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine();\n\n        //2.得到RepositoryService对象\n        RepositoryService repositoryService = processEngine.getRepositoryService();\n\n        //3.得到查询器:ProcessDefinitionQuery对象\n        ProcessDefinitionQuery processDefinitionQuery = repositoryService.createProcessDefinitionQuery();\n\n        //4.设置查询条件\n        processDefinitionQuery.processDefinitionKey(&quot;holiday&quot;);//参数是流程定义的key\n\n        //5.执行查询操作,查询出想要的流程定义\n        ProcessDefinition processDefinition = processDefinitionQuery.singleResult();\n\n        //6.通过流程定义信息，得到部署ID\n        String deploymentId = processDefinition.getDeploymentId();\n\n        //7.通过repositoryService的方法,实现读取图片信息及bpmn文件信息(输入流)\n        //getResourceAsStream()方法的参数说明：第一个参数部署id,第二个参数代表资源名称\n        //processDefinition.getDiagramResourceName() 代表获取png图片资源的名称\n        //processDefinition.getResourceName()代表获取bpmn文件的名称\n        InputStream pngIs = repositoryService\n                .getResourceAsStream(deploymentId,processDefinition.getDiagramResourceName());\n        InputStream bpmnIs = repositoryService\n                .getResourceAsStream(deploymentId,processDefinition.getResourceName());\n\n        //8.构建出OutputStream流\n        OutputStream pngOs =\n                new FileOutputStream(new File(&quot;/Users/mac/Desktop/&quot;+processDefinition.getDiagramResourceName()));\n\n        OutputStream bpmnOs =\n                new FileOutputStream(new File(&quot;/Users/mac/Desktop/&quot;+processDefinition.getResourceName()));\n\n        //9.输入流，输出流的转换  commons-io-xx.jar中的方法\n        IOUtils.copy(pngIs,pngOs);\n        IOUtils.copy(bpmnIs,bpmnOs);\n        //10.关闭流\n        pngOs.close();\n        bpmnOs.close();\n        pngIs.close();\n        bpmnIs.close();\n\n    &#125;\n&#125;流程历史信息的查看即使流程定义已经删除了，流程执行的历史信息通过前面的分析，依然保存在 activiti 的 act_hi_*相关的表中。所以我们还是可以查询流程执行的历史信息，可以通过 HistoryService 来查看相关的历史记录。\njavapublic static void main(String[] args) throws IOException &#123;\n        //1.得到ProcessEngine对象\n        ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine();\n\n        //2.得到HistoryService\n        HistoryService historyService = processEngine.getHistoryService();\n\n        //3.得到HistoricActivitiInstanceQuery对象\n        HistoricActivityInstanceQuery historicActivityInstanceQuery = historyService.createHistoricActivityInstanceQuery();\n\n        historicActivityInstanceQuery.processInstanceId(&quot;10001&quot;);//设置流程实例的id\n\n        //4.执行查询\n        List&lt;HistoricActivityInstance&gt; list = historicActivityInstanceQuery\n                .orderByHistoricActivityInstanceStartTime().asc().list();//排序StartTime\n\n        //5.遍历查询结果\n        for (HistoricActivityInstance instance :list)&#123;\n            System.out.println(instance.getActivityId());\n            System.out.println(instance.getActivityName());\n            System.out.println(instance.getProcessDefinitionId());\n            System.out.println(instance.getProcessInstanceId());\n            System.out.println(&quot;=============================&quot;);\n        &#125;\n    &#125;Article link： https://tqgoblin.site/post/activiti学习笔记(二)/  Author： Stephen  \n","slug":"activiti学习笔记(二)","date":"2019-11-25T14:48:00.000Z","categories_index":"Java","tags_index":"Activiti","author_index":"Stephen"},{"id":"41fd5f0c4276a28984db38b1e679ed38","title":"Activiti 学习笔记（一）","content":"一、 Activiti流程引擎简介\n此项目于2010年Tom Bayen（jBPM创始人）离开jBoss后加入Alfresco公司之后的又一力作；第一版在2010年5月发布，当时仅支持最简单的流程处理，后来版本发布频率固定为每两个月一版。 值得一提的是参与项目开发的除了Tom Bayen和十几位核心开发人员之外还有其他公司的成员的参与，例如：SpringSource、MuleSoft、Salves、Signavio、FuseSource、NextLevel等公司。  \nActiviti是一个针对企业用户、开发人员、系统管理员轻量级工作流业务管理平台。其核心是使用Java开发的快速、稳定BPMN 2.0流程引擎。Activiti 是在 Apache V2 许可下发布的。Activiti可以运行在任何类型的Java程序中，例如服务器、集群、云服务。它可以完美地与Spring集成。计基于简约思想的设计使Activiti非常的轻量级。 它有着活跃的社区，而且越来越多的企业都选择Activiti作为自己的流程引擎或者嵌入到自己的系统平台中（例如ESB）。\n\n二、 Activiti 数据库支持Activiti的后台是有数据库的支持，所有的表都以ACT_开头。 第二部分是表示表的用途的两个字母标识。 用途也和服务的API对应。 \n\nACT_RE_*: ‘RE’表示repository。 这个前缀的表包含了流程定义和流程静态资源 （图片，规则，等等）。 \nACT_RU_*: ‘RU’表示runtime。 这些运行时的表，包含流程实例，任务，变量，异步任务，等运行中的数据。 Activiti只在流程实例执行过程中保存这些数据， 在流程结束时就会删除这些记录。 这样运行时表可以一直很小速度很快。 \nACT_ID_*: ‘ID’表示identity。 这些表包含身份信息，比如用户，组等等。 \nACT_HI_*: ‘HI’表示history。 这些表包含历史数据，比如历史流程实例， 变量，任务等等。 \nACT_GE_*: 通用数据， 用于不同场景下，如存放资源文件\n\n三、 Activiti 如何使用1. 部署ActivitiActiviti 是一个工作流引擎（其实就是一堆jar包 API），业务系统使用 activiti 来对系统的业务流程进行自动化管理，为了方便业务系统访问(操作)activiti 的接口或功能，通常将 activiti 环境与业务系统的环境集成在一起。\n2. 流程定义使用 activiti 流程建模工具(activity-designer)定义业务流程(.bpmn 文件) 。.bpmn 文件就是业务流程定义文件，通过 xml 定义业务流程。如果使用其它公司开发的工作作引擎一般都提供了可视化的建模工具(Process Designer)用于生成流程定义文件，建模工具操作直观，一般都支持图形化拖拽方式、多窗口的用户界面、丰富的过程图形元素、过程元素拷贝、粘贴、删除等功能。\n\n\n3. 流程定义部署向 activiti 部署业务流程定义（.bpmn 文件）。使用 activiti 提供的 api 向 activiti 中部署.bpmn 文件（一般情况还需要一块儿部署业务流程的图片.png）\n4. 启动一个流程实例(ProcessInstance)启动一个流程实例表示开始一次业务流程的运行，比如员工请假流程部署完成，如果张三要请假就可以启动一个流程实例，如果李四要请假也启动一个流程实例，两个流程的执行互相不影响，就好比定义一个 java 类，实例化两个对象一样，部署的流程就好比 java 类，启动一个流程实例就好比 new 一个 java 对象。\n5. 用户查询代办任务(Task)因为现在系统的业务流程已经交给 activiti 管理，通过 activiti 就可以查询当前流程执行到哪了，当前用户需要办理什么任务了，这些 activiti帮我们管理了，而不像上边需要我们在 sql语句中的where条件中指定当前查询的状态值是多少。\n6. 用户办理任务用户查询待办任务后，就可以办理某个任务，如果这个任务办理完成还需要其它用户办理，比如采购单创建后由部门经理审核，这个过程也是由 activiti 帮我们完成了，不需要我们在代码中硬编码指定下一个任务办理人了。\n7. 流程结束当任务办理完成没有下一个任务&#x2F;结点了，这个流程实例就完成了。\n四、 Activiti 服务架构图\n\n在新版本中， IdentityService，FormService 两个 Serivce 都已经删除了。\n1. activiti.cfg.xmlactiviti 的引擎配置文件，包括：ProcessEngineConfiguration 的定义、数据源定义、事务管理器等，此文件其实就是一个 spring 配置文件，下面是一个基本的配置只配置了 ProcessEngineConfiguration和数据源：\nxml&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;\n       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\n       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;\n       xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot;\n       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\n                        http://www.springframework.org/schema/contex http://www.springframework.org/schema/context/spring-context.xsd\n                        http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd&quot;&gt;\n\n    &lt;!--数据源配置dbcp--&gt;\n    &lt;bean id=&quot;dataSource&quot; class=&quot;org.apache.commons.dbcp.BasicDataSource&quot;&gt;\n        &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot; /&gt;\n        &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/activiti&quot; /&gt;\n        &lt;property name=&quot;username&quot; value=&quot;root&quot; /&gt;\n        &lt;property name=&quot;password&quot; value=&quot;root&quot; /&gt;\n    &lt;/bean&gt;\n    &lt;!--activiti单独运行的ProcessEngine配置对象(processEngineConfiguration),使用单独启动方式\n        默认情况下：bean的id=processEngineConfiguration\n    --&gt;\n    &lt;bean id=&quot;processEngineConfiguration&quot; class=&quot;org.activiti.engine.impl.cfg.StandaloneProcessEngineConfiguration&quot;&gt;\n    &lt;!--代表数据源--&gt;\n    &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt;\n    &lt;!--代表是否生成表结构--&gt;\n    &lt;property name=&quot;databaseSchemaUpdate&quot; value=&quot;true&quot;/&gt;\n    &lt;/bean&gt;\n&lt;/beans&gt;2. ProcessEngineConfiguration流程引擎的配置类，通过 ProcessEngineConfiguration 可以创建工作流引擎 ProceccEngine，常用的两种方法如下：\nStandaloneProcessEngineConfiguration通过org.activiti.engine.impl.cfg.StandaloneProcessEngineConfigurationActiviti 可以单独运行，使用它创建的 ProcessEngine，Activiti 会自己处理事务。\n配置文件方式：通常在 activiti.cfg.xml 配置文件中定义一个 id 为 processEngineConfiguration 的 bean，这里会使用 spring 的依赖注入来构建引擎。\nxml&lt;bean id=&quot;processEngineConfiguration&quot;\nclass=&quot;org.activiti.engine.impl.cfg.StandaloneProcessEngineConfig\nuration&quot;&gt;\n&lt;!-- 数据源 --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt;\n&lt;!-- 数据库策略 --&gt; &lt;property name=&quot;databaseSchemaUpdate&quot; value=&quot;true&quot;/&gt;\n&lt;/bean&gt;SpringProcessEngineConfiguration通过 org.activiti.spring.SpringProcessEngineConfiguration 与 Spring 整合。创建 spring 与 activiti 的整合配置文件：activity-spring.cfg.xml（名称不固定）\nxml&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;\nxmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\nxmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot;\nxmlns:context=&quot;http://www.springframework.org/schema/context&quot;\nxmlns:aop=&quot;http://www.springframework.org/schema/aop&quot;\nxmlns:tx=&quot;http://www.springframework.org/schema/tx&quot;\nxsi:schemaLocation=&quot;http://www.springframework.org/schema/beans \nhttp://www.springframework.org/schema/beans/spring-beans-3.1.xsd \nhttp://www.springframework.org/schema/mvc \nhttp://www.springframework.org/schema/mvc/spring-mvc-3.1.xsd \nhttp://www.springframework.org/schema/context \nhttp://www.springframework.org/schema/context/spring-context-3.1.xsd \nhttp://www.springframework.org/schema/aop \nhttp://www.springframework.org/schema/aop/spring-aop-3.1.xsd \nhttp://www.springframework.org/schema/tx \nhttp://www.springframework.org/schema/tx/spring-tx-3.1.xsd &quot;&gt;\n&lt;!-- 工作流引擎配置bean --&gt; &lt;bean id=&quot;processEngineConfiguration&quot;\nclass=&quot;org.activiti.spring.SpringProcessEngineConfiguration&quot;&gt;\n&lt;!-- 数据源 --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt;\n&lt;!-- 使用spring事务管理器 --&gt; &lt;property name=&quot;transactionManager&quot; ref=&quot;transactionManager&quot; /&gt;\n&lt;!-- 数据库策略 --&gt; &lt;property name=&quot;databaseSchemaUpdate&quot; value=&quot;drop-create&quot; /&gt;\n&lt;!-- activiti的定时任务关闭 --&gt; &lt;property name=&quot;jobExecutorActivate&quot; value=&quot;false&quot; /&gt;\n&lt;/bean&gt;\n&lt;!-- 流程引擎 --&gt; &lt;bean id=&quot;processEngine&quot;\nclass=&quot;org.activiti.spring.ProcessEngineFactoryBean&quot;&gt;\n\n&lt;property name=&quot;processEngineConfiguration&quot;\nref=&quot;processEngineConfiguration&quot; /&gt;\n&lt;/bean&gt;\n&lt;!-- 资源服务service --&gt; &lt;bean id=&quot;repositoryService&quot; factory-bean=&quot;processEngine&quot;\nfactory-method=&quot;getRepositoryService&quot; /&gt;\n&lt;!-- 流程运行service --&gt; &lt;bean id=&quot;runtimeService&quot; factory-bean=&quot;processEngine&quot;\nfactory-method=&quot;getRuntimeService&quot; /&gt;\n&lt;!-- 任务管理service --&gt; &lt;bean id=&quot;taskService&quot; factory-bean=&quot;processEngine&quot;\nfactory-method=&quot;getTaskService&quot; /&gt;\n&lt;!-- 历史管理service --&gt; &lt;bean id=&quot;historyService&quot; factory-bean=&quot;processEngine&quot;\nfactory-method=&quot;getHistoryService&quot; /&gt;\n&lt;!-- 用户管理service --&gt; &lt;bean id=&quot;identityService&quot; factory-bean=&quot;processEngine&quot;\nfactory-method=&quot;getIdentityService&quot; /&gt;\n&lt;!-- 引擎管理service --&gt; &lt;bean id=&quot;managementService&quot; factory-bean=&quot;processEngine&quot;\nfactory-method=&quot;getManagementService&quot; /&gt;\n&lt;!-- 数据源 --&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;org.apache.commons.dbcp.BasicDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot; /&gt;\n&lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/activiti&quot; /&gt;\n&lt;property name=&quot;username&quot; value=&quot;root&quot; /&gt;\n&lt;property name=&quot;password&quot; value=&quot;mysql&quot; /&gt;\n&lt;property name=&quot;maxActive&quot; value=&quot;3&quot; /&gt;\n&lt;property name=&quot;maxIdle&quot; value=&quot;1&quot; /&gt;\n&lt;/bean&gt;\n&lt;!-- 事务管理器 --&gt; &lt;bean id=&quot;transactionManager&quot;\nclass=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt;\n&lt;/bean&gt;\n&lt;!-- 通知 --&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt;\n&lt;!-- 传播行为 --&gt; &lt;tx:method name=&quot;save*&quot; propagation=&quot;REQUIRED&quot; /&gt;\n\n&lt;tx:method name=&quot;insert*&quot; propagation=&quot;REQUIRED&quot; /&gt;\n&lt;tx:method name=&quot;delete*&quot; propagation=&quot;REQUIRED&quot; /&gt;\n&lt;tx:method name=&quot;update*&quot; propagation=&quot;REQUIRED&quot; /&gt;\n&lt;tx:method name=&quot;find*&quot; propagation=&quot;SUPPORTS&quot; read-only=&quot;true&quot; /&gt;\n&lt;tx:method name=&quot;get*&quot; propagation=&quot;SUPPORTS&quot; read-only=&quot;true&quot; /&gt;\n&lt;/tx:attributes&gt;\n&lt;/tx:advice&gt;\n&lt;!-- 切面，根据具体项目修改切点配置 --&gt; &lt;aop:config proxy-target-class=&quot;true&quot;&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot;\npointcut=&quot;execution(* com.itheima.ihrm.service.impl.*.*(..))&quot; /&gt;\n&lt;/aop:config&gt;\n&lt;/beans&gt;\n3. ProcessEngine工作流引擎，相当于一个门面接口，通过 ProcessEngineConfiguration 创建 processEngine，通过ProcessEngine 创建各个 service 接口。\n一般创建方式java//1.创建ProcessEngineConfiguration对象  第一个参数:配置文件名称  第二个参数是processEngineConfiguration的bean的id\n\nProcessEngineConfiguration configuration = ProcessEngineConfiguration\n        .createProcessEngineConfigurationFromResource(&quot;activiti.cfg.xml&quot;,&quot;processEngineConfiguration&quot;);\n                \n//2.创建ProcesEngine对象\n                \nProcessEngine processEngine = configuration.buildProcessEngine();\n\n//3.输出processEngine对象\n\nSystem.out.println(processEngine)简单创建方式将 activiti.cfg.xml 文件名及路径固定，且 activiti.cfg.xml 文件中有 processEngineConfiguration 的配置，可以使用如下代码创建 processEngine\njava //条件：1.activiti配置文件名称：activiti.cfg.xml   2.bean的id=&quot;processEngineConfiguration&quot;\n        ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine();4. ServiceService 创建方式 通过 ProcessEngine 创建 Service，Service 是工作流引擎提供用于进行工作流部署、执行、管理的服务接口。方式如下：\njavaRuntimeService runtimeService = processEngine.getRuntimeService();\nRepositoryService repositoryService = processEngine.getRepositoryService();\nTaskService taskService = processEngine.getTaskService();\n......Service 总览\n\n\n服务\n作用\n\n\n\nRepositoryService\nactiviti 的资源管理类\n\n\nRuntimeService\nactiviti 的流程运行管理类\n\n\nTaskService\nactiviti 的任务管理类\n\n\nHistoryService\nactiviti 的历史管理类\n\n\nManagerService\nactiviti 的引擎管理类\n\n\nRepositoryService\n是 activiti 的资源管理类，提供了管理和控制流程发布包和流程定义的操作。使用工作流建模工具设计的业务流程图需要使用此 service 将流程定义文件的内容部署到数据库。 除了部署流程定义以外还可以：\n查询引擎中的发布包和流程定义。\n暂停或激活发布包，对应全部和特定流程定义。 暂停意味着它们不能再执行任何操作了，激活是对应的反向操作。\n获得多种资源，像是包含在发布包里的文件， 或引擎自动生成的流程图。\n获得流程定义的 pojo 版本， 可以用来通过 java 解析流程，而不必通过 xml。\n\nRuntimeService它是 activiti 的流程运行管理类。可以从这个服务类中获取很多关于流程执行相关的信息\nTaskService是 activiti 的任务管理类。可以从这个类中获取任务的信息。\nHistoryService是 activiti 的历史管理类，可以查询历史信息，执行流程时，引擎会保存很多数据（根据配置），比如流程实例启动时间，任务的参与者， 完成任务的时间，每个流程实例的执行路径，等等。 这个服务主要通过查询功能来获得这些数据。\nManagementService是 activiti 的引擎管理类，提供了对 Activiti 流程引擎的管理和维护功能，这些功能不在工作流驱动的应用程序中使用，主要用于 Activiti 系统的日常维护。\nArticle link： https://tqgoblin.site/post/activiti学习笔记(一)/  Author： Stephen  \n","slug":"activiti学习笔记(一)","date":"2019-11-22T15:58:00.000Z","categories_index":"Java","tags_index":"Activiti","author_index":"Stephen"},{"id":"e8faf659d92c2b0deb12ac2619d15d18","title":"THUCTC 文本分类中jar冲突","content":"txt我将完整的THUCTC的Demo项目作为moudle引入到自己的工程中，Demo在其自己的moudle中可以正常运行，但我将Demo的代码整合到自己的工程中时，出现了如下错误\ntxt发现自己的工程中引入了高版本的snowball包，但是遗憾的是我并不能将它降低版本。高版本中的Amog类的构造方法发生了改变，造成了这个错误。\n解决方案: \ntxt  修改THUCTC工程中snowball包路径，重新打成jar包，再引入自己工程即可\n\n\nArticle link： https://tqgoblin.site/post/thuctc分类/  Author： Stephen  \n","slug":"thuctc分类","date":"2019-11-02T07:08:30.000Z","categories_index":"Java","tags_index":"大数据","author_index":"Stephen"},{"id":"4a207b7e885fdbcca8129599135e5f2e","title":"MySQL基础","content":"前言知识无底，学海无涯，到今天进入MySQL的学习4天了，知识点虽然简单，但是比较多，所以写一篇博客将MySQL的基础写出来，方便自己以后查找，还有就是分享给大家。\n一、SQL简述1.SQL的概述Structure Query Language(结构化查询语言)简称SQL，它被美国国家标准局(ANSI)确定为关系型数据库语言的美国标准，后被国际化标准组织(ISO)采纳为关系数据库语言的国际标准。数据库管理系统可以通过SQL管理数据库；定义和操作数据，维护数据的完整性和安全性。\n2.SQL的优点1、简单易学，具有很强的操作性2、绝大多数重要的数据库管理系统均支持SQL3、高度非过程化；用SQL操作数据库时大部分的工作由DBMS自动完成\n3.SQL的分类1、DDL(Data Definition Language) 数据定义语言，用来操作数据库、表、列等； 常用语句：CREATE、 ALTER、DROP2、DML(Data Manipulation Language) 数据操作语言，用来操作数据库中表里的数据；常用语句：INSERT、 UPDATE、 DELETE3、DCL(Data Control Language) 数据控制语言，用来操作访问权限和安全级别； 常用语句：GRANT、DENY4、DQL(Data Query Language) 数据查询语言，用来查询数据 常用语句：SELECT\n二、数据库的三大范式1、第一范式(1NF)是指数据库表的每一列都是不可分割的基本数据线；也就是说：每列的值具有原子性，不可再分割。2、第二范式(2NF)是在第一范式(1NF)的基础上建立起来得，满足第二范式(2NF)必须先满足第一范式(1NF)。如果表是单主键，那么主键以外的列必须完全依赖于主键；如果表是复合主键，那么主键以外的列必须完全依赖于主键，不能仅依赖主键的一部分。3、第三范式(3NF)是在第二范式的基础上建立起来的，即满足第三范式必须要先满足第二范式。第三范式(3NF)要求：表中的非主键列必须和主键直接相关而不能间接相关；也就是说：非主键列之间不能相关依赖。\n三、数据库的数据类型使用MySQL数据库存储数据时，不同的数据类型决定了 MySQL存储数据方式的不同。为此，MySQL数据库提供了多种数据类型，其中包括整数类型、浮点数类型、定点 数类型、日期和时间类型、字符串类型、二进制…等等数据类型。\n1.整数类型根据数值取值范围的不同MySQL 中的整数类型可分为5种，分别是TINYINT、SMALUNT、MEDIUMINT、INT和 BIGINT。下图列举了 MySQL不同整数类型所对应的字节大小和取值范围而最常用的为INT类型的，\n\n\n\n数据类型\n字节数\n无符号数的取值范围\n有符号数的取值范围\n\n\n\nTINYINT\n1\n0~255\n-128~127\n\n\nSMALLINT\n2\n0~65535\n-32768~32768\n\n\nMEDIUMINT\n3\n0~16777215\n-8388608~8388608\n\n\nINT\n4\n0~4294967295\n-2147483648~ 2147483648\n\n\nBIGINT\n8\n0~18446744073709551615\n-9223372036854775808~9223372036854775808\n\n\n2.浮点数类型和定点数类型在MySQL数据库中使用浮点数和定点数来存储小数。浮点数的类型有两种：单精度浮点数类型（FLOAT)和双精度浮点数类型（DOUBLE)。而定点数类型只有一种即DECIMAL类型。下图列举了 MySQL中浮点数和定点数类型所对应的字节大小及其取值范围：\n\n\n\n数据类型\n字节数\n有符号的取值范围\n无符号的取值范围\n\n\n\nFLOAT\n4\n-3.402823466E+38~-1.175494351E-38\n0和1.175494351E-38~3.402823466E+38\n\n\nDOUBLE\n8\n-1.7976931348623157E+308~2.2250738585072014E-308\n0和2.2250738585072014E-308~1.7976931348623157E+308\n\n\nDECIMAL（M,D）\nM+2\n-1.7976931348623157E+308~2.2250738585072014E-308\n0和2.2250738585072014E-308~1.7976931348623157E+308\n\n\n从上图中可以看出：DECIMAL类型的取值范围与DOUBLE类型相同。但是，请注意：DECIMAL类型的有效取值范围是由M和D决定的。其中，M表示的是数据的长 度，D表示的是小数点后的长度。比如，将数据类型为DECIMAL(6,2)的数据6.5243 插入数据库后显示的结果为6.52\n\n\n\n\n\n3.字符串类型在MySQL中常用CHAR 和 VARCHAR 表示字符串。两者不同的是：VARCHAR存储可变长度的字符串。当数据为CHAR(M)类型时，不管插入值的长度是实际是多少它所占用的存储空间都是M个字节；而VARCHAR(M)所对应的数据所占用的字节数为实际长度加1\n\n\n\n插入值\nCHAR(3)\n存储需求\nVARCHAR(3)\n存储需求\n\n\n\n‘’\n‘’\n3个字节\n‘’\n1个字节\n\n\n‘a’\n‘a’\n3个字节\n‘a’\n2个字节\n\n\n‘ab’\n‘ab’\n3个字节\n‘ab’\n3个字节\n\n\n‘abc’\n‘ab’\n3个字节\n‘abc’\n4个字节\n\n\n‘abcd’\n‘ab’\n3个字节\n‘abc’\n4字节\n\n\n4.字符串类型文本类型用于表示大文本数据，例如，文章内容、评论、详情等，它的类型分为如下4种：\n\n\n\n数据类型\n储存范围\n\n\n\nTINYTEXT\n0~255字节\n\n\nTEXT\n0~65535字节\n\n\nMEDIUMTEXT\n0~16777215字节\n\n\nLONGTEXT\n0~4294967295字节\n\n\n5.日期与时间类型MySQL提供的表示日期和时间的数据类型分别是 ：YEAR、DATE、TIME、DATETIME 和 TIMESTAMP。下图列举了日期和时间数据类型所对应的字节数、取值范围、日期格式以及零值：\n\n\n\n数据类型\n字节数\n取值范围\n日期格式\n零值\n\n\n\nYEAR\n1\n1901~2155\nYYYY\n0000\n\n\nDATE\n4\n1000-01-01~9999-12-31\nYYYY-MM-DD\n0000-00-00\n\n\nTIME\n3\n-838：59：59~ 838：59：59\nHH:MM:SS\n00:00:00\n\n\nDATETIME\n8\n1000-01-01 00:00:00~9999-12-31 23:59:59\nYYYY-MM-DD HH:MM:SS\n0000-00-00 00:00:00\n\n\nTIMESTAMP\n4\n1970-01-01 00:00:01~2038-01-19 03:14:07\nYYYY-MM-DD HH:MM:SS\n0000-00-00 00:00:00\n\n\n5.1 YEAR类型YEAR类型用于表示年份，在MySQL中，可以使用以下三种格式指定YEAR类型 的值。1、使用4位字符串或数字表示，范围为’1901’—‘2155’或1901—2155。例如，输入 ‘2019’或2019插入到数据库中的值均为2019。2、使用两位字符串表示，范围为’00’—‘99’。其中，‘00’—‘69’范围的值会被转换为 2000—2069范围的YEAR值，‘70’—‘99’范围的值会被转换为1970—1999范围的YEAR 值。例如，输入’19’插入到数据库中的值为2019。3、使用两位数字表示，范围为1—99。其中，1—69范围的值会被转换为2001— 2069范围的YEAR值，70—99范围的值会被转换为1970—1999范围的YEAR值。例 如，输入19插入到数据库中的值为2019。请注意：当使用YEAR类型时，一定要区分’0’和0。因为字符串格式的’0’表示的YEAR值是2000而数字格式的0表示的YEAR值是0000。\n5.2 TIME类型TIME类型用于表示时间值，它的显示形式一般为HH:MM:SS，其中，HH表示小时， MM表示分,SS表示秒。在MySQL中，可以使用以下3种格式指定TIME类型的值。1、以’D HH:MM:SS’字符串格式表示。其中，D表示日可取0—34之间的值, 插入数据时，小时的值等于(DX24+HH)。例如，输入’2 11:30:50’插入数据库中的日期为59:30:50。2、以’HHMMSS’字符串格式或者HHMMSS数字格式表示。 例如，输入’115454’或115454,插入数据库中的日期为11:54:543、使用CURRENT_TIME或NOW()输入当前系统时间。\n5.3 DATETIME类型DATETIME类型用于表示日期和时间，它的显示形式为’YYYY-MM-DD HH: MM:SS’，其中，YYYY表示年，MM表示月，DD表示日，HH表示小时，MM表示分，SS 表示秒。在MySQL中，可以使用以下4种格式指定DATETIME类型的值。以’YYYY-MM-DD HH:MM:SS’或者’YYYYMMDDHHMMSS’字符串格式表示的日期和时间，取值范围为’1000-01-01 00:00:00’—‘9999-12-3 23:59:59’。例如，输入’2019-01-22 09:01:23’或 ‘20140122_0_90123’插入数据库中的 DATETIME 值都为 2019-01-22 09:01:23。1、以’YY-MM-DD HH:MM:SS’或者’YYMMDDHHMMSS’字符串格式表示的日期和时间，其中YY表示年，取值范围为’00’—‘99’。与DATE类型中的YY相同，‘00’— ‘69’范围的值会被转换为2000—2069范围的值，‘70’—‘99’范围的值会被转换为1970—1999范围的值。2、以YYYYMMDDHHMMSS或者YYMMDDHHMMSS数字格式表示的日期 和时间。例如，插入20190122090123或者190122090123,插入数据库中的DATETIME值都 为 2019-01-22 09:01:23。3、使用NOW来输入当前系统的日期和时间。\n5.4 TIMESTAMP类型TIMESTAMP类型用于表示日期和时间，它的显示形式与DATETIME相同但取值范围比DATETIME小。在此，介绍几种TIMESTAMP类型与DATATIME类型不同的形式：1、使用CURRENT_TIMESTAMP输入系统当前日期和时间。2、输入NULL时系统会输入系统当前日期和时间。3、无任何输入时系统会输入系统当前日期和时间。\n6.二进制类型在MySQL中常用BLOB存储二进制类型的数据，例如：图片、PDF文档等。BLOB类型分为如下四种：\n\n\n\n数据类型\n储存范围\n\n\n\nTINYBLOB\n0~255字节\n\n\nBLOB\n0~65535字节\n\n\nMEDIUMBLOB\n0~16777215字节\n\n\nLONGBLOB\n0~4294967295字节\n\n\n四、数据库、数据表的基本操作1.数据库的基本操作MySQL安装完成后，要想将数据存储到数据库的表中，首先要创建一个数据库。创 建数据库就是在数据库系统中划分一块空间存储数据，语法如下：\ntxtcreate database 数据库名称;\n创建一个叫db1的数据库MySQL命令：\ntxt-- 创建一个叫db1的数据库\nshow create database db1;\n运行效果展示：  \n创建数据库后查看该数据库基本信息MySQL命令：\ntxtshow create database db1;\n运行效果展示：  \n删除数据库MySQL命令：\ntxtdrop database db1;\n运行效果展示：  \n查询出MySQL中所有的数据库MySQL命令：\ntxtshow databases;\n运行效果展示：  \n将数据库的字符集修改为gbk MySQL命令：\ntxtalter database db1 character set gbk;\n运行效果展示：  \n切换数据库 MySQL命令：\ntxtuse db1;\n运行效果展示：  \n查看当前使用的数据库 MySQL命令：\ntxtselect database();\n运行效果展示：  \n2.数据表的基本操作数据库创建成功后可在该数据库中创建数据表(简称为表)存储数据。请注意：在操作数据表之前应使用“USE 数据库名;”指定操作是在哪个数据库中进行先关操作，否则会抛出“No database selected”错误。语法如下：\ntxt create table 表名(\n         字段1 字段类型,\n         字段2 字段类型,\n         …\n         字段n 字段类型\n);\n2.1 创建数据表示例：创建学生表 MySQL命令：\ntxt create table student(\n id int,\n name varchar(20),\n gender varchar(10),\n birthday date\n );\n运行效果展示：  \n2.2 查看数据表示例：查看当前数据库中所有表 MySQL命令：\ntxtshow tables;\n运行效果展示：  \n示例：查表的基本信息 MySQL命令：\ntxtshow create table student;\n运行效果展示：  \n示例：查看表的字段信息 MySQL命令：\ntxtdesc student;\n运行效果展示：  \n2.3 修改数据表有时，希望对表中的某些信息进行修改，例如：修改表名、修改字段名、修改字段 数据类型…等等。在MySQL中使用alter table修改数据表.示例：修改表名 MySQL命令：\ntxtalter table student rename to stu;\n运行效果展示：  \n示例：修改字段名 MySQL命令：\ntxtalter table stu change name sname varchar(10);\n运行效果展示：  \n示例：修改字段数据类型 MySQL命令：\ntxtalter table stu modify sname int;\n运行效果展示：  \n示例：增加字段 MySQL命令：\ntxtalter table stu add address varchar(50);\n运行效果展示：  \n示例：删除字段 MySQL命令：\ntxtalter table stu drop address;\n运行效果展示：\n2.4 删除数据表语法：\ntxtdrop table 表名;\n示例：删除数据表 MySQL命令：\ntxtdrop table stu;\n运行效果展示：\n五、数据表的约束为防止错误的数据被插入到数据表，MySQL中定义了一些维护数据库完整性的规则；这些规则常称为表的约束。常见约束如下：\n\n\n\n约束条件\n说明\n\n\n\nPRIMARY KEY\n主键约束用于唯一标识对应的记录\n\n\nFOREIGN KEY\n外键约束\n\n\nNOT NULL\n非空约束\n\n\nUNIQUE\n唯一性约束\n\n\nDEFAULT\n默认值约束，用于设置字段的默认值\n\n\n以上五种约束条件针对表中字段进行限制从而保证数据表中数据的正确性和唯一性。换句话说，表的约束实际上就是表中数据的限制条件。\n\n\n\n1.主键约束主键约束即primary key用于唯一的标识表中的每一行。被标识为主键的数据在表中是唯一的且其值不能为空。这点类似于我们每个人都有一个身份证号，并且这个身份证号是唯一的。主键约束基本语法：\ntxt字段名 数据类型 primary key;\n设置主键约束(primary key)的第一种方式示例：MySQL命令：\ntxtcreate table student(\nid int primary key,\nname varchar(20)\n);\n运行效果展示：  \n设置主键约束(primary key)的第二·种方式示例：MySQL命令：\ntxtcreate table student01(\nid int\nname varchar(20),\nprimary key(id)\n);\n运行效果展示：  \n2.非空约束非空约束即 NOT NULL指的是字段的值不能为空，基本的语法格式如下所示：\ntxt字段名 数据类型 NOT NULL;\n示例：MySQL命令：\ntxtcreate table student02(\nid int\nname varchar(20) not null\n);\n运行效果展示：  \n3.默认值约束默认值约束即DEFAULT用于给数据表中的字段指定默认值，即当在表中插入一条新记录时若未给该字段赋值，那么，数据库系统会自动为这个字段插入默认值；其基本的语法格式如下所示：\ntxt字段名 数据类型 DEFAULT 默认值；\n示例：MySQL命令：\ntxtcreate table student03(\nid int,\nname varchar(20),\ngender varchar(10) default &#39;male&#39;\n);\n运行效果展示：  \n5.唯一性约束唯一性约束即UNIQUE用于保证数据表中字段的唯一性，即表中字段的值不能重复出现，其基本的语法格式如下所示：\ntxt字段名 数据类型 UNIQUE;\n示例：MySQL命令：\ntxtcreate table student04(\nid int,\nname varchar(20) unique\n);\n运行效果展示：  \n6.外键约束外键约束即FOREIGN KEY常用于多张表之间的约束。基本语法如下：\ntxt-- 在创建数据表时语法如下：\nCONSTRAINT 外键名 FOREIGN KEY (从表外键字段) REFERENCES 主表 (主键字段)\n-- 将创建数据表创号后语法如下：\nALTER TABLE 从表名 ADD CONSTRAINT 外键名 FOREIGN KEY (从表外键字段) REFERENCES 主表 (主键字段);\n示例：创建一个学生表 MySQL命令：\ntxtcreate table student05(\nid int primary key,\nname varchar(20)\n);\n示例：创建一个班级表 MySQL命令：\ntxtcreate table class(\nclassid int primary key,\nstudentid int\n);\n示例：学生表作为主表，班级表作为副表设置外键， MySQL命令：\ntxtalter table class add constraint fk_class_studentid foreign key(studentid) references student05(id);\n运行效果展示：  \n6.1 数据一致性概念大家知道：建立外键是为了保证数据的完整和统一性。但是，如果主表中的数据被删除或修改从表中对应的数据该怎么办呢？很明显，从表中对应的数据也应该被删除，否则数据库中会存在很多无意义的垃圾数据。\n6.2 删除外键语法如下：\ntxtalter table 从表名 drop foreign key 外键名；\n示例：删除外键 MySQL命令：\ntxtalter table class drop foreign key fk_class_studentid;\n运行效果展示：  \n外键的那个字段不在了证明删除成功了\n6.3 关于外键约束需要注意的细节1、从表里的外键通常为主表的主键2、从表里外键的数据类型必须与主表中主键的数据类型一致3、主表发生变化时应注意主表与从表的数据一致性问题\n六、数据表插入数据在MySQL通过INSERT语句向数据表中插入数据。在此，我们先准备一张学生表，代码如下：\ntxt create table student(\n id int,\n name varchar(30),\n age int,\n gender varchar(30)\n );\n1. 为表中所有字段插入数据每个字段与其值是严格一一对应的。也就是说：每个值、值的顺序、值的类型必须与对应的字段相匹配。但是，各字段也无须与其在表中定义的顺序一致，它们只要与 VALUES中值的顺序一致即可。语法如下：\ntxtINSERT INTO 表名（字段名1,字段名2,...) VALUES (值 1,值 2,...);\n示例：向学生表中插入一条学生信息 MySQL命令：\ntxtinsert into student (id,name,age,gender) values (1,&#39;bob&#39;,16,&#39;male&#39;);\n运行效果展示：  \n2. 为表中指定字段插入数据语法如下：\ntxtINSERT INTO 表名（字段名1,字段名2,...) VALUES (值 1,值 2,...);\n插入数据的方法基本和为表中所有字段插入数据，一样，只是需要插入的字段由你自己指定\n3. 同时插入多条记录语法如下：\ntxtINSERT INTO 表名 [(字段名1,字段名2,...)]VALUES (值 1,值 2,…),(值 1,值 2,…),...;\n在该方式中：(字段名1,字段名2,…)是可选的，它用于指定插入的字段名；(值 1,值 2,…),(值 1,值 2,…)表示要插入的记录，该记录可有多条并且每条记录之间用逗号隔开。示例：向学生表中插入多条学生信息 MySQL命令：\ntxtinsert into student (id,name,age,gender) values (2,&#39;lucy&#39;,17,&#39;female&#39;),(3,&#39;jack&#39;,19,&#39;male&#39;),(4,&#39;tom&#39;,18,&#39;male&#39;);\n运行效果展示：  \n七、更新数据在MySQL通过UPDATE语句更新数据表中的数据。在此，我们将就用六中的student学生表\n1. UPDATE基本语法txtUPDATE 表名 SET 字段名1=值1[,字段名2 =值2,…] [WHERE 条件表达式];\n在该语法中：字段名1、字段名2…用于指定要更新的字段名称；值1、值 2…用于表示字段的新数据；WHERE 条件表达式 是可选的，它用于指定更新数据需要满足的条件\n2. UPDATE更新部分数据示例：将name为tom的记录的age设置为20并将其gender设置为female MySQL命令：\ntxtupdate student set age=20,gender=&#39;female&#39; where name=&#39;tom&#39;;\n运行效果展示：  \n3. UPDATE更新全部数据示例：将所有记录的age设置为18 MySQL命令：\ntxtupdate student set age=18;\n运行效果展示：  \n八、删除数据在MySQL通过DELETE语句删除数据表中的数据。在此，我们先准备一张数据表，代码如下：\ntxt-- 创建学生表\n create table student(\n id int,\n name varchar(30),\n age int,\n gender varchar(30)\n );\n -- 插入数据\n insert into student (id,name,age,gender) values (2,&#39;lucy&#39;,17,&#39;female&#39;),(3,&#39;jack&#39;,19,&#39;male&#39;),(4,&#39;tom&#39;,18,&#39;male&#39;),(5,&#39;sal&#39;,19,&#39;female&#39;),(6,&#39;sun&#39;,20,&#39;male&#39;)\n,(7,&#39;sad&#39;,13,&#39;female&#39;),(8,&#39;sam&#39;,14,&#39;male&#39;);\n1. DELETE基本语法在该语法中：表名用于指定要执行删除操作的表；[WHERE 条件表达式]为可选参数用于指定删除的条件。\ntxtDELETE FROM 表名 [WHERE 条件表达式];\n2. DELETE删除部分数据示例：删除age等于14的所有记录 MySQL命令：\ntxtdelete from student where age=14;\n运行效果展示：  \n3. DELETE删除全部数据示例：删除student表中的所有记录 MySQL命令：\ntxtdelete from student;\n运行效果展示：  \n4. TRUNCATE和DETELE的区别TRUNCATE和DETELE都能实现删除表中的所有数据的功能，但两者也是有区别的：1、DELETE语句后可跟WHERE子句，可通过指定WHERE子句中的条件表达式只删除满足条件的部分记录；但是，TRUNCATE语句只能用于删除表中的所有记录。2、使用TRUNCATE语句删除表中的数据后，再次向表中添加记录时自动增加字段的默认初始值重新由1开始；使用DELETE语句删除表中所有记录后，再次向表中添加记录时自动增加字段的值为删除时该字段的最大值加13、DELETE语句是DML语句，TRUNCATE语句通常被认为是DDL语句\n九、MySQL数据表简单查询1.简单查询概述简单查询即不含where的select语句。在此，我们讲解简单查询中最常用的两种查询：查询所有字段和查询指定字段。在此，先准备测试数据，代码如下：\ntxt-- 创建数据库\nDROP DATABASE IF EXISTS mydb;\nCREATE DATABASE mydb;\nUSE mydb;\n\n-- 创建student表\nCREATE TABLE student (\n    sid CHAR(6),\n    sname VARCHAR(50),\n    age INT,\n    gender VARCHAR(50) DEFAULT &#39;male&#39;\n);\n\n-- 向student表插入数据\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1001&#39;, &#39;lili&#39;, 14, &#39;male&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1002&#39;, &#39;wang&#39;, 15, &#39;female&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1003&#39;, &#39;tywd&#39;, 16, &#39;male&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1004&#39;, &#39;hfgs&#39;, 17, &#39;female&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1005&#39;, &#39;qwer&#39;, 18, &#39;male&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1006&#39;, &#39;zxsd&#39;, 19, &#39;female&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1007&#39;, &#39;hjop&#39;, 16, &#39;male&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1008&#39;, &#39;tyop&#39;, 15, &#39;female&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1009&#39;, &#39;nhmk&#39;, 13, &#39;male&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1010&#39;, &#39;xdfv&#39;, 17, &#39;female&#39;);\n2.查询所有字段（方法不唯一只是举例）查询所有字段 MySQL命令：\ntxtselect * from student;\n运行效果展示：  \n3.查询指定字段（sid、sname）查询指定字段（sid、sname） MySQL命令：\ntxtselect sid,sname from student;\n运行效果展示：  \n4.常数的查询在SELECT中除了书写列名，还可以书写常数。可以用于标记常数的查询日期标记 MySQL命令：\ntxtselect sid,sname,&#39;2021-03-02&#39; from student;\n运行效果展示：  \n5.从查询结果中过滤重复数据在使用DISTINCT 时需要注意：在SELECT查询语句中DISTINCT关键字只能用在第一个所查列名之前。MySQL命令：\ntxtselect distinct gender from student;\n运行效果展示：  \n6.算术运算符（举例加运算符）在SELECT查询语句中还可以使用加减乘除运算符。查询学生10年后的年龄 MySQL命令：\ntxt select sname,age+10 from student;\n运行效果展示：  \n十、函数在此，先准备测试数据，代码如下：\ntxt-- 创建数据库\nDROP DATABASE IF EXISTS mydb;\nCREATE DATABASE mydb;\nUSE mydb;\n\n-- 创建student表\nCREATE TABLE student (\n    sid CHAR(6),\n    sname VARCHAR(50),\n    age INT,\n    gender VARCHAR(50) DEFAULT &#39;male&#39;\n);\n\n-- 向student表插入数据\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1001&#39;, &#39;lili&#39;, 14, &#39;male&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1002&#39;, &#39;wang&#39;, 15, &#39;female&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1003&#39;, &#39;tywd&#39;, 16, &#39;male&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1004&#39;, &#39;hfgs&#39;, 17, &#39;female&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1005&#39;, &#39;qwer&#39;, 18, &#39;male&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1006&#39;, &#39;zxsd&#39;, 19, &#39;female&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1007&#39;, &#39;hjop&#39;, 16, &#39;male&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1008&#39;, &#39;tyop&#39;, 15, &#39;female&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1009&#39;, &#39;nhmk&#39;, 13, &#39;male&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1010&#39;, &#39;xdfv&#39;, 17, &#39;female&#39;);\n1.聚合函数在开发中，我们常常有类似的需求：统计某个字段的最大值、最小值、 平均值等等。为此，MySQL中提供了聚合函数来实现这些功能。所谓聚合，就是将多行汇总成一行；其实，所有的聚合函数均如此——输入多行，输出一行。聚合函数具有自动滤空的功能，若某一个值为NULL，那么会自动将其过滤使其不参与运算。聚合函数使用规则：只有SELECT子句和HAVING子句、ORDER BY子句中能够使用聚合函数。例如，在WHERE子句中使用聚合函数是错误的。接下来，我们学习常用聚合函数。\n1.1、count（）统计表中数据的行数或者统计指定列其值不为NULL的数据个数查询有多少该表中有多少人MySQL命令：\ntxtselect count(*) from student;\n运行效果展示：  \n1.2、max（）计算指定列的最大值，如果指定列是字符串类型则使用字符串排序运算\n查询该学生表中年纪最大的学生MySQL命令：\ntxtselect max(age) from student;\n运行效果展示：  \n1.3、min（）计算指定列的最小值，如果指定列是字符串类型则使用字符串排序运算\n查询该学生表中年纪最小的学生 MySQL命令：\ntxtselect sname,min(age) from student;\n运行效果展示：  \n1.4、sum（）计算指定列的数值和，如果指定列类型不是数值类型则计算结果为0查询该学生表中年纪的总和 MySQL命令：\ntxtselect sum(age) from student;\n运行效果展示：  \n1.5、avg（）计算指定列的平均值，如果指定列类型不是数值类型则计算结果为\n查询该学生表中年纪的平均数 MySQL命令：\ntxtselect avg(age) from student;\n运行效果展示：  \n2.其他常用函数这里我就不一一举例了，基本混个眼熟，以后用到再细说\n2.1、时间函数txtSELECT NOW();\nSELECT DAY (NOW());\nSELECT DATE (NOW());\nSELECT TIME (NOW());\nSELECT YEAR (NOW());\nSELECT MONTH (NOW());\nSELECT CURRENT_DATE();\nSELECT CURRENT_TIME();\nSELECT CURRENT_TIMESTAMP();\nSELECT ADDTIME(&#39;14:23:12&#39;,&#39;01:02:01&#39;);\nSELECT DATE_ADD(NOW(),INTERVAL 1 DAY);\nSELECT DATE_ADD(NOW(),INTERVAL 1 MONTH);\nSELECT DATE_SUB(NOW(),INTERVAL 1 DAY);\nSELECT DATE_SUB(NOW(),INTERVAL 1 MONTH);\nSELECT DATEDIFF(&#39;2019-07-22&#39;,&#39;2019-05-05&#39;);\n2.2、字符串函数txt--连接函数\nSELECT CONCAT ()\n--\nSELECT INSTR ();\n--统计长度\nSELECT LENGTH();\n2.3、数学函数txt-- 绝对值\nSELECT ABS(-136);\n-- 向下取整\nSELECT FLOOR(3.14);\n-- 向上取整\nSELECT CEILING(3.14);\n十一、条件查询数据库中存有大量数据，我们可根据需求获取指定的数据。此时，我们可在查询语句中通过WHERE子句指定查询条件对查询结果进行过滤。在开始学习条件查询之前，我们先准备测试数据，代码如下：\ntxt-- 创建数据库\nDROP DATABASE IF EXISTS mydb;\nCREATE DATABASE mydb;\nUSE mydb;\n\n-- 创建student表\nCREATE TABLE student (\n    sid CHAR(6),\n    sname VARCHAR(50),\n    age INT,\n    gender VARCHAR(50) DEFAULT &#39;male&#39;\n);\n\n-- 向student表插入数据\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1001&#39;, &#39;lili&#39;, 14, &#39;male&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1002&#39;, &#39;wang&#39;, 15, &#39;female&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1003&#39;, &#39;tywd&#39;, 16, &#39;male&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1004&#39;, &#39;hfgs&#39;, 17, &#39;female&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1005&#39;, &#39;qwer&#39;, 18, &#39;male&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1006&#39;, &#39;zxsd&#39;, 19, &#39;female&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1007&#39;, &#39;hjop&#39;, 16, &#39;male&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1008&#39;, &#39;tyop&#39;, 15, &#39;female&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1009&#39;, &#39;nhmk&#39;, 13, &#39;male&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1010&#39;, &#39;xdfv&#39;, 17, &#39;female&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1012&#39;, &#39;lili&#39;, 14, &#39;male&#39;);\nINSERT INTO student (sid,sname,age,gender) VALUES (&#39;S_1013&#39;, &#39;wang&#39;, 15, &#39;female&#39;);\n1.使用关系运算符查询在WHERE中可使用关系运算符进行条件查询，常用的关系运算符如下所示：\n\n\n\n关系运算符\n说明\n\n\n\n=\n等于\n\n\n&lt;&gt;\n不等于\n\n\n!&#x3D;\n不等于\n\n\n&lt;\n小于\n\n\n&lt;&#x3D;\n小于等于\n\n\n&gt;\n大于\n\n\n&gt;&#x3D;\n大于等于\n\n\n查询年龄等于或大于17的学生的信息 MySQL命令：\ntxtselect * from student where age&gt;=17;\n运行效果展示：  \n2.使用IN关键字查询IN关键字用于判断某个字段的值是否在指定集合中。如果字段的值恰好在指定的集合中，则将字段所在的记录将査询出来。\n查询sid为S_1002和S_1003的学生信息 MySQL命令：\ntxtselect * from student where sid in (&#39;S_1002&#39;,&#39;S_1003&#39;);\n运行效果展示：  \n查询sid为S_1001以外的学生的信息 MySQL命令：\ntxtselect * from student where sid not in (&#39;S_1001&#39;);\n运行效果展示：  \n3.使用BETWEEN AND关键字查询BETWEEN AND用于判断某个字段的值是否在指定的范围之内。如果字段的值在指定范围内，则将所在的记录将查询出来查询15到18岁的学生信息 MySQL命令：\ntxtselect * from student where age between 15 and 18;\n运行效果展示：  \n查询不是15到18岁的学生信息 MySQL命令：\ntxtselect * from student where age not between 15 and 18;\n运行效果展示：  \n4.使用空值查询在MySQL中，使用 IS NULL关键字判断字段的值是否为空值。请注意：空值NULL不同于0，也不同于空字符串由于student表没有空值就不演示查询空值的了查询sname不为空值的学生信息 MySQL命令：\ntxtselect * from student where sname is not null;\n运行效果展示：  \n5.使用AND关键字查询在MySQL中可使用AND关键字可以连接两个或者多个查询条件。查询年纪大于15且性别为male的学生信息 MySQL命令：\ntxtselect * from student where age&gt;15 and gender=&#39;male&#39;;\n运行效果展示：  \n6.使用OR关键字查询在使用SELECT语句查询数据时可使用OR关键字连接多个査询条件。在使用OR关键字时，只要记录满足其中任意一个条件就会被查询出来查询年纪大于15或者性别为male的学生信息 MySQL命令：\ntxtselect * from student where age&gt;15 or gender=&#39;male&#39;;\n运行效果展示：  \n7.使用LIKE关键字查询MySQL中可使用LIKE关键字可以判断两个字符串是否相匹配\n7.1 普通字符串查询sname中与wang匹配的学生信息 MySQL命令：\ntxtselect * from student where sname like &#39;wang&#39;;\n运行效果展示：  \n7.2 含有%通配的字符串%用于匹配任意长度的字符串。例如，字符串“a%”匹配以字符a开始任意长度的字符串查询学生姓名以li开始的记录 MySQL命令：\ntxtselect * from student where sname like &#39;li%&#39;;\n运行效果展示：  \n查询学生姓名以g结尾的记录 MySQL命令：\ntxtselect * from student where sname like &#39;%g&#39;;\n运行效果展示：  \n查询学生姓名包含s的记录 MySQL命令：\ntxtselect * from student where sname like &#39;%s%&#39;;\n运行效果展示  \n7.3 含有_通配的字符串下划线通配符只匹配单个字符，如果要匹配多个字符，需要连续使用多个下划线通配符。例如，字符串“ab_”匹配以字符串“ab”开始长度为3的字符串，如abc、abp等等；字符串“a__d”匹配在字符“a”和“d”之间包含两个字符的字符串，如”abcd”、”atud”等等。查询学生姓名以zx开头且长度为4的记录 MySQL命令：\ntxtselect * from student where sname like &#39;zx__&#39;;\n运行效果展示  \n查询学生姓名以g结尾且长度为4的记录 MySQL命令：\ntxtselect * from student where sname like &#39;___g&#39;;\n运行效果展示  \n8.使用LIMIT限制查询结果的数量当执行查询数据时可能会返回很多条记录，而用户需要的数据可能只是其中的一条或者几条查询学生表中年纪最小的3位同学 MySQL命令：\ntxtselect * from student order by age asc limit 3;\n运行效果展示  \n9.使用GROUP BY进行分组查询GROUP BY 子句可像切蛋糕一样将表中的数据进行分组，再进行查询等操作。换言之，可通俗地理解为：通过GROUP BY将原来的表拆分成了几张小表。接下来，我们通过一个例子开始学习GROUP BY，代码如下\ntxt-- 创建数据库\nDROP DATABASE IF EXISTS mydb;\nCREATE DATABASE mydb;\nUSE mydb;\n\n-- 创建员工表\nCREATE TABLE employee (\n    id int,\n    name varchar(50),\n    salary int,\n    departmentnumber int\n);\n\n-- 向员工表中插入数据\nINSERT INTO employee values(1,&#39;tome&#39;,2000,1001); \nINSERT INTO employee values(2,&#39;lucy&#39;,9000,1002); \nINSERT INTO employee values(3,&#39;joke&#39;,5000,1003); \nINSERT INTO employee values(4,&#39;wang&#39;,3000,1004); \nINSERT INTO employee values(5,&#39;chen&#39;,3000,1001); \nINSERT INTO employee values(6,&#39;yukt&#39;,7000,1002); \nINSERT INTO employee values(7,&#39;rett&#39;,6000,1003); \nINSERT INTO employee values(8,&#39;mujk&#39;,4000,1004); \nINSERT INTO employee values(9,&#39;poik&#39;,3000,1001);\n9.1 GROUP BY和聚合函数一起使用统计各部门员工个数 MySQL命令：\ntxtselect count(*), departmentnumber from employee group by departmentnumber;\n运行效果展示  \n统计部门编号大于1001的各部门员工个数 MySQL命令：\ntxtselect count(*), departmentnumber from employee where departmentnumber&gt;1001 group by departmentnumber;\n运行效果展示  \n9.2 GROUP BY和聚合函数以及HAVING一起使用统计工资总和大于8000的部门 MySQL命令：\ntxtselect sum(salary),departmentnumber from employee group by departmentnumber having sum(salary)&gt;8000;\n运行效果展示  \n10.使用ORDER BY对查询结果排序从表中査询出来的数据可能是无序的或者其排列顺序不是我们期望的。为此，我们可以使用ORDER BY对查询结果进行排序其语法格式如下所示：\ntxtSELECT 字段名1,字段名2,…\nFROM 表名\nORDER BY 字段名1 [ASC 丨 DESC],字段名2 [ASC | DESC];\n在该语法中：字段名1、字段名2是查询结果排序的依据；参数 ASC表示按照升序排序，DESC表示按照降序排序；默认情况下，按照ASC方式排序。通常情况下，ORDER BY子句位于整个SELECT语句的末尾。查询所有学生并按照年纪大小升序排列 MySQL命令：\ntxtselect * from student order by age asc;\n运行效果展示  \n查询所有学生并按照年纪大小降序排列 MySQL命令：\ntxtselect * from student order by age desc;\n运行效果展示  \n十二、别名设置在査询数据时可为表和字段取別名，该别名代替表和字段的原名参与查询操作。操作的表事先已准备\n1.为表取别名在查询操作时，假若表名很长使用起来就不太方便，此时可为表取一个別名，用该别名来代替表的名称。语法格式如下所示：\ntxtSELECT * FROM 表名 [AS] 表的别名 WHERE .... ;\n将student改为stu查询整表 MySQL命令：\ntxtselect * from student as stu;\n运行效果展示  \n2.为字段取别名在查询操作时，假若字段名很长使用起来就不太方便，此时可该字段取一个別名，用该别名来代替字段的名称。语法格式如下所示：\ntxtSELECT 字段名1 [AS] 别名1 , 字段名2 [AS] 别名2 , ... FROM 表名 WHERE ... ;\n将student中的name取别名为“姓名” 查询整表 MySQL命令：\ntxtselect name as &#39;姓名&#39;,id from student;\n运行效果展示  \n十三、表的关联关系在实际开发中数据表之间存在着各种关联关系。在此，介绍MySQL中数据表的三种关联关系。多对一多对一(亦称为一对多)是数据表中最常见的一种关系。例如：员工与部门之间的关系，一个部门可以有多个员工；而一个员工不能属于多个部门只属于某个部门。在多对一的表关系 中，应将外键建在多的一方否则会造成数据的冗余。多对多多对多是数据表中常见的一种关系。例如：学生与老师之间的关系，一个学生可以有多个老师而且一个老师有多个学生。通常情况下，为了实现这种关系需要定义一张中间表(亦称为连接表)该表会存在两个外键分别参照老师表和学生表。一对一在开发过程中，一对一的关联关系在数据库中并不常见；因为以这种方式存储的信息通常会放在同一张表中。接下来，我们来学习在一对多的关联关系中如果添加和删除数据。先准备一些测试数据，代码如下：\ntxtDROP TABLE IF EXISTS student;\nDROP TABLE IF EXISTS class;\n\n-- 创建班级表\nCREATE TABLE class(\n    cid int(4) NOT NULL PRIMARY KEY,\n    cname varchar(30) \n);\n\n-- 创建学生表\nCREATE TABLE student(\n    sid int(8) NOT NULL PRIMARY KEY,\n    sname varchar(30),\n    classid int(8) NOT NULL\n);\n\n-- 为学生表添加外键约束\nALTER TABLE student ADD CONSTRAINT fk_student_classid FOREIGN KEY(classid) REFERENCES class(cid);\n-- 向班级表插入数据\nINSERT INTO class(cid,cname)VALUES(1,&#39;Java&#39;);\nINSERT INTO class(cid,cname)VALUES(2,&#39;Python&#39;);\n\n-- 向学生表插入数据\nINSERT INTO student(sid,sname,classid)VALUES(1,&#39;tome&#39;,1);\nINSERT INTO student(sid,sname,classid)VALUES(2,&#39;lucy&#39;,1);\nINSERT INTO student(sid,sname,classid)VALUES(3,&#39;lili&#39;,2);\nINSERT INTO student(sid,sname,classid)VALUES(4,&#39;domi&#39;,2);\n1.关联查询查询Java班的所有学生 MySQL命令：\ntxtselect * from student where classid=(select cid from class where cname=&#39;Java&#39;);\n运行效果展示  \n2.关于关联关系的删除数据请从班级表中删除Java班级。在此，请注意：班级表和学生表之间存在关联关系；要删除Java班级，应该先删除学生表中与该班相关联的学生。否则，假若先删除Java班那么学生表中的cid就失去了关联删除Java班 MySQL命令：\ntxtdelete from student where classid=(select cid from class where cname=&#39;Java&#39;);\ndelete from class where cname=&#39;Java&#39;;\n运行效果展示  \n十四、多表连接查询1.交叉连接查询交叉连接返回的结果是被连接的两个表中所有数据行的笛卡儿积；比如：集合A&#x3D;{a,b}，集合B&#x3D;{0,1,2}，则集合A和B的笛卡尔积为{(a,0),(a,1),(a,2),(b,0),(b,1),(b,2)}。所以，交叉连接也被称为笛卡尔连接，其语法格式如下：\ntxtSELECT * FROM 表1 CROSS JOIN 表2;\n在该语法中：CROSS JOIN用于连接两个要查询的表，通过该语句可以查询两个表中所有的数据组合。由于这个交叉连接查询在实际运用中没有任何意义，所以只做为了解即可\n2.内连接查询内连接(Inner Join)又称简单连接或自然连接，是一种非常常见的连接查询。内连接使用比较运算符对两个表中的数据进行比较并列出与连接条件匹配的数据行，组合成新的 记录。也就是说在内连接查询中只有满足条件的记录才能出现在查询结果中。其语法格式如下：\ntxtSELECT 查询字段1,查询字段2, ... FROM 表1 [INNER] JOIN 表2 ON 表1.关系字段=表2.关系字段\n在该语法中：INNER JOIN用于连接两个表，ON来指定连接条件；其中INNER可以省略。\n准备数据，代码如下：\ntxt-- 若存在数据库mydb则删除\nDROP DATABASE IF EXISTS mydb;\n-- 创建数据库mydb\nCREATE DATABASE mydb;\n-- 选择数据库mydb\nUSE mydb;\n\n-- 创建部门表\nCREATE TABLE department(\n  did int (4) NOT NULL PRIMARY KEY, \n  dname varchar(20)\n);\n\n-- 创建员工表\nCREATE TABLE employee (\n  eid int (4) NOT NULL PRIMARY KEY, \n  ename varchar (20), \n  eage int (2), \n  departmentid int (4) NOT NULL\n);\n\n-- 向部门表插入数据\nINSERT INTO department VALUES(1001,&#39;财务部&#39;);\nINSERT INTO department VALUES(1002,&#39;技术部&#39;);\nINSERT INTO department VALUES(1003,&#39;行政部&#39;);\nINSERT INTO department VALUES(1004,&#39;生活部&#39;);\n-- 向员工表插入数据\nINSERT INTO employee VALUES(1,&#39;张三&#39;,19,1003);\nINSERT INTO employee VALUES(2,&#39;李四&#39;,18,1002);\nINSERT INTO employee VALUES(3,&#39;王五&#39;,20,1001);\nINSERT INTO employee VALUES(4,&#39;赵六&#39;,20,1004);\n查询员工姓名及其所属部门名称 MySQL命令：\ntxtselect employee.ename,department.dname from department inner join employee on department.did=employee.departmentid;\n运行效果展示  \n3.外连接查询在使用内连接查询时我们发现：返回的结果只包含符合查询条件和连接条件的数据。但是，有时还需要在返回查询结果中不仅包含符合条件的数据，而且还包括左表、右表或两个表中的所有数据，此时我们就需要使用外连接查询。外连接又分为左(外)连接和右(外)连接。其语法格式如下：\ntxtSELECT 查询字段1,查询字段2, ... FROM 表1 LEFT | RIGHT [OUTER] JOIN 表2 ON 表1.关系字段=表2.关系字段 WHERE 条件\n由此可见，外连接的语法格式和内连接非常相似，只不过使用的是LEFT [OUTER] JOIN、RIGHT [OUTER] JOIN关键字。其中，关键字左边的表被称为左表，关键字右边的表被称为右表；OUTER可以省略。在使用左(外)连接和右(外)连接查询时，查询结果是不一致的，具体如下：1、LEFT [OUTER] JOIN 左(外)连接：返回包括左表中的所有记录和右表中符合连接条件的记录。2、RIGHT [OUTER] JOIN 右(外)连接：返回包括右表中的所有记录和左表中符合连接条件的记录。\n先准备数据，代码如下：\ntxt-- 若存在数据库mydb则删除\nDROP DATABASE IF EXISTS mydb;\n-- 创建数据库mydb\nCREATE DATABASE mydb;\n-- 选择数据库mydb\nUSE mydb;\n\n-- 创建班级表\nCREATE TABLE class(\n  cid int (4) NOT NULL PRIMARY KEY, \n  cname varchar(20)\n);\n\n-- 创建学生表\nCREATE TABLE student (\n  sid int (4) NOT NULL PRIMARY KEY, \n  sname varchar (20), \n  sage int (2), \n  classid int (4) NOT NULL\n);\n-- 向班级表插入数据\nINSERT INTO class VALUES(1001,&#39;Java&#39;);\nINSERT INTO class VALUES(1002,&#39;C++&#39;);\nINSERT INTO class VALUES(1003,&#39;Python&#39;);\nINSERT INTO class VALUES(1004,&#39;PHP&#39;);\n\n-- 向学生表插入数据\nINSERT INTO student VALUES(1,&#39;张三&#39;,20,1001);\nINSERT INTO student VALUES(2,&#39;李四&#39;,21,1002);\nINSERT INTO student VALUES(3,&#39;王五&#39;,24,1002);\nINSERT INTO student VALUES(4,&#39;赵六&#39;,23,1003);\nINSERT INTO student VALUES(5,&#39;Jack&#39;,22,1009);\n准备这组数据有一定的特点，为的是让大家直观的看出左连接与右连接的不同之处1、班级编号为1004的PHP班级没有学生2、学号为5的学生Jack班级编号为1009，该班级编号并不在班级表中\n3.1 左（外）连接查询左(外)连接的结果包括LEFT JOIN子句中指定的左表的所有记录，以及所有满足连接条件的记录。如果左表的某条记录在右表中不存在则在右表中显示为空。查询每个班的班级ID、班级名称及该班的所有学生的名字 MySQL命令：\ntxtselect class.cid,class.cname,student.sname from class left outer join student on class.cid=student.classid;\n运行效果展示  \n展示结果分析：1、分别找出Java班、C++班、Python班的学生2、右表的Jack不满足查询条件故其没有出现在查询结果中3、虽然左表的PHP班没有学生，但是任然显示了PHP的信息；但是，它对应的学生名字为NULL\n3.2 右（外）连接查询右(外)连接的结果包括RIGHT JOIN子句中指定的右表的所有记录，以及所有满足连接条件的记录。如果右表的某条记录在左表中没有匹配，则左表将返回空值。查询每个班的班级ID、班级名称及该班的所有学生的名字 MySQL命令：\ntxtselect class.cid,class.cname,student.sname from class right outer join student on class.cid=student.classid;\n运行效果展示  \n展示结果分析：1、分别找出Java班、C++班、Python班的学生2、左表的PHP班不满足查询条件故其没有出现在查询结果中3、虽然右表的jack没有对应班级，但是任然显示王跃跃的信息；但是，它对应的班级以及班级编号均为NULL\n十五、子查询子查询是指一个查询语句嵌套在另一个查询语句内部的查询；该查询语句可以嵌套在一个 SELECT、SELECT…INTO、INSERT…INTO等语句中。在执行查询时，首先会执行子查询中的语句，再将返回的结果作为外层查询的过滤条件。在子査询中通常可以使用比较运算符和IN、EXISTS、ANY、ALL等关键字。\n准备数据，代码如下：\ntxtDROP TABLE IF EXISTS student;\nDROP TABLE IF EXISTS class;\n\n-- 创建班级表\nCREATE TABLE class(\n  cid int (4) NOT NULL PRIMARY KEY, \n  cname varchar(20)\n);\n\n-- 创建学生表\nCREATE TABLE student (\n  sid int (4) NOT NULL PRIMARY KEY, \n  sname varchar (20), \n  sage int (2), \n  classid int (4) NOT NULL\n);\n\n-- 向班级表插入数据\nINSERT INTO class VALUES(1001,&#39;Java&#39;);\nINSERT INTO class VALUES(1002,&#39;C++&#39;);\nINSERT INTO class VALUES(1003,&#39;Python&#39;);\nINSERT INTO class VALUES(1004,&#39;PHP&#39;);\nINSERT INTO class VALUES(1005,&#39;Android&#39;);\n\n-- 向学生表插入数据\nINSERT INTO student VALUES(1,&#39;张三&#39;,20,1001);\nINSERT INTO student VALUES(2,&#39;李四&#39;,21,1002);\nINSERT INTO student VALUES(3,&#39;王五&#39;,24,1003);\nINSERT INTO student VALUES(4,&#39;赵六&#39;,23,1004);\nINSERT INTO student VALUES(5,&#39;小明&#39;,21,1001);\nINSERT INTO student VALUES(6,&#39;小红&#39;,26,1001);\nINSERT INTO student VALUES(7,&#39;小亮&#39;,27,1002);\n1.带比较运算符的子查询比较运算符前面我们提到过得，就是&gt;、&lt;、&#x3D;、&gt;&#x3D;、&lt;&#x3D;、!&#x3D;等查询张三同学所在班级的信息 MySQL命令：\ntxtselect * from class where cid=(select classid from student where sname=&#39;张三&#39;);\n运行效果展示  \n查询比张三同学所在班级编号还大的班级的信息 MySQL命令：\ntxtselect * from class where cid&gt;(select classid from student where sname=&#39;张三&#39;);\n运行效果展示  \n2.带EXISTS关键字的子查询EXISTS关键字后面的参数可以是任意一个子查询， 它不产生任何数据只返回TRUE或FALSE。当返回值为TRUE时外层查询才会 执行假如王五同学在学生表中则从班级表查询所有班级信息 MySQL命令：\ntxtselect * from class where exists (select * from student where sname=&#39;王五&#39;);\n运行效果展示  \n3.带ANY关键字的子查询ANY关键字表示满足其中任意一个条件就返回一个结果作为外层查询条件。\n查询比任一学生所属班级号还大的班级编号 MySQL命令：\ntxtselect * from class where cid &gt; any (select classid from student);\n运行效果展示  \n4.带ALL关键字的子查询ALL关键字与ANY有点类似，只不过带ALL关键字的子査询返回的结果需同时满足所有内层査询条件。\n查询比所有学生所属班级号还大的班级编号 MySQL命令：\ntxtselect * from class where cid &gt; all (select classid from student);\n运行效果展示  \n总结重要（从关键字分析）：查询语句的书写顺序和执行顺序select &#x3D;&#x3D;&#x3D;&gt; from &#x3D;&#x3D;&#x3D;&gt; where &#x3D;&#x3D;&#x3D;&gt; group by &#x3D;&#x3D;&#x3D;&gt; having &#x3D;&#x3D;&#x3D;&gt; order by &#x3D;&#x3D;&#x3D;&gt; limit查询语句的执行顺序from &#x3D;&#x3D;&#x3D;&gt; where &#x3D;&#x3D;&#x3D;&gt; group by &#x3D;&#x3D;&#x3D;&gt; having &#x3D;&#x3D;&#x3D;&gt; select &#x3D;&#x3D;&#x3D;&gt; order by &#x3D;&#x3D;&#x3D;&gt; limi\n基于复习，乐于分享 所以有了这篇文章！！！\nArticle link： https://tqgoblin.site/post/MySQL基础/  Author： Stephen  \n","slug":"MySQL基础","date":"2019-10-12T03:48:00.000Z","categories_index":"数据库","tags_index":"数据库","author_index":"Stephen"},{"id":"e384082c21cec712db7f9620d92150dc","title":"科学上网","content":"一、利用Chrome的谷歌助手实现科学上网\n进入Chrome的扩展程序界面\n\n  \n\n\n打开开发者模式\n\n    \n\n\n点击 加载已解压的扩展程序\n\n      \n\n\n加载谷歌助手\n\n    \n\n\n\n等待谷歌助手加载完成就能访问了\n\n    \n  \n\n\n  \n  \n  \n\n\n\n\n\n\n\n\n\n软件包:  点击下载  (Mac  Win 通用)\n二、Lantern代理\n谷歌助手虽然能让我们在谷歌商场下载插件，但如果是访问Youtube等网站就不能满足需求了\n\n\n使用 Lantern代理 每月有500MB 免费流量  能满足轻量用户需求 \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n官网地址 https://getlantern.org/zh_CN/index.html\n三、ShadowsocksX\n该软件能实现全局代理，可选服务器多 ，但是收费 🙂\n\n  \n\n  \n\n\n\n\n\n\n\n\n\n\n\n软件包:  点击下载  (mac 版)\nArticle link： https://tqgoblin.site/post/科学上网/  Author： Stephen  \n","slug":"科学上网","date":"2019-08-08T05:48:00.000Z","categories_index":"其他","tags_index":"科学上网","author_index":"Stephen"},{"id":"bc148c47b074649417f33e22cc961a4a","title":"private","content":"\n  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\n  \n    \n      \n      \n        Hey, password is required here.\n      \n    \n  \n\n","slug":"private","date":"2019-05-27T05:48:25.000Z","categories_index":"private","tags_index":"private","author_index":"Stephen"},{"id":"fc9ed1ed6c42c869484f950e1dcd4824","title":"Jetbrains 全家桶免费用","content":"Jetbrains软件激活方法下载激活包启动IDE，如果上来就需要注册，选择：试用（Evaluate for free）进入IDE点击你要注册的IDE菜单：”Configure” 或 “Help” -&gt; “Edit Custom VM Options …”      \n\n在打开的vmoptions编辑窗口末行添加：”-javaagent:&#x2F;absolute&#x2F;path&#x2F;to&#x2F;jetbrains-agent.jar” (填写你存放 jetbrains-agent.jar 文件的绝对路径)     \n  \n重启IDE。点击IDE菜单 “Help” -&gt; “Register…” 或 “Configure” -&gt; “Manage License…”      \n  \n填写激活码       \n  \n激活成功     \n        \n  \n\n激活包下载\n\n\n\n\n\n\n\n\n软件包:  点击下载  (Mac  Win 通用)\nArticle link： https://tqgoblin.site/post/Jetbrains/  Author： Stephen  \n","slug":"Jetbrains","date":"2019-03-02T02:48:00.000Z","categories_index":"其他","tags_index":"软件","author_index":"Stephen"},{"id":"f5234097a5a9cdb48e6197f98eea9fe5","title":"Spring boot 工程访问不到webapp文件下资源","content":"txt  当我们用IntelliJ IDEA 启动 springBoot新项目访问webapp下的静态页面时出现404，且没有任何错误的情况下   \n    \n   \n\ntxt解决方案   \n    \n   \n   \n  \n    \n   \n\n\nArticle link： https://tqgoblin.site/post/spring-boot-bug-webapp/  Author： Stephen  \n","slug":"spring-boot-bug-webapp","date":"2018-09-02T02:48:00.000Z","categories_index":"Java","tags_index":"JSP","author_index":"Stephen"},{"id":"296290365ed946b345a31171d65dd645","title":"基于freemaker 逆向工程","content":"\n    \n\n\njava  public class MysqlUtil &#123;\n      static &#123;\n          try &#123;\n              Class.forName(&quot;com.mysql.jdbc.Driver&quot;);\n          &#125; catch (ClassNotFoundException e) &#123;\n              e.printStackTrace();\n          &#125;\n      &#125;\n  \n      //获取连接\n      public static Connection getConnection(Configuration configuration) throws SQLException &#123;\n          String url = &quot;jdbc:mysql://&quot;+configuration.getAddress()+&quot;:&quot;+configuration.getPort();\n          Connection connection = null;\n          connection = DriverManager.getConnection(url, configuration.getUserName(), configuration.getPassword());\n          connection.setAutoCommit(true);\n          return connection;\n      &#125;\n  \n      //释放资源\n      public static void release(Connection connection, Statement statement, ResultSet resultSet) throws SQLException &#123;\n          if (connection!=null)&#123;\n              connection.close();\n          &#125;\n          if (statement!=null)&#123;\n              statement.close();\n          &#125;\n          if (resultSet!=null)&#123;\n              resultSet.close();\n          &#125;\n      &#125;\n  \n      //执行sql   在控制台中显示所有的数据库名称\n      public static void executeSqlForShowDatabases(Connection connection,Configuration configuration) throws SQLException &#123;\n          Statement statement = connection.createStatement();\n          ResultSet resultSet = statement.executeQuery(&quot;show databases;&quot;);\n          Set&lt;String&gt; databases = new HashSet&lt;String&gt;();\n          while (resultSet.next())&#123;\n              String databaseName = resultSet.getString(1);\n              databases.add(databaseName);\n              System.out.println(databaseName);\n          &#125;\n          configuration.setDatabases(databases);\n          MysqlUtil.release(null,statement,resultSet);\n      &#125;\n  \n      //执行sql    显示所有的表名称\n      public static void executeSqlForShowTables(Connection connection,Configuration configuration) throws SQLException &#123;\n          Statement statement = connection.createStatement();\n          statement.execute(&quot;use &quot;+configuration.getDatabase());\n          ResultSet resultSet = statement.executeQuery(&quot;show tables&quot;);\n          Set&lt;String&gt; tables = new HashSet&lt;String&gt;();\n          while (resultSet.next())&#123;\n              String tableName = resultSet.getString(1);\n              tables.add(tableName);\n              System.out.println(tableName);\n          &#125;\n          MysqlUtil.release(null,statement,resultSet);\n      &#125;\n  \n      //执行sql   查看表结构  能拿到表的字段 以及字段的数据类型\n      public static void executeSqlForDescTable(Connection connection,Configuration configuration) throws SQLException &#123;\n          Statement statement = connection.createStatement();\n          ResultSet resultSet = statement.executeQuery(&quot;desc &quot;+configuration.getTable());\n          Set&lt;ColumnModel&gt; columnModels = new HashSet&lt;ColumnModel&gt;();//当前这个表中所有的字段及类型\n          while (resultSet.next())&#123;\n              String columnName = resultSet.getString(1);\n              String columnType = resultSet.getString(2);\n              ColumnModel columnModel = new ColumnModel();\n              columnModel.setColumnName(columnName);\n              if (columnType.contains(&quot;varchar&quot;)||columnType.contains(&quot;char&quot;))&#123;\n                  columnModel.setType(&quot;String&quot;);\n              &#125;else if (columnType.contains(&quot;int&quot;))&#123;\n                  columnModel.setType(&quot;Integer&quot;);\n              &#125;else if (columnType.contains(&quot;datetime&quot;))&#123;\n                  columnModel.setType(&quot;Date&quot;);\n              &#125;else if (columnType.contains(&quot;bigint&quot;))&#123;\n                  columnModel.setType(&quot;Long&quot;);\n              &#125;\n              columnModels.add(columnModel);\n  //            System.out.println(columnName+&quot;=====&gt;&quot;+columnType);\n          &#125;\n          TableModel tableModel = new TableModel();\n          tableModel.setTableName(configuration.getTable());\n          tableModel.setColumnModels(columnModels);\n          configuration.setTableModel(tableModel);\n          MysqlUtil.release(null,statement,resultSet);\n  \n      &#125;\n  &#125;\n\n\n\n\n\n\n\n\n源码:  点击下载 \nArticle link： https://tqgoblin.site/post/codegenerator/  Author： Stephen  \n","slug":"codegenerator","date":"2018-08-12T02:48:00.000Z","categories_index":"Java","tags_index":"freemaker","author_index":"Stephen"},{"id":"d30ace39e1775eb881fa386d10463fa5","title":"正整数分解质因数","content":"将一个正整数分解质因数。例如：输入90，打印出90&#x3D;2*3*3*5java    public class Solution &#123;\n        public static void main(String[] args) &#123;\n            Scanner sc = new Scanner(System.in);\n            System.out.println(&quot;请输入要分解的正整数：&quot;);\n            int n = sc.nextInt();\n            StringBuilder sb = new StringBuilder();\n            sb.append(n + &quot;=&quot;);\n            //逻辑是：从2-n中找一个最小的且能被n整除的数k\n            //如果找到了 就用 n/k 的商作为新n  继续循环\n            //如果n == k 则说明是质数 或者商是质数 就循环结束直接输出\n            for (int i = 2; i &lt; n + 1; i++) &#123;\n                while (n % i == 0 &amp;&amp; n != i) &#123;\n                    n = n / i;\n                    sb.append(i + &quot;*&quot;);\n                &#125;\n    \n                if (n == i) &#123;     //如果上面的都不能整除说明是个质数\n                    sb.append(i);\n                    break;\n                &#125;\n            &#125;\n        System.out.println(sb.toString());\n    &#125;\n&#125;Article link： https://tqgoblin.site/post/正整数分解质因数/  Author： Stephen  \n","slug":"正整数分解质因数","date":"2018-04-13T05:48:00.000Z","categories_index":"Java","tags_index":"算法","author_index":"Stephen"},{"id":"ef5a4863e88e0b505e528fa60d638cfe","title":"Hannuota","content":"Java 实现 汉诺塔解法汉诺塔：汉诺塔问题是源于印度一个古老传说的益智玩具。大梵天创造世界的时候做了三根金刚石柱子，在一根柱子上从下往上按照大小顺序摞着64片黄金圆盘。大梵天命令婆罗门把圆盘从下面开始按大小顺序重新摆放在另一根柱子上。并且规定，在小圆盘上不能放大圆盘，在三根柱子之间一次只能移动一个圆盘。\n     \n\njavapublic class Solution &#123;\n    public static void main(String[] args) &#123;\n        int n = 3;\n        char from = &#39;A&#39;;\n        char in = &#39;B&#39;;\n        char to = &#39;c&#39;;\n        func(n, from, in, to);\n    &#125;\n\n    private static void func(int n, char from, char in, char to) &#123;\n        if (n == 1) &#123;\n            System.out.println(&quot;将第&quot; + n + &quot;块从&quot; + from + &quot;移到&quot; + to);\n            // 无论有多少盘子 都看成2个盘子 ， 最后一个盘子和 上面的所有盘子\n        &#125; else &#123;\n            // 将除最后一个之外的所有的盘子移到  temp位置去\n            func(n - 1, from, to, in);\n            // 将最后一个盘子移到目标位置\n            System.out.println(&quot;将第&quot; + n + &quot;块从&quot; + from + &quot;移到&quot; + to);\n            // 再将n-1个盘子从 temp移到目标位置\n            func(n - 1, in, from, to);\n\n        &#125;\n    &#125;\n&#125;Article link： https://tqgoblin.site/post/Hannuota/  Author： Stephen  \n","slug":"Hannuota","date":"2018-04-10T02:09:00.000Z","categories_index":"Java","tags_index":"算法","author_index":"Stephen"}]